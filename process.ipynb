{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Generate data splits.\n",
    "**ACL_ARC**: https://github.com/oacore/dynamic_citation_context/tree/main/data/acl_arc\n",
    "\n",
    "**ACT2**: https://github.com/oacore/dynamic_citation_context/tree/main/data/sdp_act\n",
    "\n",
    "We reserve 15% of the training set for the validation as there is not published validation split. We replace the original \"#AUTHOR_TAG\" as \"#CITATION_TAG\".  \n",
    "\n",
    "**FOCAL**: https://huggingface.co/datasets/adsabs/FOCAL\n",
    "\n",
    "We use the original split. \n",
    "\n",
    "Focal allows multiple labels for one cited work (although mentioned as \"citation\" in the dataset). Each label is associated with a piece of citation text (The cited work is explicited mentioned for multiple times, or the mentioning is scattered in the text). Therefore, we reorganize the dataset and insert citation anchors in such a way:\n",
    "\n",
    "- If there are **multiple different labels**, we insert a tag at the end of each citation text to create **multiple samples with different labels**. \n",
    "- If there is only **one label and one citation text** , we replace the citation text with the tag. \n",
    "- If there is only **one label but multiple citation texts**, we insert a tag at the end of the **first text** to create **one sample**. \n",
    "\n",
    "Also, the classification schema seems to be finegrained compared with that of the other two datasets among these classes: 'Similarities', 'Differences' and 'Compare/Contrast'(neutral). So, we merge them into 'Compare/Contrast' class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_acl(dataset, dataframe):\n",
    "    paragraphs = [] \n",
    "    uids = []\n",
    "    labels = []\n",
    "    pattern = r'#(.*?)_TAG'\n",
    "    for i, data in dataframe.iterrows():\n",
    "        if dataset=='acl_arc' and (data['unique_id']=='CC862' or data['unique_id']=='CC1543'): \n",
    "            continue # these citations has an empty context.\n",
    "        paragraph = data['cite_context_paragraph']\n",
    "        label = data['citation_class_label']\n",
    "        uid = data['unique_id']\n",
    "        match = re.search(pattern, paragraph)\n",
    "        if match:\n",
    "            res = match.group(1)\n",
    "            paragraph = paragraph.replace(res, 'CITATION')\n",
    "        else: # fix a few cases \n",
    "            if dataset == 'acl_arc':\n",
    "                paragraph = paragraph.replace('Clark and Curran', '#CITATION_TAG')\n",
    "            elif data['unique_id']=='CC109' or data['unique_id']=='CC42':\n",
    "                paragraph = paragraph.replace('[14]', '#CITATION_TAG') \n",
    "            elif data['unique_id']=='CC1039':\n",
    "                paragraph = paragraph.replace('Wiener, 1948', '#CITATION_TAG') \n",
    "            elif data['unique_id']=='CCT218':\n",
    "                paragraph = paragraph.replace('94', '#CITATION_TAG')\n",
    "            elif data['unique_id']=='CCT365':\n",
    "                paragraph = paragraph.replace('38', '#CITATION_TAG') \n",
    "            elif data['unique_id']=='CCT591':\n",
    "                paragraph = paragraph.replace('65', '#CITATION_TAG') \n",
    "            elif data['unique_id']=='CCT610':\n",
    "                paragraph = paragraph.replace('74', '#CITATION_TAG') \n",
    "            elif data['unique_id']=='CCT879':\n",
    "                paragraph = paragraph.replace('Pledger et al', '#CITATION_TAG') \n",
    "\n",
    "            \n",
    "        paragraph = paragraph[1:-1].replace('\\'', '\\\"').split('\\\",')\n",
    "        paragraph = ' '.join([_.strip('\\'\\\" ') for _ in paragraph])\n",
    "\n",
    "        uids.append(uid)\n",
    "        paragraphs.append(paragraph)\n",
    "        labels.append(label)\n",
    "\n",
    "    res = {\n",
    "    'unique_id':uids,\n",
    "    'context':paragraphs,\n",
    "    'label':labels,\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "\n",
    "dataset = 'acl_arc'\n",
    "\n",
    "train_data = pd.read_csv(f\"raw_data/{dataset}_train.txt\", sep='\\t')\n",
    "test_data = pd.read_csv(f\"raw_data/{dataset}_test.txt\", sep='\\t')\n",
    "valid_data = train_data.sample(frac=.15, random_state=1)\n",
    "\n",
    "train_data = train_data.drop(valid_data.index)\n",
    "\n",
    "train_data = process_acl(dataset, train_data)\n",
    "valid_data = process_acl(dataset, valid_data)\n",
    "test_data = process_acl(dataset, test_data)\n",
    "\n",
    "train_data.to_csv(f\"data/{dataset}_train.txt\", sep='\\t',index=False)\n",
    "valid_data.to_csv(f\"data/{dataset}_valid.txt\",sep='\\t', index=False)\n",
    "test_data.to_csv(f\"data/{dataset}_test.txt\",sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 2421 samples.\n",
      "To be processed: 2403 samples.\n",
      "Got  2617 samples. Unique id: 2617\n",
      "Load 606 samples.\n",
      "To be processed: 606 samples.\n",
      "Got  660 samples. Unique id: 660\n",
      "Load 821 samples.\n",
      "To be processed: 821 samples.\n",
      "Got  889 samples. Unique id: 889\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "MAP = {'Background': 0,\n",
    "    'Compare/Contrast': 1,\n",
    "    'Extends': 2,\n",
    "    'Future Work': 3,\n",
    "    'Motivation': 4,\n",
    "    'Uses': 5}\n",
    "\n",
    "\n",
    "def annotate_first_text(uid, sd, para, labels, res):\n",
    "        cur_para = para[:sd[0][0]]+'(#CITATION_TAG)'+ para[sd[0][1]:]\n",
    "        cur_label = MAP[labels[0]]\n",
    "        cur_para = cur_para.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "        res['unique_id'].append(uid)\n",
    "        res['context'].append(cur_para)\n",
    "        res['label'].append(cur_label)\n",
    "\n",
    "def annotate_every_text(uid, text_ranges, para, labels, res):\n",
    "    for label, text_range in zip(labels, text_ranges):\n",
    "        d = text_range[1]\n",
    "        if f\"{uid}_{d}\" not in res['unique_id']:\n",
    "            if para[d-1]=='.':\n",
    "                cur_para = para[:d-1]+'(#CITATION_TAG)'+ para[d-1:]\n",
    "            else:\n",
    "                cur_para = para[:d]+'(#CITATION_TAG)'+ para[d:]\n",
    "            cur_label = MAP[label]\n",
    "            cur_para = cur_para.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "\n",
    "            res['unique_id'].append(f\"{uid}_{d}\")\n",
    "            res['context'].append(cur_para)\n",
    "            res['label'].append(cur_label)\n",
    "\n",
    "def annotate_citation_entry(uid, s, d, para, labels, res):\n",
    "    cur_para = para[:s] + '(#CITATION_TAG)' +para[d:]\n",
    "    cur_label = MAP[labels[0]]\n",
    "    cur_para = cur_para.replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "    res['unique_id'].append(uid)\n",
    "    #print(cur_para)\n",
    "    res['context'].append(cur_para)\n",
    "    res['label'].append(cur_label)\n",
    "\n",
    "def process_focal(train_data):\n",
    "    print(f'Load {len(train_data)} samples.' )\n",
    "\n",
    "    # We manually handle some duplicates in identifiers of train_data.\n",
    "    updated_train_data = {}\n",
    "    for i, record in enumerate(train_data):\n",
    "        uid = record['Identifier']\n",
    "        if not uid in updated_train_data:\n",
    "            updated_train_data[uid] = record\n",
    "        else:\n",
    "            if uid=='2021MNRAS.504..146V__Vink_&_Gr√§fener_2012_Instance_1':\n",
    "                pass\n",
    "            elif uid == '2021AandA...655A..99D__Carigi_et_al._2005_Instance_3':\n",
    "                pass\n",
    "            elif uid == '2021AandA...655A..99D__Carigi_et_al._2005_Instance_2':\n",
    "                pass\n",
    "            elif uid == '2018ApJ...863..162M__Liu_et_al._2013_Instance_3':\n",
    "                pass\n",
    "            elif uid == '2018MNRAS.479.3254V___2000_Instance_1':\n",
    "                pass\n",
    "            elif uid == '2015MNRAS.450.4364N__Wu_et_al._2004_Instance_1':\n",
    "                updated_train_data[uid]['Functions Label'] += record['Functions Label']\n",
    "                updated_train_data[uid]['Functions Start End'] += record['Functions Start End']\n",
    "            elif uid == '2018AandA...619A..13V__Saviane_et_al._2012_Instance_4':\n",
    "                updated_train_data[uid] = record\n",
    "            elif uid == '2016AandA...588A..44Y__Jones_et_al._2014_Instance_1':\n",
    "                updated_train_data[uid+'_d'] = record\n",
    "            elif uid == '2022ApJ...936..102A__Williams_et_al._2006_Instance_1':\n",
    "                updated_train_data[uid+'_d'] = record\n",
    "            else: pass\n",
    "\n",
    "    print(f'To be processed: {len(updated_train_data )} samples.' )\n",
    "\n",
    "    res = {'unique_id':[],'context':[],'label':[]}\n",
    "\n",
    "    for uid in updated_train_data:\n",
    "        record = updated_train_data[uid]\n",
    "        para = record['Paragraph']\n",
    "        \n",
    "        labels = record['Functions Label']\n",
    "        for j, l in enumerate(labels):\n",
    "            if l == 'Similarities' or l == 'Differences':\n",
    "                labels[j] = 'Compare/Contrast'\n",
    "        \n",
    "        sd = record['Citation Start End']\n",
    "        text_ranges = record['Functions Start End']\n",
    "        texts = record['Citation Text']\n",
    "        \n",
    "        # multiple texts & labels\n",
    "        if len(set(labels))>1: \n",
    "            annotate_every_text(uid, text_ranges, para, labels, res)\n",
    "        else:\n",
    "            # one label\n",
    "            if len(sd)==1: # one entry\n",
    "                annotate_citation_entry(uid, sd[0][0], sd[0][1], para, labels, res)     \n",
    "            else: # entry is split\n",
    "                #the entry is locally split as (author, year)\n",
    "                if texts[1][0]>='0' and texts[1][0]<='9':  \n",
    "                    annotate_citation_entry(uid, sd[0][0], sd[1][1], para, labels, res)\n",
    "                else:\n",
    "                    # multiple texts are far apart. \n",
    "                    annotate_first_text(uid, sd, para, labels, res)\n",
    "\n",
    "    print('Got ', len(res['unique_id']), 'samples.', 'Unique id:', len(set(res['unique_id'])))\n",
    "    res = pd.DataFrame(res)\n",
    "    return res\n",
    "\n",
    "path = 'raw_data'\n",
    "save_path = 'data'\n",
    "\n",
    "with open(f'{path}/FOCAL-TRAINING.jsonl','r') as fp:\n",
    "    train_data = fp.readlines()\n",
    "    train_data = [json.loads(_) for _ in train_data]\n",
    "\n",
    "train_data = process_focal(train_data)\n",
    "train_data.to_csv(f\"{save_path}/focal_train.txt\",sep='\\t',index=False)\n",
    "\n",
    "with open(f'{path}/FOCAL-VALIDATION.jsonl','r') as fp:\n",
    "    valid_data = fp.readlines()\n",
    "    valid_data = [json.loads(_) for _ in valid_data]\n",
    "\n",
    "valid_data = process_focal(valid_data)\n",
    "valid_data.to_csv(f\"{save_path}/focal_valid.txt\",sep='\\t',index=False)\n",
    "\n",
    "with open(f'{path}/FOCAL-TESTING.jsonl','r') as fp:\n",
    "    test_data = fp.readlines()\n",
    "    test_data = [json.loads(_) for _ in test_data]\n",
    "\n",
    "test_data = process_focal(test_data)\n",
    "test_data.to_csv(f\"{save_path}/focal_test.txt\",sep='\\t',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract input context and generate SP augmentations.\n",
    "\n",
    "The split_into_sentences function is based on  https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences\n",
    "\n",
    "We manually add some rules to solve for all datasets. For Citss, we only need to process for training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. #SP 4.644031451036454 Maximum 15\n",
      "Avg. #SP 6.850196078431373 Maximum 15\n",
      "Avg. #SP 12.988536492166602 Maximum 15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr| al|Fig|\\{|Eq|right\\)|right\\) |Œºm|fig| etc|,|Eqs|eqs|\\(cf| vs|Sect)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co|)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "digits = \"([0-9])\"\n",
    "multiple_dots = r'\\.{2,}'\n",
    "\n",
    "specials = ['0.‚Ä≤‚Ä≤','Sect. 5.7']\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(multiple_dots, lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\", text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    if \"2.2.2\" in text: text = text.replace(\"2.2.2\",\"2<prd>2<prd>2<prd>\")\n",
    "    if \"4.2.1\" in text: text = text.replace(\"4.2.1\",\"4<prd>2<prd>1<prd>\")\n",
    "    if \"e.g.\" in text: text = text.replace(\"e.g.\",\"e<prd>g<prd>\")\n",
    "    if \"e.g\" in text: text = text.replace(\"e.g\",\"e<prd>g<prd>\")\n",
    "    if \".\\end\" in text: text = text.replace(\".\\end\",\"<prd>\\\\end\")\n",
    "    for s in specials:\n",
    "        if s in text: \n",
    "            tmp = s.replace('.',\"<prd>\")+\"<prd>\"\n",
    "            text = text.replace(s, tmp)\n",
    "           \n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "\n",
    "    if \"‚Äù\" in text: text = text.replace(\".‚Äù\",\"‚Äù.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    if sentences and not sentences[-1]: sentences = sentences[:-1]\n",
    "    return sentences\n",
    "\n",
    "def sp(data_path, generate_aug=False):\n",
    "    data = pd.read_csv(data_path, sep='\\t')\n",
    "\n",
    "    sas = {}\n",
    "    L = 3 # one-side range\n",
    "    input_context = [] # T_i\n",
    "    citance = []\n",
    "    num_augmentation = []\n",
    "    \n",
    "    for i, line in data.iterrows():\n",
    "        uid = line['unique_id']\n",
    "        paragraph = line['context']\n",
    "        sents = split_into_sentences(paragraph)\n",
    "\n",
    "        num_sent = len(sents)\n",
    "        sentence_augmentations = []  \n",
    "\n",
    "        no_citation = True\n",
    "        for j, sent in enumerate(sents):\n",
    "            if '#CITATION_TAG' in sent: # the citance   \n",
    "                no_citation = False\n",
    "                min_clip = max(0, j-L)\n",
    "                max_clip = min(num_sent, j+L+1)\n",
    "                input_context.append(' '.join(sents[min_clip:max_clip]))   \n",
    "                citance.append(sent[j])\n",
    "                if generate_aug:\n",
    "                    for k in range(1, 2*L+2):\n",
    "                        min_b = max(min_clip, j-k+1)\n",
    "                        for b in range(min_b, j+1): # b<=j, must contain the citance\n",
    "                            if (b+k) <= max_clip: \n",
    "                                if not b==min_clip or not b+k==max_clip:\n",
    "                                    tmp = sents[b: b+k]\n",
    "                                    sentence_augmentations.append(' '.join(tmp))\n",
    "                            else: break   \n",
    "                break\n",
    "\n",
    "        \n",
    "        num_augmentation.append(len(sentence_augmentations))\n",
    "        sas[uid] = sentence_augmentations\n",
    "    \n",
    "    if generate_aug:\n",
    "        print('Avg. #SP', sum(num_augmentation)/len(sas), 'Maximum', max(num_augmentation))\n",
    "\n",
    "    data['input_context'] = input_context\n",
    "    data['citance'] = citance\n",
    "    data.to_csv(data_path, sep='\\t', index=False)\n",
    "    return sas\n",
    "\n",
    "\n",
    "def extract_input(dataset):\n",
    "    path = 'data'\n",
    "    sp_augs = sp(f'{path}/{dataset}_train.txt', generate_aug=True)\n",
    "    with open(f'{path}/{dataset}_sas.json', 'w') as fp:\n",
    "        json.dump(sp_augs, fp) \n",
    "    sp(f'{path}/{dataset}_train.txt')\n",
    "    sp(f'{path}/{dataset}_valid.txt')\n",
    "    sp(f'{path}/{dataset}_test.txt')\n",
    "\n",
    "\n",
    "extract_input('acl_arc')\n",
    "extract_input('act2')\n",
    "extract_input('focal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphtranslator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
