{"CC2": ["Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #CITATION_TAG ) ."], "CC3": ["More details on how the structural divergences described in ( #CITATION_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) ."], "CC5": [], "CC6": ["Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill \"s tagger ( #CITATION_TAG ) ) and parsers generally aim to produce a tree spanning each sentence .", "However, knowledge of sentence boundaries is required by many NLP technologies. Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill \"s tagger ( #CITATION_TAG ) ) and parsers generally aim to produce a tree spanning each sentence ."], "CC7": ["SWIZZLE is a multilingual enhancement of COCKTAIL ( #CITATION_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.", "For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL ( #CITATION_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.", "SWIZZLE is a multilingual enhancement of COCKTAIL ( #CITATION_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.", "For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL ( #CITATION_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.", "SWIZZLE is a multilingual enhancement of COCKTAIL ( #CITATION_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference. Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents."], "CC8": ["Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.", "The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.", "Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference.", "Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.", "The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference.", "Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference. This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).", "Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques. Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.", "Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference.", "The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference. This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).", "Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference. This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority.", "Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques. Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference.", "Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference. This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).", "The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference. This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority.", "Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques. Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference. This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).", "Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #CITATION_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) . For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference. This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority."], "CC10": ["For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference", "For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference", "For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).", "Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference", "For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).", "For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority.", "The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference", "Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).", "For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority.", "For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority. Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural.", "The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).", "Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority.", "For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority. Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural.", "The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority.", "Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #CITATION_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority. Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural."], "CC11": ["For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc. Each component will return a confidence measure of the reliability of its prediction , c.f. ( #CITATION_TAG ) . The results from each component are evaluated to determine the final category of the word."], "CC12": ["We use an in-house statistical tagger ( based on ( #CITATION_TAG ) ) to tag the text in which the unknown word occurs . The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).", "The first feature represents the part of speech of the word. We use an in-house statistical tagger ( based on ( #CITATION_TAG ) ) to tag the text in which the unknown word occurs . The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).", "We use an in-house statistical tagger ( based on ( #CITATION_TAG ) ) to tag the text in which the unknown word occurs . The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD). The tag set contains just one tag to identify nouns."], "CC13": ["Research that is more similar in goal to that outlined in this paper is Vosse ( #CITATION_TAG ) . Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.", "Research that is more similar in goal to that outlined in this paper is Vosse ( #CITATION_TAG ) . Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names.", "Research that is more similar in goal to that outlined in this paper is Vosse ( #CITATION_TAG ) . Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions."], "CC14": ["Corpus frequency : ( #CITATION_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency . His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms."], "CC15": ["Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #CITATION_TAG ) . However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.", "The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations. Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #CITATION_TAG ) . However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.", "Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #CITATION_TAG ) . However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans. In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime."], "CC16": ["The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #CITATION_TAG ) .", "confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials). The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #CITATION_TAG ) ."], "CC17": ["We use the same set of binary features as in previous work on this dataset ( #CITATION_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) . Specifically, let V = {v 1 , ...", "where f (\u2022) extracts a feature vector from a classified document, \u03b8 are the corresponding weights of those features, and Z \u03b8 (x) def = y u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset ( #CITATION_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) . Specifically, let V = {v 1 , ...", "We use the same set of binary features as in previous work on this dataset ( #CITATION_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) . Specifically, let V = {v 1 , ... , v 17744 } be the set of word types with count \u2265 4 in the full 2000-document corpus.", "where f (\u2022) extracts a feature vector from a classified document, \u03b8 are the corresponding weights of those features, and Z \u03b8 (x) def = y u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset ( #CITATION_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) . Specifically, let V = {v 1 , ... , v 17744 } be the set of word types with count \u2265 4 in the full 2000-document corpus.", "We use the same set of binary features as in previous work on this dataset ( #CITATION_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) . Specifically, let V = {v 1 , ... , v 17744 } be the set of word types with count \u2265 4 in the full 2000-document corpus. Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.", "where f (\u2022) extracts a feature vector from a classified document, \u03b8 are the corresponding weights of those features, and Z \u03b8 (x) def = y u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset ( #CITATION_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) . Specifically, let V = {v 1 , ... , v 17744 } be the set of word types with count \u2265 4 in the full 2000-document corpus. Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.", "We use the same set of binary features as in previous work on this dataset ( #CITATION_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) . Specifically, let V = {v 1 , ... , v 17744 } be the set of word types with count \u2265 4 in the full 2000-document corpus. Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise. Thus \u03b8 \u2208 R 17744 , and positive weights in \u03b8 favor class label y = +1 and equally discourage y = \u22121, while negative weights do the opposite."], "CC18": ["In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( #CITATION_TAG ) , grammatical features ( Amaya and Benedy , 2001 ) , etc \" ."], "CC19": ["Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.", "As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.", "Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.", "\u0110n both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.", "As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.", "Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling.", "Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data. \u0110n both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.", "\u0110n both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.", "As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling.", "Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling. This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn\"t too far removed from the baseline.", "Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data. \u0110n both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.", "\u0110n both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling.", "As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling. This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn\"t too far removed from the baseline.", "Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data. \u0110n both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling.", "\u0110n both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #CITATION_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling. This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn\"t too far removed from the baseline."], "CC20": ["As shown in ( #CITATION_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results.", "This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in ( #CITATION_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results.", "As shown in ( #CITATION_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results. No special effort was otherwise made in order to optimize the parameters of the classifiers.", "The code for the classifier was generously provided by Daisuke Okanohara. This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in ( #CITATION_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results.", "This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in ( #CITATION_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results. No special effort was otherwise made in order to optimize the parameters of the classifiers.", "For our features we used large-margin classifiers trained using the online algorithm described in (Crammer et al., 2006). The code for the classifier was generously provided by Daisuke Okanohara. This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in ( #CITATION_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results.", "The code for the classifier was generously provided by Daisuke Okanohara. This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in ( #CITATION_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results. No special effort was otherwise made in order to optimize the parameters of the classifiers."], "CC21": ["Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #CITATION_TAG ] , which is still considered one of the best smoothing methods for n-gram language models . 2.", "Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #CITATION_TAG ] , which is still considered one of the best smoothing methods for n-gram language models . 2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.", "Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #CITATION_TAG ] , which is still considered one of the best smoothing methods for n-gram language models . 2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained. A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled."], "CC22": ["The features can be easily obtained by modifying the TAT extraction algorithm described in ( #CITATION_TAG ) . When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.", "The features can be easily obtained by modifying the TAT extraction algorithm described in ( #CITATION_TAG ) . When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately."], "CC23": ["For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #CITATION_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 )", "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #CITATION_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 )", "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #CITATION_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006)."], "CC24": ["For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #CITATION_TAG ) and history-based parsing ( Nivre and McDonald , 2008 )", "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #CITATION_TAG ) and history-based parsing ( Nivre and McDonald , 2008 )", "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #CITATION_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006)."], "CC25": ["For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #CITATION_TAG )", "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #CITATION_TAG )", "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #CITATION_TAG ) We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006)."], "CC26": ["We could also introduce new variables , e.g. , nonterminal refinements ( #CITATION_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .", "For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005;Huang, 2008) and history-based parsing (Nivre and McDonald, 2008). We could also introduce new variables , e.g. , nonterminal refinements ( #CITATION_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) ."], "CC27": ["Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #CITATION_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses . When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date."], "CC28": ["The first direct application of parse forest in translation is our previous work ( #CITATION_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below )", "The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007). The first direct application of parse forest in translation is our previous work ( #CITATION_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below )", "The first direct application of parse forest in translation is our previous work ( #CITATION_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding."], "CC29": ["Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #CITATION_TAG ) , and Machine Translation ( Boas 2002 ) . With the efforts of many researchers (Carreras and M\u00e0rquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.", "The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #CITATION_TAG ) , and Machine Translation ( Boas 2002 ) . With the efforts of many researchers (Carreras and M\u00e0rquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.", "The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #CITATION_TAG ) , and Machine Translation ( Boas 2002 ) . With the efforts of many researchers (Carreras and M\u00e0rquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast."], "CC30": ["To prove that our method is effective , we also make a comparison between the performances of our system and #CITATION_TAG , Xue ( 2008 ) . Xue (2008) is the best SRL system until now and it has the same data setting with ours.", "To prove that our method is effective , we also make a comparison between the performances of our system and #CITATION_TAG , Xue ( 2008 ) . Xue (2008) is the best SRL system until now and it has the same data setting with ours. The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.", "To prove that our method is effective , we also make a comparison between the performances of our system and #CITATION_TAG , Xue ( 2008 ) . Xue (2008) is the best SRL system until now and it has the same data setting with ours. The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing. From the table 6, we can find that our system is better than both of the related systems."], "CC32": ["The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #CITATION_TAG ) ."], "CC33": ["#CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling", "So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling", "#CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.", "However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling", "So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.", "#CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial.", "Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling", "However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.", "So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial.", "#CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.", "Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.", "However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial.", "So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.", "Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial.", "However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #CITATION_TAG did very encouraging work on the feature calibration of semantic role labeling They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings."], "CC34": ["After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.", "This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.", "After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.", "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.", "This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.", "After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.", "Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.", "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.", "This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.", "After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.", "Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.", "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.", "This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.", "Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.", "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank ( #CITATION_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL . Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling."], "CC35": ["Experiments on Chinese SRL ( #CITATION_TAG , Xue 2008 ) reassured these findings .", "For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( #CITATION_TAG , Xue 2008 ) reassured these findings .", "They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( #CITATION_TAG , Xue 2008 ) reassured these findings ."], "CC36": ["#CITATION_TAG has built a semantic role classifier exploiting the interdependence of semantic roles", "#CITATION_TAG has built a semantic role classifier exploiting the interdependence of semantic roles It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.", "#CITATION_TAG has built a semantic role classifier exploiting the interdependence of semantic roles It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features. Se- mantic context features indicates the features ex- tracted from the arguments around the current one."], "CC37": ["#CITATION_TAG has made the first attempt working on the single semantic role level to make further improvement . However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank.", "Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained. #CITATION_TAG has made the first attempt working on the single semantic role level to make further improvement . However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank.", "#CITATION_TAG has made the first attempt working on the single semantic role level to make further improvement . However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. What if we could extend the idea of hierarchical architecture to the single semantic role level?", "Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained. #CITATION_TAG has made the first attempt working on the single semantic role level to make further improvement . However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. What if we could extend the idea of hierarchical architecture to the single semantic role level?", "#CITATION_TAG has made the first attempt working on the single semantic role level to make further improvement . However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. What if we could extend the idea of hierarchical architecture to the single semantic role level? Would that help the improvement of SRC?"], "CC39": ["To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #CITATION_TAG . Xue (2008) is the best SRL system until now and it has the same data setting with ours.", "To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #CITATION_TAG . Xue (2008) is the best SRL system until now and it has the same data setting with ours. The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.", "To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #CITATION_TAG . Xue (2008) is the best SRL system until now and it has the same data setting with ours. The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing. From the table 6, we can find that our system is better than both of the related systems."], "CC40": ["be found in figure 2 , which is similar with that in #CITATION_TAG .", "The architectures of hierarchical semantic role classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g. phrase type, are limited. be found in figure 2 , which is similar with that in #CITATION_TAG .", "However, in this paper, we did SRC in two steps. The architectures of hierarchical semantic role classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g. phrase type, are limited. be found in figure 2 , which is similar with that in #CITATION_TAG ."], "CC41": [], "CC42": ["Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #CITATION_TAG ) . With the efforts of many researchers (Carreras and M\u00e0rquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.", "The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #CITATION_TAG ) . With the efforts of many researchers (Carreras and M\u00e0rquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.", "The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #CITATION_TAG ) . With the efforts of many researchers (Carreras and M\u00e0rquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast."], "CC43": ["We use the same data setting with #CITATION_TAG , however a bit different from Xue and Palmer ( 2005 ) .", "The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with #CITATION_TAG , however a bit different from Xue and Palmer ( 2005 ) .", "fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with #CITATION_TAG , however a bit different from Xue and Palmer ( 2005 ) ."], "CC44": ["Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #CITATION_TAG . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.", "Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #CITATION_TAG . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.", "Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #CITATION_TAG . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.", "Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #CITATION_TAG . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.", "Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #CITATION_TAG . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results.", "Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #CITATION_TAG . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results.", "Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #CITATION_TAG . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL."], "CC45": ["Experiments on Chinese SRL ( Xue and Palmer 2005 , #CITATION_TAG ) reassured these findings .", "For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( Xue and Palmer 2005 , #CITATION_TAG ) reassured these findings .", "They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( Xue and Palmer 2005 , #CITATION_TAG ) reassured these findings ."], "CC46": ["#CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.", "After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.", "#CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.", "This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.", "After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.", "#CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.", "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.", "This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.", "After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.", "#CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.", "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.", "This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.", "After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.", "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.", "This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL. #CITATION_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling."], "CC47": ["Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #CITATION_TAG and Xue ( 2008 ) . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.", "Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #CITATION_TAG and Xue ( 2008 ) . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.", "Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #CITATION_TAG and Xue ( 2008 ) . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.", "Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #CITATION_TAG and Xue ( 2008 ) . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.", "Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #CITATION_TAG and Xue ( 2008 ) . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results.", "Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #CITATION_TAG and Xue ( 2008 ) . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results.", "Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #CITATION_TAG and Xue ( 2008 ) . Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL."], "CC48": ["We use the same data setting with Xue ( 2008 ) , however a bit different from #CITATION_TAG .", "The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with Xue ( 2008 ) , however a bit different from #CITATION_TAG .", "fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with Xue ( 2008 ) , however a bit different from #CITATION_TAG ."], "CC50": ["The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #CITATION_TAG )", "The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #CITATION_TAG ) It is constituted of two parts.", "The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #CITATION_TAG ) It is constituted of two parts. One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank."], "CC51": ["Semantic Role labeling ( SRL ) was first defined in #CITATION_TAG . The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.", "Semantic Role labeling ( SRL ) was first defined in #CITATION_TAG . The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.", "Semantic Role labeling ( SRL ) was first defined in #CITATION_TAG . The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation (Boas 2002)."], "CC52": ["In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.", "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features.", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features.", "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #CITATION_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW)."], "CC53": ["In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.", "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features.", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features.", "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #CITATION_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW)."], "CC55": ["Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( #CITATION_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results."], "CC56": ["Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( Cucerzan , 2007 ; #CITATION_TAG ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results."], "CC57": ["It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #CITATION_TAG ) .", "Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #CITATION_TAG ) ."], "CC58": [], "CC60": ["It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #CITATION_TAG ; Artiles et al. , 2007 ) .", "Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #CITATION_TAG ; Artiles et al. , 2007 ) ."], "CC62": ["Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #CITATION_TAG ) . Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results."], "CC63": ["A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #CITATION_TAG ) . According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (Artiles et al., 2005).", "A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #CITATION_TAG ) . According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (Artiles et al., 2005). As the amount of information in the WWW grows, more of these people are mentioned in different web pages."], "CC64": ["#CITATION_TAG compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. #CITATION_TAG compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "#CITATION_TAG compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. #CITATION_TAG compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. #CITATION_TAG compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. #CITATION_TAG compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007)- . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. #CITATION_TAG compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW)."], "CC65": ["Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #CITATION_TAG ; Gooi and Allan , 2004 ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text."], "CC67": ["According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #CITATION_TAG ) . As the amount of information in the WWW grows, more of these people are mentioned in different web pages.", "A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (Spink et al., 2004). According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #CITATION_TAG ) . As the amount of information in the WWW grows, more of these people are mentioned in different web pages.", "According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #CITATION_TAG ) . As the amount of information in the WWW grows, more of these people are mentioned in different web pages. Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned."], "CC68": ["Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-. Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers ( #CITATION_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results."], "CC69": ["The Web People Search task , as defined in the first WePS evaluation campaign ( #CITATION_TAG ) , consists of grouping search results for a given name according to the different people that share it .", "In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name. The Web People Search task , as defined in the first WePS evaluation campaign ( #CITATION_TAG ) , consists of grouping search results for a given name according to the different people that share it .", "The user might refine the original query with additional terms, but this risks excluding relevant documents in the process. In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name. The Web People Search task , as defined in the first WePS evaluation campaign ( #CITATION_TAG ) , consists of grouping search results for a given name according to the different people that share it ."], "CC70": [], "CC71": ["In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.", "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features.", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features.", "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).", "The most used feature for the Web People Search task, however, are NEs. Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach.", "Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #CITATION_TAG ; Kalashnikov et al. , 2007 ) . For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. Saggion ( 2008) compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW)."], "CC72": ["The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #CITATION_TAG ) . Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature."], "CC73": ["Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc. In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #CITATION_TAG ) . Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.", "This makes NEs the second most common type of feature; only the BoW feature was more popular. Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc. In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #CITATION_TAG ) . Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.", "Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc. In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #CITATION_TAG ) . Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams. But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.", "Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation. This makes NEs the second most common type of feature; only the BoW feature was more popular. Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc. In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #CITATION_TAG ) . Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.", "This makes NEs the second most common type of feature; only the BoW feature was more popular. Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc. In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #CITATION_TAG ) . Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams. But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.", "Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc. In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #CITATION_TAG ) . Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams. But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before. In the next Section we describe this dataset and how it has been adapted for our purposes.", "Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation. This makes NEs the second most common type of feature; only the BoW feature was more popular. Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc. In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #CITATION_TAG ) . Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams. But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.", "This makes NEs the second most common type of feature; only the BoW feature was more popular. Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc. In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #CITATION_TAG ) . Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams. But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before. In the next Section we describe this dataset and how it has been adapted for our purposes."], "CC74": ["Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #CITATION_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way."], "CC75": ["Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.", "Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text.", "The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #CITATION_TAG ) - . Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007). Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way."], "CC77": ["Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #CITATION_TAG ; Bergsma et al. , 2008 ) . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.", "Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes. Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #CITATION_TAG ; Bergsma et al. , 2008 ) . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation."], "CC78": ["Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #CITATION_TAG ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case."], "CC79": ["Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #CITATION_TAG ; Huang , 2008 ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case."], "CC80": ["Self-training should also benefit other discriminatively trained parsers with latent annotations ( #CITATION_TAG ) , although training would be much slower compared to using generative models , as in our case .", "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations ( #CITATION_TAG ) , although training would be much slower compared to using generative models , as in our case ."], "CC82": [], "CC83": [], "CC84": [], "CC85": ["When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #CITATION_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .", "To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #CITATION_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage ."], "CC86": ["The reordering models we describe follow our previous work using function word models for translation ( #CITATION_TAG ; Setiawan et al. , 2009 ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.", "The reordering models we describe follow our previous work using function word models for translation ( #CITATION_TAG ; Setiawan et al. , 2009 ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases."], "CC87": ["With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #CITATION_TAG ) . However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model.", "These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #CITATION_TAG ) . However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model."], "CC88": ["To model o ( Li , S \u00e2\\x86\\x92 T ) , o ( Ri , S \u00e2\\x86\\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #CITATION_TAG . Formally, this model takes the form of probability distribution P ori (o(L i,S\u2192T ), o(R i,S\u2192T )|Y i,S\u2192T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).", "To model o ( Li , S \u00e2\\x86\\x92 T ) , o ( Ri , S \u00e2\\x86\\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #CITATION_TAG . Formally, this model takes the form of probability distribution P ori (o(L i,S\u2192T ), o(R i,S\u2192T )|Y i,S\u2192T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases). In particular, o maps the reordering into one of the following four orientation values (borrowed from Nagata et al. (2006)) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG).", "To model o ( Li , S \u00e2\\x86\\x92 T ) , o ( Ri , S \u00e2\\x86\\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #CITATION_TAG . Formally, this model takes the form of probability distribution P ori (o(L i,S\u2192T ), o(R i,S\u2192T )|Y i,S\u2192T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases). In particular, o maps the reordering into one of the following four orientation values (borrowed from Nagata et al. (2006)) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG). The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro-This heuristic is commonly used in learning phrase pairs from parallel text."], "CC89": ["The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #CITATION_TAG ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.", "The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #CITATION_TAG ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases."], "CC90": [], "CC91": ["In our previous work ( #CITATION_TAG ) , we started an initial investigation on conversation entailment", "In our previous work ( #CITATION_TAG ) , we started an initial investigation on conversation entailment We have collected a dataset of 875 instances.", "In our previous work ( #CITATION_TAG ) , we started an initial investigation on conversation entailment We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1)."], "CC93": ["Note that in our original work ( #CITATION_TAG ) , only development data were used to show some initial observations."], "CC94": ["This alignment is obtained by following the same set of rules learned from the development dataset as in ( #CITATION_TAG ) .", "Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in ( #CITATION_TAG ) .", "Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in ( #CITATION_TAG ) ."], "CC95": ["To address this limitation , our previous work ( #CITATION_TAG ) has initiated an investigation on the problem of conversation entailment . The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.", "To address this limitation , our previous work ( #CITATION_TAG ) has initiated an investigation on the problem of conversation entailment . The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot. While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data."], "CC96": ["In our previous work ( #CITATION_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 \u00e2\\x88\u00a7 ...", "In our previous work ( #CITATION_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 \u00e2\\x88\u00a7 ... \u00e2\\x88\u00a7 dm , and a hypothesis H represented by another set of clauses H = h1 \u00e2\\x88\u00a7 ...", "In our previous work ( #CITATION_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 \u00e2\\x88\u00a7 ... \u00e2\\x88\u00a7 dm , and a hypothesis H represented by another set of clauses H = h1 \u00e2\\x88\u00a7 ... \u00e2\\x88\u00a7 hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ..."], "CC97": ["We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.", "Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.", "We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).", "Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.", "Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).", "We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).", "Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data). Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.", "Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).", "Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).", "We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs.", "Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data). Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).", "Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).", "Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs.", "Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data). Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).", "Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #CITATION_TAG , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs."], "CC98": ["This revalidates the observation of #CITATION_TAG that phrase structure representations and dependency representations add complimentary value to the learning task", "Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best. This revalidates the observation of #CITATION_TAG that phrase structure representations and dependency representations add complimentary value to the learning task", "This revalidates the observation of #CITATION_TAG that phrase structure representations and dependency representations add complimentary value to the learning task We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks.", "Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information. Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best. This revalidates the observation of #CITATION_TAG that phrase structure representations and dependency representations add complimentary value to the learning task", "Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best. This revalidates the observation of #CITATION_TAG that phrase structure representations and dependency representations add complimentary value to the learning task We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks.", "Through oversampling we achieve an increase in F1-measure of 22.2% absolute over a baseline system. Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information. Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best. This revalidates the observation of #CITATION_TAG that phrase structure representations and dependency representations add complimentary value to the learning task", "Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information. Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best. This revalidates the observation of #CITATION_TAG that phrase structure representations and dependency representations add complimentary value to the learning task We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks."], "CC99": ["Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data", "As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data", "Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations.", "As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data", "As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations.", "Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.", "absolute. As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data", "As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations.", "As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.", "Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger. This means that implicit feature space is much sparser and thus not the best representation.", "absolute. As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations.", "As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.", "As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger. This means that implicit feature space is much sparser and thus not the best representation.", "absolute. As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.", "As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #CITATION_TAG ) where GR performed much worse than PET for ACE data This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger. This means that implicit feature space is much sparser and thus not the best representation."], "CC100": [], "CC101": ["For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.", "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.", "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.", "Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.", "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.", "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.", "Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.", "Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.", "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.", "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.", "Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.", "Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.", "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.", "Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.", "Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #CITATION_TAG ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse."], "CC104": ["This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #CITATION_TAG ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.", "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #CITATION_TAG ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.", "This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #CITATION_TAG ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.", "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #CITATION_TAG ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.", "This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #CITATION_TAG ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing.", "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #CITATION_TAG ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing.", "This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #CITATION_TAG ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing. Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training."], "CC105": ["We use the non-projective k-best MST algorithm to generate k-best lists ( #CITATION_TAG ) , where k = 8 for the experiments in this paper . The graphbased parser features used in the experiments in this paper are defined over a word, w i at position i; the head of this word w \u03c1(i) where \u03c1(i) provides the index of the head word; and partof-speech tags of these words t i", "\u2022 Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (McDonald et al., 2005). We use the non-projective k-best MST algorithm to generate k-best lists ( #CITATION_TAG ) , where k = 8 for the experiments in this paper . The graphbased parser features used in the experiments in this paper are defined over a word, w i at position i; the head of this word w \u03c1(i) where \u03c1(i) provides the index of the head word; and partof-speech tags of these words t i", "We use the non-projective k-best MST algorithm to generate k-best lists ( #CITATION_TAG ) , where k = 8 for the experiments in this paper . The graphbased parser features used in the experiments in this paper are defined over a word, w i at position i; the head of this word w \u03c1(i) where \u03c1(i) provides the index of the head word; and partof-speech tags of these words t i We use the following set of features similar to McDonald et al. ( 2005):"], "CC106": ["#CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.", "Consider, for example, the case of questions. #CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.", "#CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree.", "Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data. Consider, for example, the case of questions. #CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.", "Consider, for example, the case of questions. #CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree.", "#CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree. Root-F1 scores from Table 2 suggest that one simple question is \"what is the main verb of this sentence?", "Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data. Consider, for example, the case of questions. #CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree.", "Consider, for example, the case of questions. #CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree. Root-F1 scores from Table 2 suggest that one simple question is \"what is the main verb of this sentence?", "#CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree. Root-F1 scores from Table 2 suggest that one simple question is \"what is the main verb of this sentence? for sentences that are questions.", "Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data. Consider, for example, the case of questions. #CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree. Root-F1 scores from Table 2 suggest that one simple question is \"what is the main verb of this sentence?", "Consider, for example, the case of questions. #CITATION_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree. Root-F1 scores from Table 2 suggest that one simple question is \"what is the main verb of this sentence? for sentences that are questions."], "CC107": [], "CC110": ["The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings", "The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings", "The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.", "In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings", "The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.", "The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence.", "A recent study by  also investigates the task of training parsers to improve MT reordering. In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings", "In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.", "The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence.", "The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions.", "A recent study by  also investigates the task of training parsers to improve MT reordering. In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.", "In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence.", "The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions.", "A recent study by  also investigates the task of training parsers to improve MT reordering. In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence.", "In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #CITATION_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions."], "CC111": ["Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #CITATION_TAG ) and is simpler to measure . Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates.", "We use a reordering score based on the reordering penalty from the METEOR scoring metric. Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #CITATION_TAG ) and is simpler to measure . Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates."], "CC113": ["\u2022 Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #CITATION_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.", "\u2022 Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #CITATION_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists. The features used by all models are: the part-ofspeech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available)."], "CC114": ["In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 )", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 )", "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 )", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #CITATION_TAG ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model."], "CC115": ["In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #CITATION_TAG )"], "CC117": ["The work that is most similar to ours is that of #CITATION_TAG , who introduced the Constraint Driven Learning algorithm ( CODL )", "The work that is most similar to ours is that of #CITATION_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).", "The work that is most similar to ours is that of #CITATION_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets). For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints."], "CC118": ["#CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.", "Furthermore, we also evaluate the method on alternate extrinsic loss functions. #CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.", "#CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.", "This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.", "Furthermore, we also evaluate the method on alternate extrinsic loss functions. #CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.", "#CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.", "Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.", "This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.", "Furthermore, we also evaluate the method on alternate extrinsic loss functions. #CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.", "Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.", "This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #CITATION_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives."], "CC119": ["A recent study by #CITATION_TAG also investigates the task of training parsers to improve MT reordering . In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.", "A recent study by #CITATION_TAG also investigates the task of training parsers to improve MT reordering . In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.", "A recent study by #CITATION_TAG also investigates the task of training parsers to improve MT reordering . In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings."], "CC121": ["Identical to the standard perceptron proof , e.g. , #CITATION_TAG , by inserting in loss-separability for normal separability .", "If training is run indefinitely, then m \u2264 R 2 \u03b3 2 . Proof. Identical to the standard perceptron proof , e.g. , #CITATION_TAG , by inserting in loss-separability for normal separability .", "Let m be the number of mistakes made when training the perceptron (Algorithm 2) with inline ranker loss (Algorithm 3) on D, where a mistake occurs for (x, y) \u2208 D with parameter vector \u03b8 when \u2203\u0177 j \u2208 F k-best \u03b8 (x) wher\u00ea y j =\u0177 1 and L(\u0177 j , y) < L(\u0177 1 , y). If training is run indefinitely, then m \u2264 R 2 \u03b3 2 . Proof. Identical to the standard perceptron proof , e.g. , #CITATION_TAG , by inserting in loss-separability for normal separability ."], "CC122": ["Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #CITATION_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).", "The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #CITATION_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).", "The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #CITATION_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010)."], "CC123": ["In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 )", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 )", "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 )", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #CITATION_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model."], "CC124": ["One obvious approach to this problem is to employ parser reranking ( #CITATION_TAG ) . In such a setting, an auxiliary reranker is added in a pipeline following the parser.", "One obvious approach to this problem is to employ parser reranking ( #CITATION_TAG ) . In such a setting, an auxiliary reranker is added in a pipeline following the parser. The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).", "One obvious approach to this problem is to employ parser reranking ( #CITATION_TAG ) . In such a setting, an auxiliary reranker is added in a pipeline following the parser. The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework). The reranker can then be trained to optimize for the downstream or extrinsic objective."], "CC125": ["Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #CITATION_TAG ) .", "The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #CITATION_TAG ) .", "The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #CITATION_TAG ) ."], "CC126": ["This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #CITATION_TAG ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.", "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #CITATION_TAG ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.", "This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #CITATION_TAG ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.", "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #CITATION_TAG ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.", "This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #CITATION_TAG ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing.", "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #CITATION_TAG ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing.", "This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #CITATION_TAG ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing. Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training."], "CC128": ["This includes work on generalized expectation ( #CITATION_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.", "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( #CITATION_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.", "This includes work on generalized expectation ( #CITATION_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.", "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( #CITATION_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.", "This includes work on generalized expectation ( #CITATION_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing.", "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( #CITATION_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing.", "This includes work on generalized expectation ( #CITATION_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) . The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing. Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training."], "CC129": ["For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #CITATION_TAG ) .", "We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score. For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #CITATION_TAG ) .", "Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains. We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score. For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #CITATION_TAG ) ."], "CC130": ["In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG )", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG )", "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG )", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #CITATION_TAG ) But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model."], "CC131": ["This includes work on question answering ( #CITATION_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks . In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering ( #CITATION_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks . In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).", "This includes work on question answering ( #CITATION_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks . In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering ( #CITATION_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks . In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees.", "This includes work on question answering ( #CITATION_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks . In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering ( #CITATION_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks . In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree.", "This includes work on question answering ( #CITATION_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks . In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model."], "CC132": [], "CC133": ["In this paper , inspired by KNN-SVM ( #CITATION_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems . Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.", "In this paper , inspired by KNN-SVM ( #CITATION_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems . Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing. This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.", "In this paper , inspired by KNN-SVM ( #CITATION_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems . Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing. This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences. Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency."], "CC134": ["Motivated by (#CITATION_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\u2212WbII2+ A \ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier.", "Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here. Motivated by (#CITATION_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\u2212WbII2+ A \ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier.", "Motivated by (#CITATION_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\u2212WbII2+ A \ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\u03b1W \u00b7 h(fj, e)] P\u03b1(e|fj; W) = (7) Ee\"Ec; exp[\u03b1W \u00b7 h(fj, e\")], where \u03b1 > 0 is a real number valued smoother.", "Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here. Motivated by (#CITATION_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\u2212WbII2+ A \ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\u03b1W \u00b7 h(fj, e)] P\u03b1(e|fj; W) = (7) Ee\"Ec; exp[\u03b1W \u00b7 h(fj, e\")], where \u03b1 > 0 is a real number valued smoother.", "Motivated by (#CITATION_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\u2212WbII2+ A \ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\u03b1W \u00b7 h(fj, e)] P\u03b1(e|fj; W) = (7) Ee\"Ec; exp[\u03b1W \u00b7 h(fj, e\")], where \u03b1 > 0 is a real number valued smoother. One can see that, in the extreme case, for \u03b1 \u2014* oc, (6) converges to (5)."], "CC135": [], "CC136": [], "CC137": ["Actually , if we use LSH technique ( #CITATION_TAG ) in retrieval process , the local method can be easily scaled to a larger training data .", "Further, compared to the retrieval, the local training is not the bottleneck. Actually , if we use LSH technique ( #CITATION_TAG ) in retrieval process , the local method can be easily scaled to a larger training data .", "This shows that the local method is efficient. Further, compared to the retrieval, the local training is not the bottleneck. Actually , if we use LSH technique ( #CITATION_TAG ) in retrieval process , the local method can be easily scaled to a larger training data ."], "CC138": ["(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #CITATION_TAG ) employed an evaluation metric as a loss function and directly optimized it.", "(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #CITATION_TAG ) employed an evaluation metric as a loss function and directly optimized it.", "(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #CITATION_TAG ) employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza- tion objectives by introducing a margin-based and ranking-based indirect loss functions.", "Several works have proposed discriminative tech- niques to train log-linear model for SMT. (Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #CITATION_TAG ) employed an evaluation metric as a loss function and directly optimized it.", "(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #CITATION_TAG ) employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza- tion objectives by introducing a margin-based and ranking-based indirect loss functions."], "CC139": ["The local training method ( #CITATION_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) . Compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example."], "CC140": ["h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #CITATION_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #CITATION_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #CITATION_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #CITATION_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #CITATION_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #CITATION_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #CITATION_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline."], "CC142": ["We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #CITATION_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) . In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.", "We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #CITATION_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) . In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.", "We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #CITATION_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) . In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a. pl as the evaluation tool.", "We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #CITATION_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) . In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a. pl as the evaluation tool.", "We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #CITATION_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) . In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a. pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004)."], "CC143": ["h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #CITATION_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #CITATION_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #CITATION_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #CITATION_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #CITATION_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #CITATION_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #CITATION_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline."], "CC144": ["In the field of machine learning research , incremental training has been employed in the work ( #CITATION_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation . The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.", "Compared with retraining mode, incremental training can improve the training efficiency. In the field of machine learning research , incremental training has been employed in the work ( #CITATION_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation . The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.", "In the field of machine learning research , incremental training has been employed in the work ( #CITATION_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation . The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable. In this section, we will investigate the incremental trainingmethodsinSMTscenario."], "CC145": ["( Och , 2003 ; Moore and Quirk , 2008 ; #CITATION_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it . (Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.", "(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; #CITATION_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it . (Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."], "CC146": ["( Och , 2003 ; #CITATION_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it . (Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.", "(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; #CITATION_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it . (Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."], "CC147": ["The significance testing is performed by paired bootstrap re-sampling ( #CITATION_TAG ) .", "pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling ( #CITATION_TAG ) .", "In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a. pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling ( #CITATION_TAG ) ."], "CC148": ["The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#CITATION_TAG, 2011).", "(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#CITATION_TAG, 2011).", "The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#CITATION_TAG, 2011). Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as \u201d\u2022\u201d, and (\u22121, 0) corresponds to a negative example denoted as \u201d*\u201d.", "(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#CITATION_TAG, 2011). Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as \u201d\u2022\u201d, and (\u22121, 0) corresponds to a negative example denoted as \u201d*\u201d.", "The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#CITATION_TAG, 2011). Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as \u201d\u2022\u201d, and (\u22121, 0) corresponds to a negative example denoted as \u201d*\u201d. Since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.", "(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#CITATION_TAG, 2011). Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as \u201d\u2022\u201d, and (\u22121, 0) corresponds to a negative example denoted as \u201d*\u201d. Since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.", "The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#CITATION_TAG, 2011). Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as \u201d\u2022\u201d, and (\u22121, 0) corresponds to a negative example denoted as \u201d*\u201d. Since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile. However, one can obtain e11 and e21 with weights: (1, 1) and (\u22121, 1), respectively."], "CC149": ["( #CITATION_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.", "Several works have proposed discriminative techniques to train log-linear model for SMT. ( #CITATION_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.", "( #CITATION_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. (Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.", "Several works have proposed discriminative techniques to train log-linear model for SMT. ( #CITATION_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. (Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.", "( #CITATION_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. (Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it. (Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."], "CC150": ["To retrieve translation examples for a test sentence , ( #CITATION_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch \u00c2\u00a8 utze , 1999 ) as follows :"], "CC151": ["h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #CITATION_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #CITATION_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #CITATION_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #CITATION_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #CITATION_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #CITATION_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #CITATION_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline."], "CC152": ["We employ the idea of ultraconservative update ( #CITATION_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .", "Our goal is to find an optimal weight, denoted by W i , which is a local weight and used for decoding the sentence t i . Unlike the global method which performs tuning on the whole development set Dev + D i as in Algorithm 1, W i can be incrementally learned by optimizing on D i based on W b We employ the idea of ultraconservative update ( #CITATION_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows ."], "CC153": ["( #CITATION_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it . (Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.", "(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT. ( #CITATION_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it . (Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."], "CC154": ["We use an in-house developed hierarchical phrase-based translation ( #CITATION_TAG ) as our baseline system , and we denote it as In-Hiero . To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-    (Koehn et al., 2007).", "We use an in-house developed hierarchical phrase-based translation ( #CITATION_TAG ) as our baseline system , and we denote it as In-Hiero . To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-    (Koehn et al., 2007). Both of these systems are with default setting."], "CC155": ["We run GIZA + + ( #CITATION_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair", "We run GIZA + + ( #CITATION_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).", "We run GIZA + + ( #CITATION_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a."], "CC156": ["( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #CITATION_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .", "(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it. ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #CITATION_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .", "(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT. (Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it. ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #CITATION_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions ."], "CC157": ["h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #CITATION_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #CITATION_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #CITATION_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #CITATION_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #CITATION_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.", "where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #CITATION_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.", "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #CITATION_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline."], "CC158": ["Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #CITATION_TAG ) . However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to  the situation where the agent\"s representation of the shared world is problematic and full of mistakes."], "CC160": ["How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #CITATION_TAG ) . In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.", "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #CITATION_TAG ) . In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue.", "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #CITATION_TAG ) . In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans."], "CC161": ["In a similar vein , #CITATION_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task . Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein , #CITATION_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task . Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "In a similar vein , #CITATION_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task . Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.", "Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein , #CITATION_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task . Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein , #CITATION_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task . Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.", "Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein , #CITATION_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task . Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein , #CITATION_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task . Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity."], "CC163": ["They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.", "The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.", "They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.", "As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.", "The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.", "They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.", "As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.", "The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.", "They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.", "As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.", "The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words ( BoVW ) model ( #CITATION_TAG ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms."], "CC164": ["More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #CITATION_TAG ) , act as excellent substitutes for feature Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).", "Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #CITATION_TAG ) , act as excellent substitutes for feature Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).", "Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #CITATION_TAG ) , act as excellent substitutes for feature Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013)."], "CC165": ["Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #CITATION_TAG ; Roller et al. , 2012 ) .", "Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #CITATION_TAG ; Roller et al. , 2012 ) .", "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #CITATION_TAG ; Roller et al. , 2012 ) ."], "CC166": [], "CC167": ["That is, we simply take the original mLDA model of #CITATION_TAG (2009) and generalize it in the same way they generalize LDA."], "CC169": ["#CITATION_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus . In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics."], "CC171": ["This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #CITATION_TAG ) .", "The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #CITATION_TAG ) ."], "CC172": ["Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #CITATION_TAG ) .", "More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #CITATION_TAG ) .", "Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #CITATION_TAG ) ."], "CC173": ["Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #CITATION_TAG ) .", "Some works abstract perception via the us- age of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholin- guistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #CITATION_TAG ) ."], "CC174": ["Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #CITATION_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) . Some efforts have tackled tasks such as automatic image caption generation (Feng and La- pata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identifica- tion of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012).", "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #CITATION_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) . Some efforts have tackled tasks such as automatic image caption generation (Feng and La- pata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identifica- tion of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012)."], "CC175": ["Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #CITATION_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .", "Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #CITATION_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .", "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #CITATION_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) ."], "CC176": [], "CC177": ["We use the same method as #CITATION_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word \"s feature distribution , creating a word-feature pair . Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.", "In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities. We use the same method as #CITATION_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word \"s feature distribution , creating a word-feature pair . Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.", "We use the same method as #CITATION_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word \"s feature distribution , creating a word-feature pair . Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair. 5 That is, for the feature norm modal- ity, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc. The resulting stochastically generated corpus is used in its corresponding experiments."], "CC179": ["Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #CITATION_TAG . Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).", "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #CITATION_TAG . Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).", "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #CITATION_TAG . Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009)."], "CC180": ["This seems to provide additional evidence of #CITATION_TAGb ) \"s suggestion that something like a distributional hypothesis of images is plausible .", "Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of #CITATION_TAGb ) \"s suggestion that something like a distributional hypothesis of images is plausible .", "The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model. Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of #CITATION_TAGb ) \"s suggestion that something like a distributional hypothesis of images is plausible ."], "CC181": ["It is frequently used in tasks like scene identification , and #CITATION_TAG shows that distance in GIST space correlates well with semantic distance in WordNet . After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.", "The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification , and #CITATION_TAG shows that distance in GIST space correlates well with semantic distance in WordNet . After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.", "Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification , and #CITATION_TAG shows that distance in GIST space correlates well with semantic distance in WordNet . After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality."], "CC183": ["Some efforts have tackled tasks such as automatic image caption generation ( #CITATION_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .", "Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( #CITATION_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .", "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( #CITATION_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) ."], "CC184": ["For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #CITATION_TAG ) containing approximately 1.7 B word tokens", "For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #CITATION_TAG ) containing approximately 1.7 B word tokens We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100."], "CC185": ["To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #CITATION_TAG show that verb clusters can be used to improve activity recognition in videos ."], "CC186": ["#CITATION_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity . Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.", "In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. #CITATION_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity . Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.", "Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. #CITATION_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity . Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity."], "CC187": ["Following #CITATION_TAG , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.", "Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following #CITATION_TAG , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.", "Following #CITATION_TAG , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., \"cat\" is more similar to \"dog\" than 97.3% of the rest of the vocabulary.", "Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following #CITATION_TAG , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.", "Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following #CITATION_TAG , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., \"cat\" is more similar to \"dog\" than 97.3% of the rest of the vocabulary.", "Following #CITATION_TAG , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., \"cat\" is more similar to \"dog\" than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once.", "Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following #CITATION_TAG , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., \"cat\" is more similar to \"dog\" than 97.3% of the rest of the vocabulary.", "Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following #CITATION_TAG , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., \"cat\" is more similar to \"dog\" than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once."], "CC188": ["In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #CITATION_TAG", "In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #CITATION_TAG We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA."], "CC189": ["We also compute GIST vectors ( #CITATION_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) . Unlike SURF descriptors, GIST produces a single vector representation for an image.", "We also compute GIST vectors ( #CITATION_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) . Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image.", "We also compute GIST vectors ( #CITATION_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) . Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet."], "CC190": ["Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #CITATION_TAG ) .", "Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #CITATION_TAG ) .", "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #CITATION_TAG ) ."], "CC192": ["To name a few examples , Rohrbach et al. ( 2010 ) and #CITATION_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos ."], "CC193": ["ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #CITATION_TAG ) . Multiple synsets exist for each meaning of a word.", "BilderNetle (\"little ImageNet\" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings. ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #CITATION_TAG ) . Multiple synsets exist for each meaning of a word.", "ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #CITATION_TAG ) . Multiple synsets exist for each meaning of a word. For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.", "BilderNetle (\"little ImageNet\" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings. ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #CITATION_TAG ) . Multiple synsets exist for each meaning of a word. For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.", "ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #CITATION_TAG ) . Multiple synsets exist for each meaning of a word. For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral. This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet."], "CC194": ["#CITATION_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .", "Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. #CITATION_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .", "In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. #CITATION_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity ."], "CC196": ["The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #CITATION_TAG .", "We do not optimize these hyperparameters or vary them over time. The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #CITATION_TAG ."], "CC197": [], "CC198": [], "CC199": ["Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #CITATION_TAG )", "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #CITATION_TAG )", "Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #CITATION_TAG ) It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).", "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #CITATION_TAG ) It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).", "Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #CITATION_TAG ) It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).", "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #CITATION_TAG ) It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).", "Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #CITATION_TAG ) It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time."], "CC200": [], "CC201": ["Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #CITATION_TAG ; Regneri et al. , 2013 ) .", "More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #CITATION_TAG ; Regneri et al. , 2013 ) .", "Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #CITATION_TAG ; Regneri et al. , 2013 ) ."], "CC204": ["Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #CITATION_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).", "Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #CITATION_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).", "Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #CITATION_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.", "Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #CITATION_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.", "Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #CITATION_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.", "Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #CITATION_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.", "Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #CITATION_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task."], "CC205": ["To name a few examples , #CITATION_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos ."], "CC206": ["Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #CITATION_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).", "Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #CITATION_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).", "Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #CITATION_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.", "Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #CITATION_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.", "Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #CITATION_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.", "Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #CITATION_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.", "Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #CITATION_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task."], "CC207": ["#CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.", "Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. #CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.", "#CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. #CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.", "Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. #CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "#CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.", "Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. #CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.", "Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. #CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. #CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.", "Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. #CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. #CITATION_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity."], "CC208": [], "CC209": ["It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #CITATION_TAG ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).", "Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #CITATION_TAG ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).", "It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #CITATION_TAG ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time.", "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #CITATION_TAG ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).", "Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #CITATION_TAG ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time.", "It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #CITATION_TAG ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.", "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #CITATION_TAG ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time.", "Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #CITATION_TAG ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities."], "CC210": ["Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #CITATION_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .", "Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #CITATION_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .", "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #CITATION_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) ."], "CC211": ["Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #CITATION_TAG ) . Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a;Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012).", "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #CITATION_TAG ) . Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a;Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012)."], "CC212": [], "CC213": ["To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #CITATION_TAG ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.", "To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #CITATION_TAG ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.", "To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #CITATION_TAG ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set."], "CC214": [], "CC215": [], "CC216": ["This result is consistent with other works using this model with these features ( #CITATION_TAG ; Silberer and Lapata , 2012 ) .", "The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features ( #CITATION_TAG ; Silberer and Lapata , 2012 ) ."], "CC217": [], "CC218": [], "CC219": ["Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.", "Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.", "Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.", "Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.", "Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.", "Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.", "Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.", "Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.", "Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.", "Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.", "Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.", "Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #CITATION_TAG ) in the prediction of association norms . Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity."], "CC220": ["To solve these scaling issues , we implement Online Variational Bayesian Inference ( #CITATION_TAG ; Hoffman et al. , 2012 ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.", "To solve these scaling issues , we implement Online Variational Bayesian Inference ( #CITATION_TAG ; Hoffman et al. , 2012 ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.", "To solve these scaling issues , we implement Online Variational Bayesian Inference ( #CITATION_TAG ; Hoffman et al. , 2012 ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set."], "CC221": ["The first work to do this with topic models is #CITATION_TAGb )", "As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is #CITATION_TAGb )", "The first work to do this with topic models is #CITATION_TAGb ) They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.", "As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is #CITATION_TAGb ) They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.", "The first work to do this with topic models is #CITATION_TAGb ) They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.", "As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is #CITATION_TAGb ) They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.", "The first work to do this with topic models is #CITATION_TAGb ) They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation."], "CC222": ["This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.", "The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.", "This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.", "In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.", "The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.", "This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features. Finally, we describe two ways to extend the model by incorporating three or more modalities.", "In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.", "The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features. Finally, we describe two ways to extend the model by incorporating three or more modalities.", "This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features. Finally, we describe two ways to extend the model by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.", "In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features. Finally, we describe two ways to extend the model by incorporating three or more modalities.", "The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #CITATION_TAG ; Silberer and Lapata , 2012 ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features. Finally, we describe two ways to extend the model by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning."], "CC223": ["This choice is motivated by an observation we made previously ( #CITATION_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3", "In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence. This choice is motivated by an observation we made previously ( #CITATION_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3"], "CC225": ["Our experimental design with professional bilingual translators follows our previous work #CITATION_TAGa ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Mart\u00ednez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013).", "However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work #CITATION_TAGa ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Mart\u00ednez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013).", "Our experimental design with professional bilingual translators follows our previous work #CITATION_TAGa ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Mart\u00ednez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature.", "The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work #CITATION_TAGa ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Mart\u00ednez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013).", "However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work #CITATION_TAGa ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Mart\u00ednez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature."], "CC226": ["This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators", "Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators", "This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others.", "In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates\" recent poll standings. Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators", "Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others.", "This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others. On the other hand, we did not obtain any significant correlation for the features proposed by Nguyen et al. (2013).", "We obtain a highly significant (p = 0.002) negative correlation between topic shift tendency of a candidate (PI) and his/her power. In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates\" recent poll standings. Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators", "In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates\" recent poll standings. Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others.", "Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others. On the other hand, we did not obtain any significant correlation for the features proposed by Nguyen et al. (2013).", "We obtain a highly significant (p = 0.002) negative correlation between topic shift tendency of a candidate (PI) and his/her power. In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates\" recent poll standings. Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others.", "In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates\" recent poll standings. Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #CITATION_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others. On the other hand, we did not obtain any significant correlation for the features proposed by Nguyen et al. (2013)."], "CC227": ["We follow our previous work ( #CITATION_TAGb ) and restrict bridging to non-coreferential cases", "Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work ( #CITATION_TAGb ) and restrict bridging to non-coreferential cases", "We follow our previous work ( #CITATION_TAGb ) and restrict bridging to non-coreferential cases We also exclude comparative anaphora (Modjeska et al., 2003).", "Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993;L\u00f6bner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work ( #CITATION_TAGb ) and restrict bridging to non-coreferential cases", "Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work ( #CITATION_TAGb ) and restrict bridging to non-coreferential cases We also exclude comparative anaphora (Modjeska et al., 2003)."], "CC228": ["mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #CITATION_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection . Some of these features overlap with the atomic features used in the rule-based system.", "All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features. mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #CITATION_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection . Some of these features overlap with the atomic features used in the rule-based system.", "9 To compare the learning-based approach to the rulebased system described in Section 3 directly, we report the mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rule-based system. All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features. mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #CITATION_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection . Some of these features overlap with the atomic features used in the rule-based system."], "CC229": ["in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ...", "The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ...", "in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i", ", dm) is equal to the joint probability of the derivation\"s tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ...", "The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i", "in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.", ", dm . Because there is a one-to-one mapping from phrase structure trees to our derivations, the probability of a derivation P(di,... , dm) is equal to the joint probability of the derivation\"s tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ...", ", dm) is equal to the joint probability of the derivation\"s tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i", "The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.", ", dm . Because there is a one-to-one mapping from phrase structure trees to our derivations, the probability of a derivation P(di,... , dm) is equal to the joint probability of the derivation\"s tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i", ", dm) is equal to the joint probability of the derivation\"s tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #CITATION_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions."], "CC230": ["The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #CITATION_TAG ) .", "Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #CITATION_TAG ) .", ", di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #CITATION_TAG ) ."], "CC231": [], "CC233": ["The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #CITATION_TAG ; Charniak , 2000 ) .", "Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #CITATION_TAG ; Charniak , 2000 ) .", ", di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #CITATION_TAG ; Charniak , 2000 ) ."], "CC234": ["Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins \" previous work on re-ranking using a finite set of features ( #CITATION_TAG ) .", "feature sets, but then efficiency becomes a problem. Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins \" previous work on re-ranking using a finite set of features ( #CITATION_TAG ) .", "We do not believe these transforms have a major impact on performance, but we have not currently run tests without them. feature sets, but then efficiency becomes a problem. Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins \" previous work on re-ranking using a finite set of features ( #CITATION_TAG ) ."], "CC235": ["For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #CITATION_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) . Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i \ufffd 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.", "For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top \"s left-corner child (its leftmost child, if any), and top \"s most recent child (which was top,_ 1 , if any). For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #CITATION_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) . Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i \ufffd 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.", "For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #CITATION_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) . Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i \ufffd 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases.", "The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality. For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top \"s left-corner child (its leftmost child, if any), and top \"s most recent child (which was top,_ 1 , if any). For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #CITATION_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) . Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i \ufffd 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.", "For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top \"s left-corner child (its leftmost child, if any), and top \"s most recent child (which was top,_ 1 , if any). For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #CITATION_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) . Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i \ufffd 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases."], "CC236": ["#CITATION_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins \" previous work on re-ranking using a finite set of features ( Collins , 2000 ) .", "feature sets, but then efficiency becomes a problem. #CITATION_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins \" previous work on re-ranking using a finite set of features ( Collins , 2000 ) .", "We do not believe these transforms have a major impact on performance, but we have not currently run tests without them. feature sets, but then efficiency becomes a problem. #CITATION_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins \" previous work on re-ranking using a finite set of features ( Collins , 2000 ) ."], "CC237": ["We used a publicly available tagger ( #CITATION_TAG ) to tag the words and then used these in the input to the system ."], "CC238": ["The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #CITATION_TAG ; Collins , 2000 ; Bod , 2001 ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.", "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #CITATION_TAG ; Collins , 2000 ; Bod , 2001 ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.", "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #CITATION_TAG ; Collins , 2000 ; Bod , 2001 ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words."], "CC239": ["Many statistical parsers ( Ratnaparkhi , 1999 ; #CITATION_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.", "Many statistical parsers ( Ratnaparkhi , 1999 ; #CITATION_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).", "Many statistical parsers ( Ratnaparkhi , 1999 ; #CITATION_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001). In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history."], "CC240": ["For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #CITATION_TAG ) . Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.", "For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top \"s left-corner child (its leftmost child, if any), and top \"s most recent child (which was top,_ 1 , if any). For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #CITATION_TAG ) . Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.", "For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #CITATION_TAG ) . Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases.", "The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality. For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top \"s left-corner child (its leftmost child, if any), and top \"s most recent child (which was top,_ 1 , if any). For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #CITATION_TAG ) . Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.", "For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top \"s left-corner child (its leftmost child, if any), and top \"s most recent child (which was top,_ 1 , if any). For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #CITATION_TAG ) . Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases."], "CC241": ["The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #CITATION_TAG ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.", "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #CITATION_TAG ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.", "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #CITATION_TAG ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words."], "CC243": ["Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model.", "The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model.", "Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.", "The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model.", "The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.", "Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.", "In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model.", "The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.", "The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.", "Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.", "In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.", "The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.", "The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.", "In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.", "The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #CITATION_TAG ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features."], "CC244": ["This is roughly an 11 % relative reduction in error rate over #CITATION_TAG and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.", "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over #CITATION_TAG and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.", "This is roughly an 11 % relative reduction in error rate over #CITATION_TAG and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in", "But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over #CITATION_TAG and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.", "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over #CITATION_TAG and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in", "Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over #CITATION_TAG and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.", "But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over #CITATION_TAG and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in"], "CC245": ["#CITATION_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 \"s subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ . Goodman (2002) furthermore showed how Bonnema et al.\"s (1999) and Bod\"s (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.", "Although Bod\"s method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003). #CITATION_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 \"s subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ . Goodman (2002) furthermore showed how Bonnema et al.\"s (1999) and Bod\"s (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.", "Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. Although Bod\"s method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003). #CITATION_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 \"s subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ . Goodman (2002) furthermore showed how Bonnema et al.\"s (1999) and Bod\"s (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions."], "CC246": ["But while Bod \"s estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. \"s estimator performs worse and is comparable to #CITATION_TAG .", "We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001Bod ( , 2003. But while Bod \"s estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. \"s estimator performs worse and is comparable to #CITATION_TAG ."], "CC247": ["Compared to the reranking technique in #CITATION_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction . While SL-DOP and LS-DOP have been compared before in", "This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod\"s PCFG-reduction reported in Table 1. Compared to the reranking technique in #CITATION_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction . While SL-DOP and LS-DOP have been compared before in", "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod\"s PCFG-reduction reported in Table 1. Compared to the reranking technique in #CITATION_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction . While SL-DOP and LS-DOP have been compared before in"], "CC248": ["But while Bod \"s estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #CITATION_TAG , Bonnema et al. \"s estimator performs worse and is comparable to Collins ( 1996 ) .", "We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001Bod ( , 2003. But while Bod \"s estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #CITATION_TAG , Bonnema et al. \"s estimator performs worse and is comparable to Collins ( 1996 ) ."], "CC250": ["Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.", "Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.", "Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times.", "as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.", "Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times.", "Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).", "nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.", "as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times.", "Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).", "Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words). This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.", "nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times.", "as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).", "Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words). This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.", "nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).", "as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and #CITATION_TAG ) . As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words). This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction."], "CC251": ["For our experiments we used the standard division of the WSJ ( #CITATION_TAG ) , with sections 2 through 21 for training ( approx . 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.", "For our experiments we used the standard division of the WSJ ( #CITATION_TAG ) , with sections 2 through 21 for training ( approx . 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set. As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.", "For our experiments we used the standard division of the WSJ ( #CITATION_TAG ) , with sections 2 through 21 for training ( approx . 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set. As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks. Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing)."], "CC252": ["#CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996).", "Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996).", "#CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.", "as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996).", "Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.", "#CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times.", "nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996).", "as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.", "Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times.", "#CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).", "nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.", "as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times.", "Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).", "nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times.", "as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #CITATION_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words)."], "CC253": ["But while Bod \"s estimator obtains state-of-the-art results on the WSJ , comparable to #CITATION_TAG and Collins ( 2000 ) , Bonnema et al. \"s estimator performs worse and is comparable to Collins ( 1996 ) .", "We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001Bod ( , 2003. But while Bod \"s estimator obtains state-of-the-art results on the WSJ , comparable to #CITATION_TAG and Collins ( 2000 ) , Bonnema et al. \"s estimator performs worse and is comparable to Collins ( 1996 ) ."], "CC254": ["And #CITATION_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees \"\" , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .", "The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999;Charniak 2000;Goodman 1998). And #CITATION_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees \"\" , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .", "While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a). The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999;Charniak 2000;Goodman 1998). And #CITATION_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees \"\" , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) ."], "CC255": ["Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima\"an ( 2000 ) and #CITATION_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence"], "CC257": ["Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , #CITATION_TAG and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence"], "CC258": ["Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #CITATION_TAG , 2002 ) . Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002).", "A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account. Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #CITATION_TAG , 2002 ) . Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002)."], "CC260": ["The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #CITATION_TAG ; Goodman 1998 ) . And Collins (2000) argues for \"keeping track of counts of arbitrary fragments within parse trees which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992).", "While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a). The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #CITATION_TAG ; Goodman 1998 ) . And Collins (2000) argues for \"keeping track of counts of arbitrary fragments within parse trees which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992).", "However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a). The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #CITATION_TAG ; Goodman 1998 ) . And Collins (2000) argues for \"keeping track of counts of arbitrary fragments within parse trees which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992)."], "CC261": ["And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees \"\" , which has indeed been carried out in #CITATION_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .", "The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999;Charniak 2000;Goodman 1998). And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees \"\" , which has indeed been carried out in #CITATION_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .", "While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a). The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999;Charniak 2000;Goodman 1998). And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees \"\" , which has indeed been carried out in #CITATION_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) ."], "CC262": ["And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #CITATION_TAG ) . Goodman\"s main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.", "Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a. And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #CITATION_TAG ) . Goodman\"s main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.", "And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #CITATION_TAG ) . Goodman\"s main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability. This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.", "Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a. And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #CITATION_TAG ) . Goodman\"s main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability. This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.", "And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #CITATION_TAG ) . Goodman\"s main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability. This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG. Note that Goodman\"s reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", "Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a. And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #CITATION_TAG ) . Goodman\"s main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability. This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG. Note that Goodman\"s reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", "And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #CITATION_TAG ) . Goodman\"s main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability. This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG. Note that Goodman\"s reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree. But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse."], "CC263": ["#CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents.", "Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents.", "#CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent.", "Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima\"an 1999;Chappelier et al. 2002). Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents.", "Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent.", "#CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.", "However, the problem of computing the most probable parse turns out to be NP-hard (Sima\"an 1996), mainly because the same parse tree can be generated by exponentially many derivations. Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima\"an 1999;Chappelier et al. 2002). Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents.", "Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima\"an 1999;Chappelier et al. 2002). Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent.", "Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.", "#CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization. This resulted in a statistically consistent model dubbed ML-DOP.", "However, the problem of computing the most probable parse turns out to be NP-hard (Sima\"an 1996), mainly because the same parse tree can be generated by exponentially many derivations. Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima\"an 1999;Chappelier et al. 2002). Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent.", "Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima\"an 1999;Chappelier et al. 2002). Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.", "Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization. This resulted in a statistically consistent model dubbed ML-DOP.", "However, the problem of computing the most probable parse turns out to be NP-hard (Sima\"an 1996), mainly because the same parse tree can be generated by exponentially many derivations. Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima\"an 1999;Chappelier et al. 2002). Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.", "Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima\"an 1999;Chappelier et al. 2002). Sima\"an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. #CITATION_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . While Goodman\\\"s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the \"maximum constituents parse i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b showed that DOP1\"s subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization. This resulted in a statistically consistent model dubbed ML-DOP."], "CC264": ["The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG )", "Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG )", "The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system.", "We symmetrize using the \"grow-diag-final-and\" heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG )", "Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system.", "The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system. The recaser used is the same for all systems.", "We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the \"grow-diag-final-and\" heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG )", "We symmetrize using the \"grow-diag-final-and\" heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system.", "Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system. The recaser used is the same for all systems.", "The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system. The recaser used is the same for all systems. It is the standard recaser supplied with Moses, trained on all German training data.", "We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the \"grow-diag-final-and\" heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system.", "We symmetrize using the \"grow-diag-final-and\" heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system. The recaser used is the same for all systems.", "Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system. The recaser used is the same for all systems. It is the standard recaser supplied with Moses, trained on all German training data.", "We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the \"grow-diag-final-and\" heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system. The recaser used is the same for all systems.", "We symmetrize using the \"grow-diag-final-and\" heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #CITATION_TAG ) We run MERT separately for each system. The recaser used is the same for all systems. It is the standard recaser supplied with Moses, trained on all German training data."], "CC266": ["For compound splitting , we follow #CITATION_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies . Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)).", "For compound splitting , we follow #CITATION_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies . Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied.", "For compound splitting , we follow #CITATION_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies . Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list."], "CC267": ["#CITATION_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models ."], "CC270": ["Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #CITATION_TAG ) . Compound merging is less well studied.", "For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #CITATION_TAG ) . Compound merging is less well studied.", "Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #CITATION_TAG ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.", "For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #CITATION_TAG ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.", "Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #CITATION_TAG ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds.", "For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #CITATION_TAG ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds.", "Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #CITATION_TAG ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow Stymne and Cancedda (2011), for compound merging."], "CC272": [], "CC273": ["We prepare the training data by splitting compounds in two steps , following the technique of #CITATION_TAG . First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.", "We prepare the training data by splitting compounds in two steps , following the technique of #CITATION_TAG . First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies. Training data is then stemmed as described in Section 2.3.", "We prepare the training data by splitting compounds in two steps , following the technique of #CITATION_TAG . First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies. Training data is then stemmed as described in Section 2.3. The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb."], "CC274": ["Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection.", "As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection.", "Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.", "This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection.", "As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.", "Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.", "Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT. This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection.", "This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.", "As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.", "Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT. This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.", "This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #CITATION_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing."], "CC275": ["#CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).", "We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).", "#CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets.", "Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).", "We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets.", "#CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.", "There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).", "Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets.", "We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.", "#CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features.", "There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets.", "Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.", "We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features.", "There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.", "Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. #CITATION_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features."], "CC277": ["Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #CITATION_TAG and others . Toutanova et.", "Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #CITATION_TAG and others . Toutanova et. al.\"s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."], "CC278": ["Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #CITATION_TAG , Yeniterzi and Oflazer ( 2010 ) and others . Toutanova et.", "Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #CITATION_TAG , Yeniterzi and Oflazer ( 2010 ) and others . Toutanova et. al.\"s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."], "CC279": ["Other approaches use less deep linguistic resources ( e.g. , POS-tags #CITATION_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) . Compound merging is less well studied.", "For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags #CITATION_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) . Compound merging is less well studied.", "Other approaches use less deep linguistic resources ( e.g. , POS-tags #CITATION_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.", "For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags #CITATION_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.", "Other approaches use less deep linguistic resources ( e.g. , POS-tags #CITATION_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds.", "For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags #CITATION_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds.", "Other approaches use less deep linguistic resources ( e.g. , POS-tags #CITATION_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) . Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow Stymne and Cancedda (2011), for compound merging."], "CC280": ["Our approach to extract and classify social events builds on our previous work ( #CITATION_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) . Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.", "Our approach to extract and classify social events builds on our previous work ( #CITATION_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) . Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003;Culotta and Sorensen, 2004).", "Our approach to extract and classify social events builds on our previous work ( #CITATION_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) . Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003;Culotta and Sorensen, 2004). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005)."], "CC281": [], "CC282": [], "CC283": ["Proceedings of EACL \"99 example , the ALE parser ( #CITATION_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted.", "Combining control strategies depends on a way to differentiate between types of constraints. Proceedings of EACL \"99 example , the ALE parser ( #CITATION_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted.", "Proceedings of EACL \"99 example , the ALE parser ( #CITATION_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type.", "Combining control strategies depends on a way to differentiate between types of constraints. Proceedings of EACL \"99 example , the ALE parser ( #CITATION_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type.", "Proceedings of EACL \"99 example , the ALE parser ( #CITATION_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type. 1\u00b0 All types in the type hierarchy can be used as parse types.", "Combining control strategies depends on a way to differentiate between types of constraints. Proceedings of EACL \"99 example , the ALE parser ( #CITATION_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type. 1\u00b0 All types in the type hierarchy can be used as parse types.", "Proceedings of EACL \"99 example , the ALE parser ( #CITATION_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type. 1\u00b0 All types in the type hierarchy can be used as parse types. This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering."], "CC285": ["Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #CITATION_TAG ) 3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992)."], "CC286": ["Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #CITATION_TAG ) . 3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into TIT definite clauses which are used to restrict lexical entries.", "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #CITATION_TAG ) . 3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into TIT definite clauses which are used to restrict lexical entries. (GStz and Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.", "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #CITATION_TAG ) . 3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into TIT definite clauses which are used to restrict lexical entries. (GStz and Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints. 4 Because of space limitations we have to refrain from an example."], "CC287": ["See , among others , ( #CITATION_TAG ) . As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.", "Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs. See , among others , ( #CITATION_TAG ) . As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.", "See , among others , ( #CITATION_TAG ) . As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements. In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv\ufffd:; GStz, 1995).", "Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs. See , among others , ( #CITATION_TAG ) . As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements. In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv\ufffd:; GStz, 1995).", "See , among others , ( #CITATION_TAG ) . As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements. In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv\ufffd:; GStz, 1995). Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) as discussed in (GStz and Meurers, 1997a) and (Meurers and Minnen, 1997).", "Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs. See , among others , ( #CITATION_TAG ) . As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements. In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv\ufffd:; GStz, 1995). Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) as discussed in (GStz and Meurers, 1997a) and (Meurers and Minnen, 1997).", "See , among others , ( #CITATION_TAG ) . As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements. In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv\ufffd:; GStz, 1995). Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) as discussed in (GStz and Meurers, 1997a) and (Meurers and Minnen, 1997). Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation."], "CC288": ["The ConTroll grammar development system as described in ( #CITATION_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .", "4 Because of space limitations we have to refrain from an example. The ConTroll grammar development system as described in ( #CITATION_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .", "Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints. 4 Because of space limitations we have to refrain from an example. The ConTroll grammar development system as described in ( #CITATION_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars ."], "CC290": ["As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #CITATION_TAG ) . Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.", "In contrast to Johnson and DSrre\"s deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies. As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #CITATION_TAG ) . Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.", "As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #CITATION_TAG ) . Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered. feature grammars on the basis of an example and introduce a dynamic bottom-up interpreter that can be used for goM-directed interpretation of magic-compiled typed feature grammars.", "The proposed parser is related to the so-called Lemma Table deduction system (Johnson and DSrre, 1995) which allows the user to specify whether top-down sub-computations are to be tabled. In contrast to Johnson and DSrre\"s deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies. As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #CITATION_TAG ) . Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.", "In contrast to Johnson and DSrre\"s deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies. As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #CITATION_TAG ) . Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered. feature grammars on the basis of an example and introduce a dynamic bottom-up interpreter that can be used for goM-directed interpretation of magic-compiled typed feature grammars."], "CC291": ["Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #CITATION_TAGa ) and ( Meurers and Minnen , 1997 ) . Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.", "In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv\u00a3:;GStz, 1995). Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #CITATION_TAGa ) and ( Meurers and Minnen , 1997 ) . Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation."], "CC292": ["` See ( #CITATION_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG . append ([~,[~,[~).", "` See ( #CITATION_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG . append ([~,[~,[~). Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.", "` See ( #CITATION_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG . append ([~,[~,[~). Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints. 4 Because of space limitations we have to refrain from an example."], "CC294": [], "CC295": ["At the same time , we believe our method has advantages over the approach developed initially at IBM ( #CITATION_TAG ; Brown et al. 1993 ) for training translation systems automatically . One advantage is that our method attempts to model the natural decomposition of sentences into phrases.", "At the same time , we believe our method has advantages over the approach developed initially at IBM ( #CITATION_TAG ; Brown et al. 1993 ) for training translation systems automatically . One advantage is that our method attempts to model the natural decomposition of sentences into phrases. Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model."], "CC296": ["1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 )", "Each matching plan\"s preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan\"s body. 1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 )", "1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 ) Their planner uses plan structures similar to IGEN\"s, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987).", "The planner first checks all of its top-level plans to see which have effects that match the goal. Each matching plan\"s preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan\"s body. 1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 )", "Each matching plan\"s preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan\"s body. 1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 ) Their planner uses plan structures similar to IGEN\"s, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987).", "1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 ) Their planner uses plan structures similar to IGEN\"s, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987). In IGEN, the plans can involve any goals or actions that could be achieved via communication.", "IGEN constructs its plans using a hierarchical planning algorithm (Nilsson 1980). The planner first checks all of its top-level plans to see which have effects that match the goal. Each matching plan\"s preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan\"s body. 1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 )", "The planner first checks all of its top-level plans to see which have effects that match the goal. Each matching plan\"s preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan\"s body. 1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 ) Their planner uses plan structures similar to IGEN\"s, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987).", "Each matching plan\"s preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan\"s body. 1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 ) Their planner uses plan structures similar to IGEN\"s, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987). In IGEN, the plans can involve any goals or actions that could be achieved via communication.", "IGEN constructs its plans using a hierarchical planning algorithm (Nilsson 1980). The planner first checks all of its top-level plans to see which have effects that match the goal. Each matching plan\"s preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan\"s body. 1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 ) Their planner uses plan structures similar to IGEN\"s, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987).", "The planner first checks all of its top-level plans to see which have effects that match the goal. Each matching plan\"s preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan\"s body. 1 \u00c2\u00b0 The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner \"\" ( #CITATION_TAG , 203 ) Their planner uses plan structures similar to IGEN\"s, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987). In IGEN, the plans can involve any goals or actions that could be achieved via communication."], "CC297": ["McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #CITATION_TAG ) . Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).", "For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way. McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #CITATION_TAG ) . Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).", "Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way. McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #CITATION_TAG ) . Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994)."], "CC299": ["McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #CITATION_TAG ; Panaget 1994 ; Wanner 1994 ) . Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).", "For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way. McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #CITATION_TAG ; Panaget 1994 ; Wanner 1994 ) . Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).", "Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way. McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #CITATION_TAG ; Panaget 1994 ; Wanner 1994 ) . Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994)."], "CC301": ["These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #CITATION_TAG ; Sondheimer and Nebel 1986 ) , and Hovy \"s notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) . All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.", "There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #CITATION_TAG ; Sondheimer and Nebel 1986 ) , and Hovy \"s notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) . All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.", "These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #CITATION_TAG ; Sondheimer and Nebel 1986 ) , and Hovy \"s notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) . All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance. The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.", "There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #CITATION_TAG ; Sondheimer and Nebel 1986 ) , and Hovy \"s notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) . All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance. The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.", "These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #CITATION_TAG ; Sondheimer and Nebel 1986 ) , and Hovy \"s notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) . All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance. The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative. 5"], "CC303": ["This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in #CITATION_TAG and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost ."], "CC306": ["Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #CITATION_TAG ) .", "McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #CITATION_TAG ) .", "For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way. McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #CITATION_TAG ) ."], "CC307": ["The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.", "The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language. The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.", "The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.", "Research in natural language generation has generally separated the task into distinct text planning and linguistic components. The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language. The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.", "The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language. The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.", "The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way.", "Research in natural language generation has generally separated the task into distinct text planning and linguistic components. The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language. The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.", "The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language. The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way.", "The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way. McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994; Panaget 1994; Wanner 1994).", "Research in natural language generation has generally separated the task into distinct text planning and linguistic components. The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language. The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way.", "The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language. The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , \"planning\" and \"realization\" ( e.g. , McDonald 1983 ; #CITATION_TAGa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , Danlos 1987 ; Reithinger 1990 ) . The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun\"s work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way. McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994; Panaget 1994; Wanner 1994)."], "CC308": [], "CC309": ["An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #CITATION_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .", "Combinatory categorial grammar does not concern itself with the capture of all (or only) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design. An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #CITATION_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction ."], "CC311": ["It is known that certain cue words and phrases ( #CITATION_TAG ) can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types.", "DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases ( #CITATION_TAG ) can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types.", "It is known that certain cue words and phrases ( #CITATION_TAG ) can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh\\\"s occur in BACKCHANNELS, and 88.4% of the trigrams \"<start> do you\" occur in YES-NO-QUESTIONS.", "DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases ( #CITATION_TAG ) can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh\\\"s occur in BACKCHANNELS, and 88.4% of the trigrams \"<start> do you\" occur in YES-NO-QUESTIONS.", "It is known that certain cue words and phrases ( #CITATION_TAG ) can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh\\\"s occur in BACKCHANNELS, and 88.4% of the trigrams \"<start> do you\" occur in YES-NO-QUESTIONS. To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type.", "DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases ( #CITATION_TAG ) can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh\\\"s occur in BACKCHANNELS, and 88.4% of the trigrams \"<start> do you\" occur in YES-NO-QUESTIONS. To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type.", "It is known that certain cue words and phrases ( #CITATION_TAG ) can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh\\\"s occur in BACKCHANNELS, and 88.4% of the trigrams \"<start> do you\" occur in YES-NO-QUESTIONS. To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type. 5.1."], "CC312": ["A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #CITATION_TAG ) . Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which further techniques could be borrowed.", "However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #CITATION_TAG ) . Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which further techniques could be borrowed.", "As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way. However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #CITATION_TAG ) . Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which further techniques could be borrowed."], "CC313": ["The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #CITATION_TAG )", "The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #CITATION_TAG ) It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).", "The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #CITATION_TAG ) It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995). To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ....."], "CC314": ["It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #CITATION_TAG ) . To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 .....", "The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988). It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #CITATION_TAG ) . To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 .....", "It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #CITATION_TAG ) . To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n. We can compute the per-utterance posterior DA probabilities by summing:"], "CC315": ["This equivalence is doing essentially the same job as Pereira \"s pronoun abstraction schema in #CITATION_TAG"], "CC316": ["These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #CITATION_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) . Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at QLF (again unlike standard QLFs) but are assumed to be available as components of the linguistic context.", "What is required is that QLFs are, as here, expressed in a typed higher-order logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis, focus, etc.). These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #CITATION_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) . Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at QLF (again unlike standard QLFs) but are assumed to be available as components of the linguistic context.", "These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #CITATION_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) . Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at QLF (again unlike standard QLFs) but are assumed to be available as components of the linguistic context. ~"], "CC317": ["In the CLE-QLF approach, as rationally reconstructed by #CITATION_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.", "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI\"s Core Language Engine (CLE). In the CLE-QLF approach, as rationally reconstructed by #CITATION_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.", "In the CLE-QLF approach, as rationally reconstructed by #CITATION_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context."], "CC318": ["A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #CITATION_TAG", "A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #CITATION_TAG This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.", "A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #CITATION_TAG This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume. Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints."], "CC319": [], "CC320": ["The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #CITATION_TAG , 1992 ) , and implemented in SRI \"s Core Language Engine ( CLE ) . In the CLE-QLF approach, as ra-tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994), the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules."], "CC321": [], "CC322": ["The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed \"\" structure ( #CITATION_TAG ) , with the resolution process as described here . Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here.", "One is to adopt Pinkal\\\"s \"radical underspecification\" approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity. The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed \"\" structure ( #CITATION_TAG ) , with the resolution process as described here . Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here."], "CC323": ["The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #CITATION_TAG , 1991 ) , with some differences that are commented on below . Like Pereira\"s approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin.", "We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism. The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #CITATION_TAG , 1991 ) , with some differences that are commented on below . Like Pereira\"s approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin.", "The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #CITATION_TAG , 1991 ) , with some differences that are commented on below . Like Pereira\"s approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin. We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (Alshawi 1990), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.", "We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism. The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #CITATION_TAG , 1991 ) , with some differences that are commented on below . Like Pereira\"s approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin. We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (Alshawi 1990), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.", "The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #CITATION_TAG , 1991 ) , with some differences that are commented on below . Like Pereira\"s approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin. We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (Alshawi 1990), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity. 6"], "CC324": ["It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #CITATION_TAG , 1991 ) . Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a \"free variable\" of type e is introduced in the NP position, with an associated \"quantifier assumption,\" which is added as a kind of premise.", "It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #CITATION_TAG , 1991 ) . Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a \"free variable\" of type e is introduced in the NP position, with an associated \"quantifier assumption,\" which is added as a kind of premise. At a later stage the quantifier assumption is \"discharged,\" capturing all occurrences of the free variable."], "CC325": [], "CC326": ["But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #CITATION_TAG ) work to our own framework . Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.", "Developing a calculus for reasoning with QLFs is too large a task to be undertaken here. But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #CITATION_TAG ) work to our own framework . Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.", "But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #CITATION_TAG ) work to our own framework . Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is. His example is:"], "CC327": [], "CC328": ["A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #CITATION_TAG ; Mitkov 1996 , 1998b ) .", "However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #CITATION_TAG ; Mitkov 1996 , 1998b ) ."], "CC329": ["Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #CITATION_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #CITATION_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #CITATION_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."], "CC330": ["Tetreault \"s contribution features comparative evaluation involving the author \"s own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs \"s naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube \"s 5list approach ( #CITATION_TAG ) . The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally."], "CC332": ["A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #CITATION_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .", "However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #CITATION_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) ."], "CC333": ["Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #CITATION_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .", "Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #CITATION_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #CITATION_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) ."], "CC334": ["Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #CITATION_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #CITATION_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #CITATION_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."], "CC335": ["Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #CITATION_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #CITATION_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #CITATION_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #CITATION_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #CITATION_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #CITATION_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #CITATION_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."], "CC336": ["A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #CITATION_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .", "However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #CITATION_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) ."], "CC338": ["Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #CITATION_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #CITATION_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #CITATION_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #CITATION_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #CITATION_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #CITATION_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #CITATION_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."], "CC339": ["The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #CITATION_TAGa ) . A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are.", "The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #CITATION_TAGa ) . A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are. In particular, more research should be carried out on the factors influencing the performance of these algorithms.", "The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #CITATION_TAGa ) . A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are. In particular, more research should be carried out on the factors influencing the performance of these algorithms. One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links."], "CC340": ["Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #CITATION_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #CITATION_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #CITATION_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."], "CC341": ["Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #CITATION_TAG ) , which was difficult both to represent and to process , and which required considerable human input . However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies."], "CC342": ["The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #CITATION_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #CITATION_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #CITATION_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #CITATION_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #CITATION_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #CITATION_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #CITATION_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."], "CC343": ["Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #CITATION_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #CITATION_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #CITATION_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #CITATION_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #CITATION_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #CITATION_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #CITATION_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."], "CC344": ["A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #CITATION_TAG , 1998b ) .", "However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #CITATION_TAG , 1998b ) ."], "CC345": ["Tetreault \"s contribution features comparative evaluation involving the author \"s own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs \"s naive algorithm ( #CITATION_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube \"s 5list approach ( Strube 1998 ) . The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally."], "CC346": ["Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #CITATION_TAG ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #CITATION_TAG ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #CITATION_TAG ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #CITATION_TAG ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #CITATION_TAG ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #CITATION_TAG ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #CITATION_TAG ; Mitkov , Belguith , and Stys 1998 ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."], "CC347": ["The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #CITATION_TAG . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #CITATION_TAG . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #CITATION_TAG . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #CITATION_TAG . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #CITATION_TAG . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #CITATION_TAG . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.", "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #CITATION_TAG . The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998). Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."], "CC348": ["Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #CITATION_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input . However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies."], "CC349": ["Proper names are the main concern of the named-entity recognition subtask ( #CITATION_TAG 1998) of information extraction.", "Proper names are the main concern of the named-entity recognition subtask ( #CITATION_TAG 1998) of information extraction. The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).", "Proper names are the main concern of the named-entity recognition subtask ( #CITATION_TAG 1998) of information extraction. The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.). 1 There the disambiguation of the first word in a sentence (and in other ambiguous positions) is one of the central problems: about 20% of named entities occur in ambiguous positions."], "CC350": ["We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history.", "There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history.", "There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #CITATION_TAG ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history."], "CC351": ["This is implemented as a cascade of simple strategies , which were briefly described in #CITATION_TAG .", "We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies , which were briefly described in #CITATION_TAG .", "Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies , which were briefly described in #CITATION_TAG ."], "CC353": ["Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations.", "Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations.", "Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).", "It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998). Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations.", "Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).", "Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to \"one sense per collocation\" idea of Yarowsky (1993).", "The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998). Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations.", "It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998). Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).", "Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to \"one sense per collocation\" idea of Yarowsky (1993).", "The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998). Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).", "It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998). Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #CITATION_TAG ) . Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to \"one sense per collocation\" idea of Yarowsky (1993)."], "CC355": ["Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #CITATION_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) . Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.", "Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #CITATION_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) . Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.", "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #CITATION_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) . Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.", "Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #CITATION_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) . Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.", "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #CITATION_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) . Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase."], "CC356": ["We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history.", "There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history.", "There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ #CITATION_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history."], "CC357": ["We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history.", "There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history.", "There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.", "Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill \"s [ Brill 1995a ] , and MaxEnt [ #CITATION_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history."], "CC358": ["The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #CITATION_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.", "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #CITATION_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.", "The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #CITATION_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #CITATION_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.", "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #CITATION_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #CITATION_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.", "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #CITATION_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #CITATION_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."], "CC359": ["For instance , the Alembic workbench ( #CITATION_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex . Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains.", "The rule-based systems use manually built rules that are usually encoded in terms of regular-expression grammars supplemented with lists of abbreviations, common words, proper names, etc. To put together a few rules is fast and easy, but to develop a rule-based system with good performance is quite a labor-consuming enterprise. For instance , the Alembic workbench ( #CITATION_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex . Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains."], "CC360": ["On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant.", "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant.", "On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).", "Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant.", "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).", "On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.", "Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).", "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.", "On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus.", "Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.", "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #CITATION_TAG ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus."], "CC361": ["This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill \"s [ #CITATION_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.", "We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill \"s [ #CITATION_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.", "This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill \"s [ #CITATION_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique.", "First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill \"s [ #CITATION_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.", "We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill \"s [ #CITATION_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique.", "We decided to train the tagger with the minimum of preannotated resources. First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill \"s [ #CITATION_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.", "First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill \"s [ #CITATION_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique."], "CC362": ["In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #CITATION_TAG", "The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #CITATION_TAG", "In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #CITATION_TAG We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.", "The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #CITATION_TAG", "The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #CITATION_TAG We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.", "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #CITATION_TAG", "The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #CITATION_TAG We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."], "CC363": ["As #CITATION_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not . Estimates from the Brown Corpus can be misleading.", "Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus. As #CITATION_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not . Estimates from the Brown Corpus can be misleading.", "As #CITATION_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not . Estimates from the Brown Corpus can be misleading. For example, the capitalized word \"Acts\" is found twice in the Brown Corpus, both times as a proper noun (in a title).", "Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus. As #CITATION_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not . Estimates from the Brown Corpus can be misleading. For example, the capitalized word \"Acts\" is found twice in the Brown Corpus, both times as a proper noun (in a title).", "As #CITATION_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not . Estimates from the Brown Corpus can be misleading. For example, the capitalized word \"Acts\" is found twice in the Brown Corpus, both times as a proper noun (in a title). It would be misleading to infer from this evidence that the word \\\"Acts\\\" is always a proper noun."], "CC364": ["This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #CITATION_TAG ] or Brill \"s [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.", "We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #CITATION_TAG ] or Brill \"s [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.", "This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #CITATION_TAG ] or Brill \"s [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique.", "First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #CITATION_TAG ] or Brill \"s [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.", "We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #CITATION_TAG ] or Brill \"s [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique.", "We decided to train the tagger with the minimum of preannotated resources. First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #CITATION_TAG ] or Brill \"s [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.", "First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #CITATION_TAG ] or Brill \"s [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique."], "CC365": ["Before using the DCA method , we applied a Russian morphological processor ( #CITATION_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. . For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms.", "Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue. Before using the DCA method , we applied a Russian morphological processor ( #CITATION_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. . For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms.", "Before using the DCA method , we applied a Russian morphological processor ( #CITATION_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. . For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms. Since the documents in the BBC news corpus were rather short, we applied the cache module, as described in Section 11.1.", "Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue. Before using the DCA method , we applied a Russian morphological processor ( #CITATION_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. . For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms. Since the documents in the BBC news corpus were rather short, we applied the cache module, as described in Section 11.1.", "Before using the DCA method , we applied a Russian morphological processor ( #CITATION_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. . For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms. Since the documents in the BBC news corpus were rather short, we applied the cache module, as described in Section 11.1. This allowed us to reuse information across the documents."], "CC366": ["This is similar to \"one sense per collocation\" idea of #CITATION_TAG .", "In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to \"one sense per collocation\" idea of #CITATION_TAG .", "Gale, Church, and Yarowsky\"s observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to \"one sense per collocation\" idea of #CITATION_TAG ."], "CC368": ["The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #CITATION_TAG : 0.28 % vs. 0.20 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate).", "Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #CITATION_TAG : 0.28 % vs. 0.20 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate).", "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #CITATION_TAG : 0.28 % vs. 0.20 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although these error rates seem to be very small, they are quite significant.", "Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #CITATION_TAG : 0.28 % vs. 0.20 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although these error rates seem to be very small, they are quite significant.", "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #CITATION_TAG : 0.28 % vs. 0.20 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).", "Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #CITATION_TAG : 0.28 % vs. 0.20 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).", "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #CITATION_TAG : 0.28 % vs. 0.20 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level."], "CC369": ["In some systems such dependencies are learned from labeled examples ( #CITATION_TAG ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.", "Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998). In some systems such dependencies are learned from labeled examples ( #CITATION_TAG ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.", "In some systems such dependencies are learned from labeled examples ( #CITATION_TAG ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class. The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data.", "In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name. Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998). In some systems such dependencies are learned from labeled examples ( #CITATION_TAG ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.", "Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998). In some systems such dependencies are learned from labeled examples ( #CITATION_TAG ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class. The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data.", "In some systems such dependencies are learned from labeled examples ( #CITATION_TAG ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class. The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data. Also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port.", "In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name. Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998). In some systems such dependencies are learned from labeled examples ( #CITATION_TAG ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class. The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data.", "Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998). In some systems such dependencies are learned from labeled examples ( #CITATION_TAG ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class. The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data. Also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port."], "CC370": ["For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #CITATION_TAG )", "\u2022 abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts. For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #CITATION_TAG )", "For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #CITATION_TAG ) We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.", "\u2022 abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts. For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #CITATION_TAG ) We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.", "For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #CITATION_TAG ) We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain."], "CC371": ["The best performance on the Brown corpus , a 0.2 % error rate , was reported by #CITATION_TAG , who trained a decision tree classifier on a 25-million-word corpus . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best performance on the Brown corpus , a 0.2 % error rate , was reported by #CITATION_TAG , who trained a decision tree classifier on a 25-million-word corpus . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "The best performance on the Brown corpus , a 0.2 % error rate , was reported by #CITATION_TAG , who trained a decision tree classifier on a 25-million-word corpus . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.", "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best performance on the Brown corpus , a 0.2 % error rate , was reported by #CITATION_TAG , who trained a decision tree classifier on a 25-million-word corpus . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best performance on the Brown corpus , a 0.2 % error rate , was reported by #CITATION_TAG , who trained a decision tree classifier on a 25-million-word corpus . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.", "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best performance on the Brown corpus , a 0.2 % error rate , was reported by #CITATION_TAG , who trained a decision tree classifier on a 25-million-word corpus . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best performance on the Brown corpus , a 0.2 % error rate , was reported by #CITATION_TAG , who trained a decision tree classifier on a 25-million-word corpus . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."], "CC372": ["One of the better-known approaches is described in #CITATION_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing", "Not much information has been published on abbreviation identification. One of the better-known approaches is described in #CITATION_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing", "One of the better-known approaches is described in #CITATION_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.", "Not much information has been published on abbreviation identification. One of the better-known approaches is described in #CITATION_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.", "One of the better-known approaches is described in #CITATION_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.", "Not much information has been published on abbreviation identification. One of the better-known approaches is described in #CITATION_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.", "One of the better-known approaches is described in #CITATION_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. Park and Byrd (2001) recently described a hybrid method for finding abbreviations and their definitions."], "CC374": ["A detailed introduction to the SBD problem can be found in #CITATION_TAG .", "Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD problem can be found in #CITATION_TAG .", "In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary. Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD problem can be found in #CITATION_TAG ."], "CC375": ["This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #CITATION_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .", "We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #CITATION_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .", "Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #CITATION_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."], "CC376": ["#CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names", "For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names", "#CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.", "Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names", "For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.", "#CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.", "In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names", "Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.", "For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.", "#CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).", "In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.", "Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.", "For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).", "In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.", "Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #CITATION_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998)."], "CC377": ["Unlike other POS taggers , this POS tagger ( #CITATION_TAG ) was also trained to disambiguate sentence boundaries ."], "CC378": ["The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #CITATION_TAG ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.", "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #CITATION_TAG ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.", "The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #CITATION_TAG ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #CITATION_TAG ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.", "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #CITATION_TAG ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #CITATION_TAG ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.", "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #CITATION_TAG ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).", "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #CITATION_TAG ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."], "CC379": ["This is where robust syntactic systems like SATZ ( #CITATION_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .", "We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ ( #CITATION_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .", "Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ ( #CITATION_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."], "CC380": ["#CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.", "But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.", "#CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.", "The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.", "But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.", "#CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.", "Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage. The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.", "The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.", "But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.", "#CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.", "Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage. The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.", "The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.", "But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.", "Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage. The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.", "The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #CITATION_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word \"s last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names."], "CC381": ["The description of the EAGLE workbench for linguistic engineering ( #CITATION_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document", "The description of the EAGLE workbench for linguistic engineering ( #CITATION_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.", "The description of the EAGLE workbench for linguistic engineering ( #CITATION_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions. It is quite similar to our method for capitalized-word disambiguation."], "CC382": ["Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #CITATION_TAG ) . Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.", "Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #CITATION_TAG ) . Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.", "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #CITATION_TAG ) . Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.", "Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #CITATION_TAG ) . Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.", "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #CITATION_TAG ) . Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase."], "CC384": ["#CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions", "The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions", "#CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document.", "This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions", "The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document.", "#CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).", "One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing. This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions", "This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document.", "The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).", "#CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK). The abbreviation recognizer for these purposes is allowed to overgenerate significantly.", "One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing. This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document.", "This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).", "The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK). The abbreviation recognizer for these purposes is allowed to overgenerate significantly.", "One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing. This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).", "This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #CITATION_TAG recently described a hybrid method for finding abbreviations and their definitions This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK). The abbreviation recognizer for these purposes is allowed to overgenerate significantly."], "CC385": ["For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words", "Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words", "For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words This is a relatively small training set that can be manually marked in a few hours\" time.", "The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words", "Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words This is a relatively small training set that can be manually marked in a few hours\" time.", "For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words This is a relatively small training set that can be manually marked in a few hours\" time. But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).", "A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention. The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words", "The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words This is a relatively small training set that can be manually marked in a few hours\" time.", "Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words This is a relatively small training set that can be manually marked in a few hours\" time. But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).", "A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention. The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words This is a relatively small training set that can be manually marked in a few hours\" time.", "The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #CITATION_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words This is a relatively small training set that can be manually marked in a few hours\" time. But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%)."], "CC386": ["Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #CITATION_TAG ) .7 As an example , consider the translation into French of the house collapsed .", "In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #CITATION_TAG ) .7 As an example , consider the translation into French of the house collapsed ."], "CC387": [], "CC388": ["#CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.", "#CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.", "There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.", "#CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor.", "All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.", "There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor.", "#CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.", "There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor.", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor.", "There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #CITATION_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking."], "CC389": [], "CC390": ["Other similar approaches include those of Cicekli and G \u00c2\u00a8 uvenir ( 1996 ) , #CITATION_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .", "Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and G \u00c2\u00a8 uvenir ( 1996 ) , #CITATION_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .", "Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and G \u00c2\u00a8 uvenir ( 1996 ) , #CITATION_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia ."], "CC391": [], "CC392": ["Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #CITATION_TAG ; Veale and Way 1997 ; Carl 1999 ) .7", "In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #CITATION_TAG ; Veale and Way 1997 ; Carl 1999 ) .7"], "CC393": ["From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #CITATION_TAG ) .", "If a new input string cannot be found exactly in the translation database, a search is conducted for close (or \"fuzzy\") matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation. From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #CITATION_TAG ) .", "TM systems store a set of source, target translation pairs in their databases. If a new input string cannot be found exactly in the translation database, a search is conducted for close (or \"fuzzy\") matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation. From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #CITATION_TAG ) ."], "CC394": ["#CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor.", "Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor.", "#CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor.", "Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "#CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor.", "Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.", "#CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.", "Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.", "Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #CITATION_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities . The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:"], "CC395": [], "CC396": [], "CC397": [], "CC398": ["More recently , #CITATION_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch \u00c2\u00a8 aler ( 2002 ) and Sch \u00c2\u00a8 aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment . This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, and from there they suggest that \ufffdit is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those described in [Carl and Way 2003]."], "CC399": ["#CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.", "Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.", "#CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor.", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.", "Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor.", "#CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor.", "Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "#CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.", "There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor.", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.", "There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.", "3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. #CITATION_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics."], "CC400": ["#CITATION_TAG , 1997 ) assumes that words ending in - ed are verbs . However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.", "#CITATION_TAG , 1997 ) assumes that words ending in - ed are verbs . However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category. Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.", "#CITATION_TAG , 1997 ) assumes that words ending in - ed are verbs . However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category. Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides. That is, for a rule in the Penn Treebank VP \u2212\u2192 VBG, NP, PP, we are certain (if the annotators have done their job correctly) that the first word in each of the strings corresponding to this right-hand side is a VBG, that is, a present participle."], "CC403": ["However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #CITATION_TAG . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.", "The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #CITATION_TAG . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.", "However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #CITATION_TAG . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. Rather than search for competing candidates, we select the \"best\" translation and have its morphological variants searched for on-line.", "The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #CITATION_TAG . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. Rather than search for competing candidates, we select the \"best\" translation and have its morphological variants searched for on-line.", "However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #CITATION_TAG . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. Rather than search for competing candidates, we select the \"best\" translation and have its morphological variants searched for on-line. In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l\"ordinateurs personnels.", "The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #CITATION_TAG . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. Rather than search for competing candidates, we select the \"best\" translation and have its morphological variants searched for on-line. In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l\"ordinateurs personnels.", "However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #CITATION_TAG . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. Rather than search for competing candidates, we select the \"best\" translation and have its morphological variants searched for on-line. In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l\"ordinateurs personnels. Interestingly, using Lycos, and setting the search language to French, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2,454 times, whereas the others are not found at all."], "CC405": [], "CC406": ["In a final processing stage , we generalize over the marker lexicon following a process found in #CITATION_TAG . In Block\"s approach, word alignments are assigned probabilities by means of a statistical word alignment tool."], "CC407": ["Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #CITATION_TAG to permit a limited form of insertion in the translation process . As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.", "Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #CITATION_TAG to permit a limited form of insertion in the translation process . As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.", "Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #CITATION_TAG to permit a limited form of insertion in the translation process . As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.", "Nevertheless, Juola (1998, page 23) observes that \"a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily. Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #CITATION_TAG to permit a limited form of insertion in the translation process . As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.", "Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #CITATION_TAG to permit a limited form of insertion in the translation process . As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.", "However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless, Juola (1998, page 23) observes that \"a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily. Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #CITATION_TAG to permit a limited form of insertion in the translation process . As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.", "Nevertheless, Juola (1998, page 23) observes that \"a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily. Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #CITATION_TAG to permit a limited form of insertion in the translation process . As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input."], "CC408": ["In their Gaijin system , #CITATION_TAG give a result of 63 % accurate translations obtained for English \u00e2\\x88\\x92 > German on a test set of 791 sentences from CorelDRAW manuals .", "On novel test sentences, he gives results of 72% correct translation. In their Gaijin system , #CITATION_TAG give a result of 63 % accurate translations obtained for English \u00e2\\x88\\x92 > German on a test set of 791 sentences from CorelDRAW manuals .", "For English \u2212\u2192 Urdu, Juola (1997, page 213) notes that \"the system learned the original training corpus . . . perfectly and could reproduce it without errors\"; that is, it scored 100% accuracy when tested against the training corpus. On novel test sentences, he gives results of 72% correct translation. In their Gaijin system , #CITATION_TAG give a result of 63 % accurate translations obtained for English \u00e2\\x88\\x92 > German on a test set of 791 sentences from CorelDRAW manuals ."], "CC410": [], "CC411": ["For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ...", "For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ...", "For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation.", "Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu. For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ...", "For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation.", "For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation. In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English __ German on a test set of 791 sentences from CorelDRAW manuals.", "Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of _source, target_ chunks. Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu. For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ...", "Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu. For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation.", "For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation. In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English __ German on a test set of 791 sentences from CorelDRAW manuals.", "Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of _source, target_ chunks. Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu. For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation.", "Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu. For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English \u00e2\\x88\\x92 > Urdu , #CITATION_TAG , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation. In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English __ German on a test set of 791 sentences from CorelDRAW manuals."], "CC412": ["Other similar approaches include those of Cicekli and G \u00c2\u00a8 uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #CITATION_TAG , and Brown ( 2000 ) , inter alia .", "Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and G \u00c2\u00a8 uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #CITATION_TAG , and Brown ( 2000 ) , inter alia .", "Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and G \u00c2\u00a8 uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #CITATION_TAG , and Brown ( 2000 ) , inter alia ."], "CC413": ["This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.", "First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.", "This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up.", "From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.", "First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up.", "This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up. univ-mrs.", "In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems. From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.", "From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up.", "First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up. univ-mrs.", "This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up. univ-mrs. fr/\u02dcveronis/biblios/ptp.", "In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems. From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up.", "From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up. univ-mrs.", "First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up. univ-mrs. fr/\u02dcveronis/biblios/ptp.", "In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems. From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up. univ-mrs.", "From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #CITATION_TAG , to generate the `` generalized marker lexicon . Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www. up. univ-mrs. fr/\u02dcveronis/biblios/ptp."], "CC414": [], "CC416": ["That is, where #CITATION_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.", "That is, where #CITATION_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag. Given that examples such as \ufffd\ufffd<DET> a : un\ufffd are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme."], "CC417": ["Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.", "However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.", "Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process.", "Juola\\\"s (1994Juola\\\"s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form. However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.", "However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process.", "Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process. As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.", "The research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis. Juola\\\"s (1994Juola\\\"s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form. However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.", "Juola\\\"s (1994Juola\\\"s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form. However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process.", "However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process. As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.", "Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process. As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.", "The research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis. Juola\\\"s (1994Juola\\\"s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form. However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process.", "Juola\\\"s (1994Juola\\\"s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form. However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process. As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.", "However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process. As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.", "The research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis. Juola\\\"s (1994Juola\\\"s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form. However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process. As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.", "Juola\\\"s (1994Juola\\\"s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form. However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #CITATION_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process. As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input."], "CC418": ["Other similar approaches include those of Cicekli and G \u00c2\u00a8 uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #CITATION_TAG , inter alia .", "Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and G \u00c2\u00a8 uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #CITATION_TAG , inter alia .", "Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and G \u00c2\u00a8 uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #CITATION_TAG , inter alia ."], "CC419": ["More specifically , the notion of the phrasal lexicon ( used first by #CITATION_TAG ) has been used successfully in a number of areas :", "Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically , the notion of the phrasal lexicon ( used first by #CITATION_TAG ) has been used successfully in a number of areas :", "Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically , the notion of the phrasal lexicon ( used first by #CITATION_TAG ) has been used successfully in a number of areas :"], "CC420": ["All EBMT systems , from the initial proposal by #CITATION_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext . There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus.", "All EBMT systems , from the initial proposal by #CITATION_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext . There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.", "All EBMT systems , from the initial proposal by #CITATION_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext . There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3 Kay and R\u00f6scheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora."], "CC421": ["#CITATION_TAG combines lexical and dependency mappings to form his generalizations . Other similar approaches include those of Cicekli and G\u00fcvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.", "Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. #CITATION_TAG combines lexical and dependency mappings to form his generalizations . Other similar approaches include those of Cicekli and G\u00fcvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.", "Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: \"for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. #CITATION_TAG combines lexical and dependency mappings to form his generalizations . Other similar approaches include those of Cicekli and G\u00fcvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia."], "CC423": ["The work of #CITATION_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.", "Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #CITATION_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.", "The work of #CITATION_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing Hwa et al. 2003).", "This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #CITATION_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.", "Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #CITATION_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing Hwa et al. 2003).", "They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #CITATION_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.", "This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #CITATION_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing Hwa et al. 2003)."], "CC424": ["Some examples include text categorization ( #CITATION_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .", "In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications. Some examples include text categorization ( #CITATION_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) ."], "CC425": ["Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.", "This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.", "Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.", "They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.", "This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.", "Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing Hwa et al. 2003).", "For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.", "They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.", "This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing Hwa et al. 2003).", "For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.", "They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining ( #CITATION_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another . The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing Hwa et al. 2003)."], "CC426": ["Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different.", "In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different.", "Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.", "Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different.", "In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.", "Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data. In contrast, PLTIG\"s expectation-maximization-based induction algorithm is partially supervised; the model\"s parameters are estimated indirectly from the training data.", "In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model\"s current hypothesis in estimating the training utility of the candidates. Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different.", "Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.", "In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data. In contrast, PLTIG\"s expectation-maximization-based induction algorithm is partially supervised; the model\"s parameters are estimated indirectly from the training data.", "In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model\"s current hypothesis in estimating the training utility of the candidates. Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.", "Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #CITATION_TAG ; Hwa 1998 ) , and Collins \"s Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data. In contrast, PLTIG\"s expectation-maximization-based induction algorithm is partially supervised; the model\"s parameters are estimated indirectly from the training data."], "CC429": [], "CC430": [") Our algorithm is similar to the approach taken by #CITATION_TAG for inducing PCFG parsers .", "would be labeled as \"((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))). ) Our algorithm is similar to the approach taken by #CITATION_TAG for inducing PCFG parsers .", "For example, the sentence Several fund managers expect a rough market this morning before prices stabilize. would be labeled as \"((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))). ) Our algorithm is similar to the approach taken by #CITATION_TAG for inducing PCFG parsers ."], "CC431": ["For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #CITATION_TAG ) . Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).", "Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #CITATION_TAG ) . Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).", "For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #CITATION_TAG ) . Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.", "Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #CITATION_TAG ) . Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.", "For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #CITATION_TAG ) . Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.", "Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #CITATION_TAG ) . Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.", "For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #CITATION_TAG ) . Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor."], "CC432": ["Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #CITATION_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .", "In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications. Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #CITATION_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) ."], "CC434": ["#CITATION_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training . Similar approaches are being explored for parsing Hwa et al. 2003).", "The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. #CITATION_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training . Similar approaches are being explored for parsing Hwa et al. 2003).", "Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. #CITATION_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training . Similar approaches are being explored for parsing Hwa et al. 2003)."], "CC435": ["Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.", "For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.", "Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.", "Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.", "For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.", "Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.", "Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.", "For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.", "Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor. The goal of this work is to minimize a system\"s reliance on annotated training data.", "Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.", "For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers ( #CITATION_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor. The goal of this work is to minimize a system\"s reliance on annotated training data."], "CC436": ["Some well-known approaches include rule-based models ( #CITATION_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.", "Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models ( #CITATION_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.", "Some well-known approaches include rule-based models ( #CITATION_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.", "One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models ( #CITATION_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.", "Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models ( #CITATION_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier."], "CC437": ["In the first experiment , we use an induction algorithm ( #CITATION_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs . The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar\"s likelihood of generating the training data.", "In the first experiment , we use an induction algorithm ( #CITATION_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs . The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar\"s likelihood of generating the training data. In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.", "In the first experiment , we use an induction algorithm ( #CITATION_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs . The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar\"s likelihood of generating the training data. In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing. Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units."], "CC438": ["That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #CITATION_TAG ) . The underlying assumption is that an uncertain output is likely to be wrong."], "CC439": ["Similar approaches are being explored for parsing ( Steedman , #CITATION_TAG ; Hwa et al. 2003 ) .", "Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing ( Steedman , #CITATION_TAG ; Hwa et al. 2003 ) .", "The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing ( Steedman , #CITATION_TAG ; Hwa et al. 2003 ) ."], "CC440": ["The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples.", "For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples.", "The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition.", "preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification. For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples.", "For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition.", "The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition. Each training example forms eight characteristic tuples:", "The Collins-Brooks PP-attachment classification algorithm. preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification. For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples.", "preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification. For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition.", "For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition. Each training example forms eight characteristic tuples:", "The Collins-Brooks PP-attachment classification algorithm. preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification. For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition.", "preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification. For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #CITATION_TAG . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition. Each training example forms eight characteristic tuples:"], "CC441": ["Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #CITATION_TAG ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.", "Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #CITATION_TAG ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.", "Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #CITATION_TAG ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.", "One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #CITATION_TAG ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.", "Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #CITATION_TAG ) . Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier."], "CC442": ["We follow the notation convention of #CITATION_TAG .", "The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities). We follow the notation convention of #CITATION_TAG .", "14 The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees. The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities). We follow the notation convention of #CITATION_TAG ."], "CC445": ["#CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%).", "As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%).", "#CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX.", "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%).", "As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX.", "#CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.", "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%).", "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX.", "As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.", "#CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect.", "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX.", "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.", "As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect.", "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.", "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #CITATION_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect."], "CC446": ["The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner.", "Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner.", "The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner.", "Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099. Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099. Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #CITATION_TAG and Collins ( 1997 ) . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099. Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank."], "CC447": [], "CC448": ["While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #CITATION_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level . LFG argues that subcategorization requirements are best stated at the f-structure level, in functional rather than phrasal terms.", "While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #CITATION_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level . LFG argues that subcategorization requirements are best stated at the f-structure level, in functional rather than phrasal terms. This is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives of phrase structural position."], "CC449": ["#CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur", "Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur", "#CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.", "The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur", "Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.", "#CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).", "Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur", "The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.", "Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).", "#CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.", "Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.", "The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).", "Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.", "Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).", "The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #CITATION_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb."], "CC450": ["#CITATION_TAG , by comparison , employ 163 distinct predefined frames ."], "CC451": ["Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.", "The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.", "Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.", "Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.", "The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.", "Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \"inverse schemata.", "The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997). Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.", "Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.", "The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \"inverse schemata.", "The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997). Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.", "Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #CITATION_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \"inverse schemata."], "CC452": ["Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #CITATION_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) . In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).", "Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #CITATION_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) . In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches.", "Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #CITATION_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) . In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches. In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate."], "CC453": ["#CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions.", "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions.", "#CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions.", "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "#CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.", "#CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.", "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #CITATION_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns."], "CC455": [], "CC456": ["Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #CITATION_TAG ) is a member of the family of constraint-based grammars", "Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #CITATION_TAG ) is a member of the family of constraint-based grammars It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.", "Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #CITATION_TAG ) is a member of the family of constraint-based grammars It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case. C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries."], "CC457": ["Lexical functional grammar ( Kaplan and Bresnan 1982 ; #CITATION_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars", "Lexical functional grammar ( Kaplan and Bresnan 1982 ; #CITATION_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.", "Lexical functional grammar ( Kaplan and Bresnan 1982 ; #CITATION_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case. C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries."], "CC458": ["According to #CITATION_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ . OBJ \u03b8 and OBL \u03b8 represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.", "In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS (\u2191 SUBJ)(\u2191 OBL on ) . The argument list can be empty, as in the PRED value for judge in Figure 1. According to #CITATION_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ . OBJ \u03b8 and OBL \u03b8 represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.", "According to #CITATION_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ . OBJ \u03b8 and OBL \u03b8 represent families of grammatical functions indexed by their semantic role, represented by the theta subscript. This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1.", "The semantic form provides an argument list gf 1 ,gf 2 , . . .  gf n specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction. In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS (\u2191 SUBJ)(\u2191 OBL on ) . The argument list can be empty, as in the PRED value for judge in Figure 1. According to #CITATION_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ . OBJ \u03b8 and OBL \u03b8 represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.", "In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS (\u2191 SUBJ)(\u2191 OBL on ) . The argument list can be empty, as in the PRED value for judge in Figure 1. According to #CITATION_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ . OBJ \u03b8 and OBL \u03b8 represent families of grammatical functions indexed by their semantic role, represented by the theta subscript. This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1.", "The value of the PRED attribute in an f-structure is a semantic form \u03a0 gf 1 , gf 2 , . . .  gf n , where \u03a0 is a lemma and gf a grammatical function. The semantic form provides an argument list gf 1 ,gf 2 , . . .  gf n specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction. In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS (\u2191 SUBJ)(\u2191 OBL on ) . The argument list can be empty, as in the PRED value for judge in Figure 1. According to #CITATION_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ . OBJ \u03b8 and OBL \u03b8 represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.", "The semantic form provides an argument list gf 1 ,gf 2 , . . .  gf n specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction. In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS (\u2191 SUBJ)(\u2191 OBL on ) . The argument list can be empty, as in the PRED value for judge in Figure 1. According to #CITATION_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ . OBJ \u03b8 and OBL \u03b8 represent families of grammatical functions indexed by their semantic role, represented by the theta subscript. This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1."], "CC459": ["#CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem.", "Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem.", "#CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.", "Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem.", "Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.", "#CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).", "Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each. Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem.", "Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.", "Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).", "#CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001). We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.", "Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each. Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.", "Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).", "Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001). We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.", "Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each. Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).", "Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #CITATION_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument\ufffdadjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001). We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3."], "CC460": ["Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #CITATION_TAG ) . With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail.", "However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question. Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #CITATION_TAG ) . With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail.", "Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #CITATION_TAG ) . With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail. For example, to identify a that-clause, we use", "The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question. Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #CITATION_TAG ) . With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail.", "However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question. Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #CITATION_TAG ) . With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail. For example, to identify a that-clause, we use"], "CC461": ["The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct", "Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct", "The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.", "As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct", "Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.", "The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.", "Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct", "As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.", "Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.", "The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.", "Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.", "As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.", "Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.", "Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.", "As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by #CITATION_TAG in combination with a variation of Collins \"s ( 1997 ) approach to the differentiation between complement and adjunct This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank."], "CC462": ["The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.", "Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions.", "Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.", "Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.", "Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes\" theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions.", "Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #CITATION_TAG . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames."], "CC463": ["We applied lexical-redundancy rules ( #CITATION_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects."], "CC464": ["Both use the evaluation software and triple encoding presented in #CITATION_TAG . The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures", "The results of two different evaluations of the automatically generated f-structures are presented in Table 2. Both use the evaluation software and triple encoding presented in #CITATION_TAG . The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures"], "CC466": ["Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #CITATION_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.", "Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #CITATION_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.", "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #CITATION_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \ufffdinverse schemata.", "Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #CITATION_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \ufffdinverse schemata.", "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #CITATION_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \ufffdinverse schemata. \ufffd"], "CC467": [], "CC468": ["Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #CITATION_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept . However, more recent work (Cahill et al. 2002;Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994), containing more than 1,000,000 words and 49,000 sentences.", "F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures. Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #CITATION_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept . However, more recent work (Cahill et al. 2002;Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994), containing more than 1,000,000 words and 49,000 sentences."], "CC469": ["Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.", "Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.", "Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.", "The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.", "Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.", "Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate. They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.", "Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.", "The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.", "Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate. They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.", "Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate. They perform a mapping between their frames and those of the OALD, resulting in 15 frame types. These do not contain details of specific prepositions.", "Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.", "The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate. They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.", "Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate. They perform a mapping between their frames and those of the OALD, resulting in 15 frame types. These do not contain details of specific prepositions.", "Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate. They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.", "The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #CITATION_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate. They perform a mapping between their frames and those of the OALD, resulting in 15 frame types. These do not contain details of specific prepositions."], "CC470": ["In Charniak ( 1996 ) and #CITATION_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank", "The rate of accession may also be represented graphically. In Charniak ( 1996 ) and #CITATION_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank", "In Charniak ( 1996 ) and #CITATION_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.", "The rate of accession may also be represented graphically. In Charniak ( 1996 ) and #CITATION_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.", "In Charniak ( 1996 ) and #CITATION_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).", "The rate of accession may also be represented graphically. In Charniak ( 1996 ) and #CITATION_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).", "In Charniak ( 1996 ) and #CITATION_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined). Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count."], "CC471": ["Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #CITATION_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.", "Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #CITATION_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.", "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #CITATION_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \ufffdinverse schemata.", "Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #CITATION_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \ufffdinverse schemata.", "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #CITATION_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \ufffdinverse schemata. \ufffd"], "CC472": ["#CITATION_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank", "Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data. #CITATION_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank", "#CITATION_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.", "Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data. #CITATION_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.", "#CITATION_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank. Each of these sequences was categorized as a modifier or argument.", "Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data. #CITATION_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank. Each of these sequences was categorized as a modifier or argument.", "#CITATION_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank. Each of these sequences was categorized as a modifier or argument. Arguments were then mapped to traditional syntactic functions."], "CC473": ["#CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts.", "The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts.", "#CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames.", "This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts.", "The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames.", "#CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.", "Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts.", "This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames.", "The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.", "#CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees. The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.", "Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames.", "This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.", "The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees. The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.", "Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.", "This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #CITATION_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) . In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming\"s system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees. The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences."], "CC474": ["#CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).", "#CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner.", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner.", "#CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins\"s (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "#CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins\"s (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner.", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins\"s (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #CITATION_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099."], "CC475": [], "CC476": ["It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains", "The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains", "It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur.", "Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains", "The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur.", "It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus.", "Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres. Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains", "Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur.", "The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus.", "It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus. The most important of these was the way in which we distinguish between oblique and adjunct.", "Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres. Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur.", "Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus.", "The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus. The most important of these was the way in which we distinguish between oblique and adjunct.", "Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres. Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus.", "Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #CITATION_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus. The most important of these was the way in which we distinguish between oblique and adjunct."], "CC477": ["Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #CITATION_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) . In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).", "Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #CITATION_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) . In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches.", "Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #CITATION_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) . In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches. In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate."], "CC478": ["Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions.", "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions.", "Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions.", "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.", "Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.", "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.", "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.", "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #CITATION_TAG ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns."], "CC479": ["The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193.", "The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193.", "The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193. Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads.", "We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations. The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193.", "The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193. Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads.", "The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193. Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads. To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be the subject of the sentence (\u2191 SUBJ =\u2193), while the leftmost NP to the right of the V head of a VP is most probably the verb\"s object (\u2191 OBJ =\u2193).", "We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations. The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193. Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads.", "The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193. Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads. To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be the subject of the sentence (\u2191 SUBJ =\u2193), while the leftmost NP to the right of the V head of a VP is most probably the verb\"s object (\u2191 OBJ =\u2193).", "The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193. Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads. To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be the subject of the sentence (\u2191 SUBJ =\u2193), while the leftmost NP to the right of the V head of a VP is most probably the verb\"s object (\u2191 OBJ =\u2193). Cahill, McCarthy, et al. (2004) provide four classes of annotation principles: one for noncoordinate configurations, one for coordinate configurations, one for traces (long-distance dependencies), and a final \"catch all and clean up\" phase.", "We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations. The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193. Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads. To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be the subject of the sentence (\u2191 SUBJ =\u2193), while the leftmost NP to the right of the V head of a VP is most probably the verb\"s object (\u2191 OBJ =\u2193).", "The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter , for which an amended version of #CITATION_TAG is used . The head is annotated with the LFG equation \u2191=\u2193. Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads. To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be the subject of the sentence (\u2191 SUBJ =\u2193), while the leftmost NP to the right of the V head of a VP is most probably the verb\"s object (\u2191 OBJ =\u2193). Cahill, McCarthy, et al. (2004) provide four classes of annotation principles: one for noncoordinate configurations, one for coordinate configurations, one for traces (long-distance dependencies), and a final \"catch all and clean up\" phase."], "CC480": [], "CC483": ["Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ", "This can be expressed as a measure of the coverage of the induced lexicon on new data. Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ", "Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ We then compare this to a test lexicon from Section 23.", "In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced. This can be expressed as a measure of the coverage of the induced lexicon on new data. Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ", "This can be expressed as a measure of the coverage of the induced lexicon on new data. Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ We then compare this to a test lexicon from Section 23.", "Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ We then compare this to a test lexicon from Section 23. Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.", "In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced. This can be expressed as a measure of the coverage of the induced lexicon on new data. Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ We then compare this to a test lexicon from Section 23.", "This can be expressed as a measure of the coverage of the induced lexicon on new data. Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ We then compare this to a test lexicon from Section 23. Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.", "Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ We then compare this to a test lexicon from Section 23. Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only. There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23.", "In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced. This can be expressed as a measure of the coverage of the induced lexicon on new data. Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ We then compare this to a test lexicon from Section 23. Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.", "This can be expressed as a measure of the coverage of the induced lexicon on new data. Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #CITATION_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ We then compare this to a test lexicon from Section 23. Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only. There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23."], "CC484": ["#CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions.", "Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. #CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions.", "#CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.", "We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input. Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. #CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions.", "Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. #CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.", "#CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.", "We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input. Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. #CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.", "Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. #CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.", "#CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.", "We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input. Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. #CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.", "Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. #CITATION_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts."], "CC485": ["Lexical functional grammar ( #CITATION_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars", "Lexical functional grammar ( #CITATION_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.", "Lexical functional grammar ( #CITATION_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case. C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries."], "CC486": ["#CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon.", "As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon.", "#CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.", "In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon.", "As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.", "#CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame.", "For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject. In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon.", "In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.", "As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame.", "#CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.", "For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject. In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.", "In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame.", "As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.", "For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject. In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame.", "In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. #CITATION_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score."], "CC487": ["The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner.", "Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner.", "The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner.", "Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099. Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099. Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.", "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.", "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #CITATION_TAG . Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099. Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank."], "CC488": ["#CITATION_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization . In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.", "3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. #CITATION_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization . In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.", "As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. #CITATION_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization . In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function."], "CC489": ["In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #CITATION_TAG ) .", "Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX. In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #CITATION_TAG ) .", "We believe our semantic forms are fine-grained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations. Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX. In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #CITATION_TAG ) ."], "CC490": ["As noted above , it is well documented ( #CITATION_TAG ) that subcategorization frames ( and their frequencies ) vary across domains", "Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above , it is well documented ( #CITATION_TAG ) that subcategorization frames ( and their frequencies ) vary across domains", "As noted above , it is well documented ( #CITATION_TAG ) that subcategorization frames ( and their frequencies ) vary across domains We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.", "Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above , it is well documented ( #CITATION_TAG ) that subcategorization frames ( and their frequencies ) vary across domains We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.", "As noted above , it is well documented ( #CITATION_TAG ) that subcategorization frames ( and their frequencies ) vary across domains We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.", "Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above , it is well documented ( #CITATION_TAG ) that subcategorization frames ( and their frequencies ) vary across domains We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.", "As noted above , it is well documented ( #CITATION_TAG ) that subcategorization frames ( and their frequencies ) vary across domains We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases."], "CC492": ["In #CITATION_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank", "The rate of accession may also be represented graphically. In #CITATION_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank", "In #CITATION_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.", "The rate of accession may also be represented graphically. In #CITATION_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.", "In #CITATION_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).", "The rate of accession may also be represented graphically. In #CITATION_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).", "In #CITATION_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined). Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count."], "CC493": ["#CITATION_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature . Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.", "In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998). #CITATION_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature . Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.", "However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component. In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998). #CITATION_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature . Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue."], "CC494": [], "CC496": ["As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.", "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.", "As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%).", "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.", "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%).", "As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX.", "It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases. This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.", "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%).", "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX.", "As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.", "It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases. This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%).", "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX.", "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.", "It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases. This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX.", "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #CITATION_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26."], "CC498": ["#CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner )", "We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. #CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner )", "#CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 \" bolt, human assemblers \"unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt\" (Dale and Reiter 1995).", "Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication. We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. #CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner )", "We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. #CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 \" bolt, human assemblers \"unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt\" (Dale and Reiter 1995).", "#CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 \" bolt, human assemblers \"unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt\" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.", "Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication. We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. #CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 \" bolt, human assemblers \"unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt\" (Dale and Reiter 1995).", "We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. #CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 \" bolt, human assemblers \"unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt\" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.", "#CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 \" bolt, human assemblers \"unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt\" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction. In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles.", "Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication. We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. #CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 \" bolt, human assemblers \"unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt\" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.", "We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. #CITATION_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 \" bolt, human assemblers \"unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt\" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction. In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles."], "CC499": [], "CC500": ["Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #CITATION_TAG ) . The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large.", "Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #CITATION_TAG ) . The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Our own proposal will abstract away from the effects of linguistic context.", "Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #CITATION_TAG ) . The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words."], "CC501": [], "CC502": ["One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #CITATION_TAG ; Malouf 2000 ) . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives\" relative position.", "One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #CITATION_TAG ; Malouf 2000 ) . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives\" relative position. Interestingly, vague properties tend to be realized before others.", "One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #CITATION_TAG ; Malouf 2000 ) . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives\" relative position. Interestingly, vague properties tend to be realized before others. Quirk et al. (1985), for example, report that \"adjectives denoting size, length, and height normally precede other nonderived adjectives\" (e.g., the small round table is usually preferred to the round small table)."], "CC503": [], "CC504": ["Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #CITATION_TAG )", "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #CITATION_TAG ) We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.", "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #CITATION_TAG ) We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm . If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm)."], "CC505": ["This is the strongest version of the sorites paradox ( e.g. , #CITATION_TAG ) .", "A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it. This is the strongest version of the sorites paradox ( e.g. , #CITATION_TAG ) .", "Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between \"observationally indifferent\" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it. This is the strongest version of the sorites paradox ( e.g. , #CITATION_TAG ) ."], "CC506": ["We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #CITATION_TAG ) . Before we do this, consider the tractability of the original IA.", "We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #CITATION_TAG ) . Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes.", "We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #CITATION_TAG ) . Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes. This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002)."], "CC507": ["This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 )", "We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 )", "This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions.", "Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 )", "We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions.", "This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions. Here, the context tends to obliterate the vagueness associated with the adjective.", "Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact. Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 )", "Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions.", "We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions. Here, the context tends to obliterate the vagueness associated with the adjective.", "This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions. Here, the context tends to obliterate the vagueness associated with the adjective. Suppose you enter a vet\"s surgery in the company of two dogs: a big one on a leash, and a tiny one in your arms.", "Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact. Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions.", "Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions. Here, the context tends to obliterate the vagueness associated with the adjective.", "We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions. Here, the context tends to obliterate the vagueness associated with the adjective. Suppose you enter a vet\"s surgery in the company of two dogs: a big one on a leash, and a tiny one in your arms.", "Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact. Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions. Here, the context tends to obliterate the vagueness associated with the adjective.", "Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #CITATION_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions. Here, the context tends to obliterate the vagueness associated with the adjective. Suppose you enter a vet\"s surgery in the company of two dogs: a big one on a leash, and a tiny one in your arms."], "CC508": ["#CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle.", "Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? #CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle.", "#CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows.", "Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? #CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle.", "Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? #CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows.", "#CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable.", "Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? #CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows.", "Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? #CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable.", "#CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable. Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones.", "Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? #CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable.", "Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? #CITATION_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable. Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones."], "CC509": ["While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #CITATION_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen.", "While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #CITATION_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen. ) We shall see that vague descriptions pose particular challenges to incrementality.", "While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #CITATION_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen. ) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation."], "CC510": ["The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #CITATION_TAG ) . IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}."], "CC511": ["#CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.", "1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.", "#CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes.", "9.4. 1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.", "1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes.", "#CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience.", "As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous. 9.4. 1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.", "9.4. 1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes.", "1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience.", "#CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience. Then our algorithm might generate this list of properties: L = mouse, black, salience > 4", "As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous. 9.4. 1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes.", "9.4. 1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience.", "1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience. Then our algorithm might generate this list of properties: L = mouse, black, salience > 4", "As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous. 9.4. 1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience.", "9.4. 1 A New Perspective on Salience. #CITATION_TAG have argued that Dale and Reiter \"s ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience. Then our algorithm might generate this list of properties: L = mouse, black, salience > 4"], "CC512": ["Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #CITATION_TAG ) . These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?", "A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #CITATION_TAG ) . These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?", "Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #CITATION_TAG ) . These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little? , when two hats of different sizes are visible).", "Gradability is especially widespread in adjectives. A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #CITATION_TAG ) . These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?", "A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #CITATION_TAG ) . These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little? , when two hats of different sizes are visible)."], "CC513": ["The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #CITATION_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large", "Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #CITATION_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large", "The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #CITATION_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context.", "Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #CITATION_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context.", "The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #CITATION_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.", "Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #CITATION_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.", "The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #CITATION_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of"], "CC514": ["We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #CITATION_TAG , Krahmer and Theune 2002 ) . Before we do this, consider the tractability of the original IA.", "We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #CITATION_TAG , Krahmer and Theune 2002 ) . Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes.", "We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #CITATION_TAG , Krahmer and Theune 2002 ) . Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes. This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002)."], "CC515": ["Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle.", "Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle.", "Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows.", "Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle.", "Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows.", "Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable.", "Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows.", "Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable.", "Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable. Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones.", "Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable.", "Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch ( 1976 ; also reported in #CITATION_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking . In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say \"the tall candle\" when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to \"the fat candle. Hermann and Deutsch\"s findings may be implemented as follows. First, the Values of the different Attributes should be normalized to make them comparable. Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones."], "CC516": ["The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #CITATION_TAG )", "The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #CITATION_TAG ) But IA may be replaced by any other reasonable GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always \"greedily\" selects the property that removes the maximum number of distractors."], "CC517": ["Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole", "1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole", "Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.", "This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole", "1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.", "Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.", "Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees. This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole", "This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.", "1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.", "Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness. Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean.", "Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees. This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.", "This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.", "1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness. Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean.", "Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees. This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.", "This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #CITATION_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness. Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean."], "CC518": ["A similar problem is discussed in the psycholinguistics of interpretation ( #CITATION_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.", "This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989). A similar problem is discussed in the psycholinguistics of interpretation ( #CITATION_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.", "A similar problem is discussed in the psycholinguistics of interpretation ( #CITATION_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.", "This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989). A similar problem is discussed in the psycholinguistics of interpretation ( #CITATION_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.", "This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989). A similar problem is discussed in the psycholinguistics of interpretation ( #CITATION_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.", "If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected? This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989). A similar problem is discussed in the psycholinguistics of interpretation ( #CITATION_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.", "This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989). A similar problem is discussed in the psycholinguistics of interpretation ( #CITATION_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said."], "CC519": ["Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #CITATION_TAG ] Chapter 8 ) . Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions.", "In a vague description, the property last added to the description is context dependent. Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #CITATION_TAG ] Chapter 8 ) . Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions."], "CC520": ["Common sense ( as well as the Gricean maxims ; #CITATION_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication", "Common sense ( as well as the Gricean maxims ; #CITATION_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.", "Common sense ( as well as the Gricean maxims ; #CITATION_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. Dale and Reiter (1995), for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture (originally recorded by Candy Sidner)."], "CC521": ["Similar things hold for multifaceted properties like intelligence ( #CITATION_TAG ) .", "But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation. Similar things hold for multifaceted properties like intelligence ( #CITATION_TAG ) .", "If there exists a formula for mapping three dimensions into one (e.g., length \u00d7 width \u00d7 height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation. Similar things hold for multifaceted properties like intelligence ( #CITATION_TAG ) ."], "CC522": ["For some adjectives , including the ones that #CITATION_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate", "What we said above has also disregarded elements of the \"global\" (i.e., not immediately available) context. For some adjectives , including the ones that #CITATION_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate", "For some adjectives , including the ones that #CITATION_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.", "What we said above has also disregarded elements of the \"global\" (i.e., not immediately available) context. For some adjectives , including the ones that #CITATION_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.", "For some adjectives , including the ones that #CITATION_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms. For example (after Bierwisch 1989),"], "CC523": ["In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #CITATION_TAG ) .", "Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #CITATION_TAG ) ."], "CC524": ["In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #CITATION_TAG ) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database.", "If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #CITATION_TAG ) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database.", "In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #CITATION_TAG ) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE.", "If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #CITATION_TAG ) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE.", "In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #CITATION_TAG ) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE. Far from being a peculiarity of a few adjectives, vagueness is widespread.", "If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #CITATION_TAG ) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE. Far from being a peculiarity of a few adjectives, vagueness is widespread.", "In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #CITATION_TAG ) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE. Far from being a peculiarity of a few adjectives, vagueness is widespread. We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (Section 9.3); r nouns that allow different degrees of strictness (Section 9.5); r degrees of salience (Section 9.4); and r imprecise pointing (Section 9.5)."], "CC525": ["CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #CITATION_TAG , Section 8.6.", "For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #CITATION_TAG , Section 8.6.", "CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #CITATION_TAG , Section 8.6. 2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed . Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation (contains-a=b means that a is contained by b):", "Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #CITATION_TAG , Section 8.6.", "For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #CITATION_TAG , Section 8.6. 2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed . Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation (contains-a=b means that a is contained by b):"], "CC526": ["While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #CITATION_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen.", "While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #CITATION_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen. ) We shall see that vague descriptions pose particular challenges to incrementality.", "While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #CITATION_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen. ) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation."], "CC527": ["When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.", "1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.", "When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1.", "9.3 Multidimensionality 9.3. 1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.", "1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1.", "When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1. \u2203V i \u2208 V : V i (x) > V i (r) and 2.", "For example, the generator will have to decide whether to say the patients that are old or the patients that are not young. 9.3 Multidimensionality 9.3. 1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.", "9.3 Multidimensionality 9.3. 1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1.", "1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1. \u2203V i \u2208 V : V i (x) > V i (r) and 2.", "When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1. \u2203V i \u2208 V : V i (x) > V i (r) and 2. \u00ac\u2203V j \u2208 V : V j (x) < V j (r)", "For example, the generator will have to decide whether to say the patients that are old or the patients that are not young. 9.3 Multidimensionality 9.3. 1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1.", "9.3 Multidimensionality 9.3. 1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1. \u2203V i \u2208 V : V i (x) > V i (r) and 2.", "1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1. \u2203V i \u2208 V : V i (x) > V i (r) and 2. \u00ac\u2203V j \u2208 V : V j (x) < V j (r)", "For example, the generator will have to decide whether to say the patients that are old or the patients that are not young. 9.3 Multidimensionality 9.3. 1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1. \u2203V i \u2208 V : V i (x) > V i (r) and 2.", "9.3 Multidimensionality 9.3. 1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #CITATION_TAG ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r \u2208 C has a Pareto-optimal combination of Values V iff there is no other x \u2208 C such that 1. \u2203V i \u2208 V : V i (x) > V i (r) and 2. \u00ac\u2203V j \u2208 V : V j (x) < V j (r)"], "CC528": ["While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #CITATION_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen.", "While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #CITATION_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen. ) We shall see that vague descriptions pose particular challenges to incrementality.", "While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #CITATION_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen. ) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation."], "CC529": [") If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans\ufffds and Fritz\ufffds heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #CITATION_TAG 1999, discussed in Section 7.2).", "(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right. ) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans\ufffds and Fritz\ufffds heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #CITATION_TAG 1999, discussed in Section 7.2).", ") If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans\ufffds and Fritz\ufffds heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #CITATION_TAG 1999, discussed in Section 7.2). Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary.", "We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension. (For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right. ) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans\ufffds and Fritz\ufffds heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #CITATION_TAG 1999, discussed in Section 7.2).", "(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right. ) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans\ufffds and Fritz\ufffds heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #CITATION_TAG 1999, discussed in Section 7.2). Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary."], "CC530": ["NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #CITATION_TAG ) : The selected expression should also be felicitous . Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between \"observationally indifferent\" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not?", "NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #CITATION_TAG ) : The selected expression should also be felicitous . Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between \"observationally indifferent\" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it."], "CC531": ["One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #CITATION_TAG ) . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives\" relative position.", "One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #CITATION_TAG ) . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives\" relative position. Interestingly, vague properties tend to be realized before others.", "One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #CITATION_TAG ) . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives\" relative position. Interestingly, vague properties tend to be realized before others. Quirk et al. (1985), for example, report that \"adjectives denoting size, length, and height normally precede other nonderived adjectives\" (e.g., the small round table is usually preferred to the round small table)."], "CC532": [], "CC533": ["The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #CITATION_TAG ; Thorisson 1994 , for other plans ) .", "Many alternative strategies are possible. The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #CITATION_TAG ; Thorisson 1994 , for other plans ) .", "For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #CITATION_TAG ; Thorisson 1994 , for other plans ) ."], "CC534": ["The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #CITATION_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large", "Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #CITATION_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large", "The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #CITATION_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context.", "Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #CITATION_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context.", "The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #CITATION_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.", "Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #CITATION_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.", "The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #CITATION_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of"], "CC537": ["The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #CITATION_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .", "Many alternative strategies are possible. The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #CITATION_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .", "For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #CITATION_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) ."], "CC539": ["A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997).", "FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997).", "A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart\ufffds piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart\ufffds sonatas.", "The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997).", "FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart\ufffds piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart\ufffds sonatas.", "A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart\ufffds piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart\ufffds sonatas. The sonata was called a famous sonata if x >> y. Like DYD, the work reported in this article will abandon the use of fixed boundary values for gradable adjectives, letting these values depend on the context in which the adjective is used.", "Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997).", "The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart\ufffds piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart\ufffds sonatas.", "FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart\ufffds piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart\ufffds sonatas. The sonata was called a famous sonata if x >> y. Like DYD, the work reported in this article will abandon the use of fixed boundary values for gradable adjectives, letting these values depend on the context in which the adjective is used.", "Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart\ufffds piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart\ufffds sonatas.", "The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by #CITATION_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997). To determine, for example, whether one of Mozart\ufffds piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart\ufffds sonatas. The sonata was called a famous sonata if x >> y. Like DYD, the work reported in this article will abandon the use of fixed boundary values for gradable adjectives, letting these values depend on the context in which the adjective is used."], "CC540": ["Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #CITATION_TAG ; Krahmer and Theune 2002 )", "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #CITATION_TAG ; Krahmer and Theune 2002 ) We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.", "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #CITATION_TAG ; Krahmer and Theune 2002 ) We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm . If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm)."], "CC541": ["While IA is generally thought to be consistent with findings on human language production ( #CITATION_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen.", "While IA is generally thought to be consistent with findings on human language production ( #CITATION_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen. ) We shall see that vague descriptions pose particular challenges to incrementality.", "While IA is generally thought to be consistent with findings on human language production ( #CITATION_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . (Wildly redundant descriptions can result if the \"wrong\" preference order are chosen. ) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation."], "CC542": [") A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.", "(The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.", ") A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair.", "This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.", "(The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair.", ") A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair. ) Different attitudes towards multidimensionality are possible.", "If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue). This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.", "This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair.", "(The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair. ) Different attitudes towards multidimensionality are possible.", ") A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair. ) Different attitudes towards multidimensionality are possible. One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense.", "If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue). This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair.", "This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair. ) Different attitudes towards multidimensionality are possible.", "(The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair. ) Different attitudes towards multidimensionality are possible. One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense.", "If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue). This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair. ) Different attitudes towards multidimensionality are possible.", "This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green. ) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #CITATION_TAG , pages 10 -- 12 ) . (Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent. Red as in red hair, e.g., differs from red as in red chair. ) Different attitudes towards multidimensionality are possible. One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense."], "CC543": ["#CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup.", "Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. #CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup.", "#CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).", "It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input. Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. #CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup.", "Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. #CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).", "#CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key). Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied \"intrinsically\" to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations.", "It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input. Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. #CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).", "Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. #CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key). Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied \"intrinsically\" to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations.", "#CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key). Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied \"intrinsically\" to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations. The time subjects took before looking at the target for the first time was measured, and although these latency times were somewhat greater when the referent were not intrinsically tall than when they were, the average difference was tiny at 554 versus 538 miliseconds.", "It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input. Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. #CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key). Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied \"intrinsically\" to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations.", "Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects. #CITATION_TAG asked subjects to identify the target of a vague description in a visual scene . Consider the tall cup. The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key). Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied \"intrinsically\" to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations. The time subjects took before looking at the target for the first time was measured, and although these latency times were somewhat greater when the referent were not intrinsically tall than when they were, the average difference was tiny at 554 versus 538 miliseconds."], "CC544": ["It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #CITATION_TAG )", "It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #CITATION_TAG ) We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.", "It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #CITATION_TAG ) We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions. Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes)."], "CC545": [], "CC546": ["In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language \"\" into account ( #CITATION_TAG ; see our Section 2 ) .", "Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless. In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language \"\" into account ( #CITATION_TAG ; see our Section 2 ) .", "Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it. Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless. In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language \"\" into account ( #CITATION_TAG ; see our Section 2 ) ."], "CC548": ["Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #CITATION_TAG )."], "CC549": ["Although originally developed as a tool to assist in query formulation , #CITATION_TAG pointed out that PICO frames can be employed to structure IR results for improving precision . PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).", "Cimino and Mendon\u00e7a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation , #CITATION_TAG pointed out that PICO frames can be employed to structure IR results for improving precision . PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).", "Although originally developed as a tool to assist in query formulation , #CITATION_TAG pointed out that PICO frames can be employed to structure IR results for improving precision . PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision.", "The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994;Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendon\u00e7a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation , #CITATION_TAG pointed out that PICO frames can be employed to structure IR results for improving precision . PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).", "Cimino and Mendon\u00e7a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation , #CITATION_TAG pointed out that PICO frames can be employed to structure IR results for improving precision . PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision.", "Based on analyses of 4,000 MEDLINE citations, Mendon\u00e7a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994;Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendon\u00e7a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation , #CITATION_TAG pointed out that PICO frames can be employed to structure IR results for improving precision . PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).", "The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994;Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendon\u00e7a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formulation , #CITATION_TAG pointed out that PICO frames can be employed to structure IR results for improving precision . PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision."], "CC550": ["We first identified the most informative unigrams and bigrams using the information gain measure ( #CITATION_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.", "The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure ( #CITATION_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.", "We first identified the most informative unigrams and bigrams using the information gain measure ( #CITATION_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort.", "The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure ( #CITATION_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort.", "We first identified the most informative unigrams and bigrams using the information gain measure ( #CITATION_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort. This classifier also outputs the probability of a class assignment."], "CC551": ["The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians \" queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #CITATION_TAG ) . Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.", "In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs. The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians \" queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #CITATION_TAG ) . Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.", "The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians \" queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #CITATION_TAG ) . Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project."], "CC552": ["For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #CITATION_TAG ) . Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance.", "The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #CITATION_TAG ) . Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance.", "For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #CITATION_TAG ) . Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.", "The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #CITATION_TAG ) . Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.", "For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #CITATION_TAG ) . Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering."], "CC553": ["We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #CITATION_TAGb ) , but these features are also beyond the capabilities of current summarization systems .", "The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion-it need not be repeated unless the physician wishes to \"drill down\"; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #CITATION_TAGb ) , but these features are also beyond the capabilities of current summarization systems .", "Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences. The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion-it need not be repeated unless the physician wishes to \"drill down\"; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #CITATION_TAGb ) , but these features are also beyond the capabilities of current summarization systems ."], "CC555": ["However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #CITATION_TAG ) . Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.", "MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians\" questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003). However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #CITATION_TAG ) . Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.", "However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #CITATION_TAG ) . Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts. Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.", "Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005. MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians\" questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003). However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #CITATION_TAG ) . Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.", "MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians\" questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003). However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #CITATION_TAG ) . Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts. Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care."], "CC556": ["Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #CITATION_TAG ) . Nevertheless, the indexing process remains firmly human-centered.", "Indexing is performed by approximately 100 indexers with at least bachelor\"s degrees in life sciences and formal training in indexing provided by NLM. Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #CITATION_TAG ) . Nevertheless, the indexing process remains firmly human-centered.", "NLM\"s controlled vocabulary thesaurus, Medical Subject Headings (MeSH), 2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a separate thesaurus. Indexing is performed by approximately 100 indexers with at least bachelor\"s degrees in life sciences and formal training in indexing provided by NLM. Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #CITATION_TAG ) . Nevertheless, the indexing process remains firmly human-centered."], "CC558": ["The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #CITATION_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005"], "CC561": ["The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #CITATION_TAG", "The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #CITATION_TAG Their study also illustrates the importance of semantic classes and relations.", "The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #CITATION_TAG Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope)."], "CC563": ["The work of #CITATION_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .", "PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989). The work of #CITATION_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .", "Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989). The work of #CITATION_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision ."], "CC565": ["Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #CITATION_TAG , 2005 ) . MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians\" questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003).", "Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #CITATION_TAG , 2005 ) . MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians\" questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).", "Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #CITATION_TAG , 2005 ) . MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians\" questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts."], "CC566": ["We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #CITATION_TAG and Lin (2006).", "Finally, answer generation remains an area that awaits further exploration, although we would have to first define what a good answer should be. We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #CITATION_TAG and Lin (2006).", "We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #CITATION_TAG and Lin (2006). Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements.", "Finally, answer generation remains an area that awaits further exploration, although we would have to first define what a good answer should be. We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #CITATION_TAG and Lin (2006). Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements.", "We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #CITATION_TAG and Lin (2006). Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements. To address these very difficult challenges, finer-grained semantic analysis of medical texts is required."], "CC567": [], "CC568": ["We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #CITATION_TAG ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.", "The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #CITATION_TAG ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.", "We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #CITATION_TAG ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort.", "The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #CITATION_TAG ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort.", "We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #CITATION_TAG ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort. This classifier also outputs the probability of a class assignment."], "CC569": ["Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #CITATION_TAG .", "Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #CITATION_TAG .", "Furthermore, it is unclear if textual strings make \"good answers. Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #CITATION_TAG ."], "CC570": ["Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other \"\" questions ( #CITATION_TAG ) . A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b).", "Recently, there is a growing consensus that an evaluation methodology based on the notion of \"information nuggets\" may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other \"\" questions ( #CITATION_TAG ) . A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b).", "Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other \"\" questions ( #CITATION_TAG ) . A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.", "In Sections 9 and 10, we have discussed many of these issues. Recently, there is a growing consensus that an evaluation methodology based on the notion of \"information nuggets\" may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other \"\" questions ( #CITATION_TAG ) . A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b).", "Recently, there is a growing consensus that an evaluation methodology based on the notion of \"information nuggets\" may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other \"\" questions ( #CITATION_TAG ) . A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.", "Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems. In Sections 9 and 10, we have discussed many of these issues. Recently, there is a growing consensus that an evaluation methodology based on the notion of \"information nuggets\" may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other \"\" questions ( #CITATION_TAG ) . A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b).", "In Sections 9 and 10, we have discussed many of these issues. Recently, there is a growing consensus that an evaluation methodology based on the notion of \"information nuggets\" may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other \"\" questions ( #CITATION_TAG ) . A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken."], "CC571": ["In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #CITATION_TAGa ) for a brief overview .", "For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #CITATION_TAGa ) for a brief overview .", "In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #CITATION_TAGa ) for a brief overview ."], "CC572": ["A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #CITATION_TAGa , 2006b ) . However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.", "Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and \"other\" questions (Voorhees 2003). A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #CITATION_TAGa , 2006b ) . However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.", "Recently, there is a growing consensus that an evaluation methodology based on the notion of \"information nuggets\" may provide an appropriate framework for assessing the quality of answers to complex questions. Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and \"other\" questions (Voorhees 2003). A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #CITATION_TAGa , 2006b ) . However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken."], "CC573": ["Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #CITATION_TAG ) ."], "CC574": ["MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians \" questions , and is commonly used in that capacity ( #CITATION_TAG ; De Groote and Dorsch 2003 ) . However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).", "Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005. MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians \" questions , and is commonly used in that capacity ( #CITATION_TAG ; De Groote and Dorsch 2003 ) . However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).", "MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians \" questions , and is commonly used in that capacity ( #CITATION_TAG ; De Groote and Dorsch 2003 ) . However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.", "Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005. MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians \" questions , and is commonly used in that capacity ( #CITATION_TAG ; De Groote and Dorsch 2003 ) . However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.", "MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians \" questions , and is commonly used in that capacity ( #CITATION_TAG ; De Groote and Dorsch 2003 ) . However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts. Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care."], "CC575": ["For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #CITATION_TAG ) . In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview.", "In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #CITATION_TAG ) . In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview.", "As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #CITATION_TAG ) . In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview."], "CC576": ["Third , the paradigm of evidence-based medicine ( #CITATION_TAG ) provides a task-based model of the clinical information-seeking process . The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.", "The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third , the paradigm of evidence-based medicine ( #CITATION_TAG ) provides a task-based model of the clinical information-seeking process . The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.", "Third , the paradigm of evidence-based medicine ( #CITATION_TAG ) provides a task-based model of the clinical information-seeking process . The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The confluence of these many factors makes clinical question answering a very exciting area of research.", "The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third , the paradigm of evidence-based medicine ( #CITATION_TAG ) provides a task-based model of the clinical information-seeking process . The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.", "The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third , the paradigm of evidence-based medicine ( #CITATION_TAG ) provides a task-based model of the clinical information-seeking process . The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The confluence of these many factors makes clinical question answering a very exciting area of research.", "Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third , the paradigm of evidence-based medicine ( #CITATION_TAG ) provides a task-based model of the clinical information-seeking process . The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.", "The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third , the paradigm of evidence-based medicine ( #CITATION_TAG ) provides a task-based model of the clinical information-seeking process . The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The confluence of these many factors makes clinical question answering a very exciting area of research."], "CC577": ["As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #CITATION_TAG ) .", "However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #CITATION_TAG ) ."], "CC578": ["As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #CITATION_TAG ; Hirschman and Gaizauskas 2001 ) .", "However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #CITATION_TAG ; Hirschman and Gaizauskas 2001 ) ."], "CC579": ["For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #CITATION_TAG .", "For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers\" tasks in assigning terms is to identify the main topic of the article (sometimes a disorder). For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #CITATION_TAG ."], "CC581": ["Our knowledge extractors rely extensively on MetaMap ( #CITATION_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus . Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).", "Our knowledge extractors rely extensively on MetaMap ( #CITATION_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus . Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS). An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.", "Our knowledge extractors rely extensively on MetaMap ( #CITATION_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus . Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS). An additional feature we take advantage of (when present) is explicit section markers present in some abstracts. These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes."], "CC582": ["This section , which elaborates on preliminary results reported in #CITATION_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence . For an example of a completely annotated abstract, see Figure 2.", "The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section , which elaborates on preliminary results reported in #CITATION_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence . For an example of a completely annotated abstract, see Figure 2.", "This section , which elaborates on preliminary results reported in #CITATION_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence . For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases."], "CC583": ["Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #CITATION_TAG ) ."], "CC584": ["The PICO framework ( #CITATION_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system . The confluence of these many factors makes clinical question answering a very exciting area of research.", "Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process. The PICO framework ( #CITATION_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system . The confluence of these many factors makes clinical question answering a very exciting area of research.", "The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process. The PICO framework ( #CITATION_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system . The confluence of these many factors makes clinical question answering a very exciting area of research."], "CC586": [], "CC587": [], "CC588": ["Our re-ranking approach , like the approach to parse re-ranking of #CITATION_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes . The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings.", "To avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables. Our re-ranking approach , like the approach to parse re-ranking of #CITATION_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes . The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings.", "To tackle the efficiency problem, we adopt dynamic programming and re-ranking algorithms. To avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables. Our re-ranking approach , like the approach to parse re-ranking of #CITATION_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes . The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings."], "CC589": ["Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions", "Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions", "Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.", "For argument labeling, the number of possible assignments is \u2248 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions", "Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.", "Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.", "This number can run into the hundreds of billions for a normal-sized tree. For argument labeling, the number of possible assignments is \u2248 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions", "For argument labeling, the number of possible assignments is \u2248 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.", "Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.", "Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance. We used a value of n = 10 for training.", "This number can run into the hundreds of billions for a normal-sized tree. For argument labeling, the number of possible assignments is \u2248 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.", "For argument labeling, the number of possible assignments is \u2248 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.", "Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance. We used a value of n = 10 for training.", "This number can run into the hundreds of billions for a normal-sized tree. For argument labeling, the number of possible assignments is \u2248 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.", "For argument labeling, the number of possible assignments is \u2248 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #CITATION_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance. We used a value of n = 10 for training."], "CC590": ["Following our previous work ( #CITATION_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists . A set of candidate orderings is produced by creating different permutations of these lists.", "Following our previous work ( #CITATION_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists . A set of candidate orderings is produced by creating different permutations of these lists. A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output.", "Following our previous work ( #CITATION_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists . A set of candidate orderings is produced by creating different permutations of these lists. A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9 A wide range of metrics of coherence can be defined in centering\"s terms, simply on the basis of the work we reviewed in Section 3."], "CC591": ["Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #CITATION_TAG", "Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #CITATION_TAG We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.", "Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #CITATION_TAG We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants. Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word."], "CC592": ["We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #CITATION_TAG ) ."], "CC593": ["Such technologies require significant human input , and are difficult to create and maintain ( #CITATION_TAG ) . In contrast, the techniques examined in this article are corpus-based and data-driven.", "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997). Such technologies require significant human input , and are difficult to create and maintain ( #CITATION_TAG ) . In contrast, the techniques examined in this article are corpus-based and data-driven.", "Such technologies require significant human input , and are difficult to create and maintain ( #CITATION_TAG ) . In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus."], "CC595": ["This method follows a traditional Information Retrieval paradigm ( #CITATION_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query . In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus:", "(Doc-Ret). This method follows a traditional Information Retrieval paradigm ( #CITATION_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query . In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus:"], "CC596": ["For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero.", "We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero.", "For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses.", "The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero.", "We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses.", "For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place.", "The first column shows which document retrieval variant is being evaluated. The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero.", "The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses.", "We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place.", "For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place. Here too the third variant yields the best similarity score (0.52).", "The first column shows which document retrieval variant is being evaluated. The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses.", "The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place.", "We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place. Here too the third variant yields the best similarity score (0.52).", "The first column shows which document retrieval variant is being evaluated. The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place.", "The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #CITATION_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place. Here too the third variant yields the best similarity score (0.52)."], "CC597": ["This situation suggests a response-automation approach that follows the document retrieval paradigm ( #CITATION_TAG ) , where a new request is matched with existing response documents ( e-mails ) . However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively."], "CC598": ["It is therefore no surprise that early attempts at response automation were knowledge-driven ( #CITATION_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) . These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).", "Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven ( #CITATION_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) . These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998)."], "CC599": ["We then use the program Snob ( Wallace and Boulton 1968 ; #CITATION_TAG ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.", "We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively). We then use the program Snob ( Wallace and Boulton 1968 ; #CITATION_TAG ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.", "We then use the program Snob ( Wallace and Boulton 1968 ; #CITATION_TAG ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently).", "We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively). We then use the program Snob ( Wallace and Boulton 1968 ; #CITATION_TAG ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently).", "We then use the program Snob ( Wallace and Boulton 1968 ; #CITATION_TAG ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15 These clusters were chosen because they illustrate clearly three situations of interest."], "CC601": ["7 We employed the LIBSVM package ( #CITATION_TAG ) . prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.", "During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package ( #CITATION_TAG ) . prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.", "7 We employed the LIBSVM package ( #CITATION_TAG ) . prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps.", "7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package ( #CITATION_TAG ) . prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.", "During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package ( #CITATION_TAG ) . prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps.", "We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users\" requests. 7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package ( #CITATION_TAG ) . prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.", "7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package ( #CITATION_TAG ) . prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps."], "CC602": ["In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #CITATION_TAG )", "The idea behind the Doc-Pred method is similar to Bickel and Scheffer\"s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request\"s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #CITATION_TAG )", "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #CITATION_TAG ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.", "The idea behind the Doc-Pred method is similar to Bickel and Scheffer\"s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request\"s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #CITATION_TAG ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.", "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #CITATION_TAG ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2. 2).", "The idea behind the Doc-Pred method is similar to Bickel and Scheffer\"s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request\"s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #CITATION_TAG ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2. 2).", "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #CITATION_TAG ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2. 2). The input to Snob is a set of binary vectors, one vector per response document."], "CC604": ["The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #CITATION_TAG ) . Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).", "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #CITATION_TAG ) . Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998). In contrast, the techniques examined in this article are corpus-based and data-driven."], "CC605": ["#CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.", "In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.", "#CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.", "This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.", "In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.", "#CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).", "Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer. This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.", "This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.", "In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).", "Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer. This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.", "This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #CITATION_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts)."], "CC606": [], "CC607": ["Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.", "This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.", "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.", "An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.", "This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.", "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.", "In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.", "An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.", "This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.", "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.", "In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.", "An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.", "This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.", "In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.", "An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard."], "CC608": ["Two applications that, like help-desk, deal with question\ufffdanswer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #CITATION_TAG ) . An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.", "Two applications that, like help-desk, deal with question\ufffdanswer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #CITATION_TAG ) . An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information."], "CC609": [], "CC610": ["Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #CITATION_TAG ) . However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur.", "In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges\" assessments would be comparable. Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #CITATION_TAG ) . However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur.", "14 e maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges. In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges\" assessments would be comparable. Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #CITATION_TAG ) . However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur."], "CC611": ["In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #CITATION_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .", "Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #CITATION_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .", "This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #CITATION_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) ."], "CC612": ["A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #CITATION_TAG ) . However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred).", "A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #CITATION_TAG ) . However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence.", "A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #CITATION_TAG ) . However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence. Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance."], "CC614": [], "CC616": ["In #CITATION_TAGa ) we identified several systems that resemble ours in that they provide answers to queries . These systems addressed the evaluation issue as follows."], "CC617": ["In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #CITATION_TAG ; Wallace 2005 )", "The idea behind the Doc-Pred method is similar to Bickel and Scheffer\"s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request\"s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #CITATION_TAG ; Wallace 2005 )", "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #CITATION_TAG ; Wallace 2005 ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.", "The idea behind the Doc-Pred method is similar to Bickel and Scheffer\"s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request\"s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #CITATION_TAG ; Wallace 2005 ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.", "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #CITATION_TAG ; Wallace 2005 ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2. 2).", "The idea behind the Doc-Pred method is similar to Bickel and Scheffer\"s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request\"s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #CITATION_TAG ; Wallace 2005 ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2. 2).", "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #CITATION_TAG ; Wallace 2005 ) We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2. 2). The input to Snob is a set of binary vectors, one vector per response document."], "CC618": ["In FAQs , #CITATION_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document", "In FAQs , #CITATION_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.", "In FAQs , #CITATION_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. compared two retrieval approaches (TF."], "CC619": [], "CC620": ["Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.", "This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.", "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.", "An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.", "This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.", "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.", "In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.", "An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.", "This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.", "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.", "In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization.", "An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.", "This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.", "In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.", "An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #CITATION_TAG ; Malik , Subramaniam , and Kaushik 2007 ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard."], "CC622": ["After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #CITATION_TAG to penalize redundant sentences in cohesive clusters", "Removing redundant sentences. After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #CITATION_TAG to penalize redundant sentences in cohesive clusters", "After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #CITATION_TAG to penalize redundant sentences in cohesive clusters This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented).", "Removing redundant sentences. After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #CITATION_TAG to penalize redundant sentences in cohesive clusters This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented).", "After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #CITATION_TAG to penalize redundant sentences in cohesive clusters This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented). Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score(s k ) (= Pr(SC l ) \u00d7 Pr(s k |SC l )) is subtracted from Score(s k ).", "Removing redundant sentences. After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #CITATION_TAG to penalize redundant sentences in cohesive clusters This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented). Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score(s k ) (= Pr(SC l ) \u00d7 Pr(s k |SC l )) is subtracted from Score(s k ).", "After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #CITATION_TAG to penalize redundant sentences in cohesive clusters This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented). Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score(s k ) (= Pr(SC l ) \u00d7 Pr(s k |SC l )) is subtracted from Score(s k ). After applying these penalties, we retain only the sentences whose adjusted score is greater than zero (for a highly cohesive cluster, typically only one sentence remains)."], "CC623": ["4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature.", "The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature.", "4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.", "The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature.", "The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.", "4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request.", "2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature.", "The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.", "The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request.", "4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request. As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).", "2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.", "The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request.", "The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request. As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).", "2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request.", "The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency). 4 The predictive model is a Decision Graph ( #CITATION_TAG ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request. As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3)."], "CC625": ["There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.", "There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response.", "There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #CITATION_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses."], "CC627": ["#CITATION_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval . Two significant differences between help-desk and FAQs are the following.", "Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques. #CITATION_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval . Two significant differences between help-desk and FAQs are the following.", "IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques. #CITATION_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval . Two significant differences between help-desk and FAQs are the following."], "CC628": ["6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #CITATION_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results . 7 We employed the LIBSVM package (Chang and Lin 2001).", "7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #CITATION_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results . 7 We employed the LIBSVM package (Chang and Lin 2001).", "6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #CITATION_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results . 7 We employed the LIBSVM package (Chang and Lin 2001). prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.", "We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users\" requests. 7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #CITATION_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results . 7 We employed the LIBSVM package (Chang and Lin 2001).", "7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #CITATION_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results . 7 We employed the LIBSVM package (Chang and Lin 2001). prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.", "6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #CITATION_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results . 7 We employed the LIBSVM package (Chang and Lin 2001). prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps.", "We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users\" requests. 7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #CITATION_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results . 7 We employed the LIBSVM package (Chang and Lin 2001). prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.", "7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #CITATION_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results . 7 We employed the LIBSVM package (Chang and Lin 2001). prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps."], "CC631": ["It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #CITATION_TAG ; Delic and Lahaix 1998 ) . These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).", "Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #CITATION_TAG ; Delic and Lahaix 1998 ) . These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998)."], "CC634": [], "CC635": ["In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #CITATION_TAG ) .", "Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #CITATION_TAG ) .", "This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #CITATION_TAG ) ."], "CC636": ["Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #CITATION_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).", "Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #CITATION_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.", "Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #CITATION_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information."], "CC638": [], "CC640": ["In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #CITATION_TAGa )", "In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #CITATION_TAGa ) Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator."], "CC641": ["The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected.", "Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected.", "The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.", "A similar approach is taken in Rotaru and Litman\"s (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected.", "Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.", "The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method.", "More specifically, it belongs to Burke\"s switching sub-category, where a single method is selected on a case-by-case basis. A similar approach is taken in Rotaru and Litman\"s (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected.", "A similar approach is taken in Rotaru and Litman\"s (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.", "Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method.", "More specifically, it belongs to Burke\"s switching sub-category, where a single method is selected on a case-by-case basis. A similar approach is taken in Rotaru and Litman\"s (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.", "A similar approach is taken in Rotaru and Litman\"s (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #CITATION_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke \"s cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method."], "CC642": ["But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #CITATION_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .", "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #CITATION_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .", "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #CITATION_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) ."], "CC643": ["Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #CITATION_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable . In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.", "Here \u03b7 is an optimization precision, \u03b1 is a step size chosen with the strong Wolfe\"s rule (Nocedal and Wright 1999). Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #CITATION_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable . In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.", "We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here \u03b7 is an optimization precision, \u03b1 is a step size chosen with the strong Wolfe\"s rule (Nocedal and Wright 1999). Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #CITATION_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable . In practice this only happens at the start of optimization and we use a sub-gradient for the first direction."], "CC645": ["Many researchers use the GIZA + + software package ( #CITATION_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency . All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word).", "IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend. Many researchers use the GIZA + + software package ( #CITATION_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency . All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word).", "Many researchers use the GIZA + + software package ( #CITATION_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency . All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word). Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%).", "IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend. Many researchers use the GIZA + + software package ( #CITATION_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency . All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word).", "IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend. Many researchers use the GIZA + + software package ( #CITATION_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency . All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word). Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%).", "Many researchers use the GIZA + + software package ( #CITATION_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency . All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word). Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%). This leads to the common practice of post-processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003).", "IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend. Many researchers use the GIZA + + software package ( #CITATION_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency . All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word). Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%).", "IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend. Many researchers use the GIZA + + software package ( #CITATION_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency . All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word). Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%). This leads to the common practice of post-processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003)."], "CC646": ["Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe \"s rule ( #CITATION_TAG ) . Here, \u03b2\u2207(\u03bb) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when \u03bb = 0, the objective is not differentiable.", "We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe \"s rule ( #CITATION_TAG ) . Here, \u03b2\u2207(\u03bb) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when \u03bb = 0, the objective is not differentiable.", "Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe \"s rule ( #CITATION_TAG ) . Here, \u03b2\u2207(\u03bb) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when \u03bb = 0, the objective is not differentiable. In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.", "Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of \u03bb. We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe \"s rule ( #CITATION_TAG ) . Here, \u03b2\u2207(\u03bb) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when \u03bb = 0, the objective is not differentiable.", "We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe \"s rule ( #CITATION_TAG ) . Here, \u03b2\u2207(\u03bb) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when \u03bb = 0, the objective is not differentiable. In practice this only happens at the start of optimization and we use a sub-gradient for the first direction."], "CC647": ["This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.", "One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.", "This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors.", "Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.", "One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors.", "This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).", "Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors.", "One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).", "This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model). Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.", "Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).", "One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #CITATION_TAG ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model). Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior."], "CC649": ["results are based on a corpus of movie subtitles ( #CITATION_TAG ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 )", "Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( #CITATION_TAG ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 )", "results are based on a corpus of movie subtitles ( #CITATION_TAG ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.", "Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( #CITATION_TAG ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 )", "Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( #CITATION_TAG ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.", "We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish. Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( #CITATION_TAG ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 )", "Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( #CITATION_TAG ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM."], "CC650": ["Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #CITATION_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).", "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #CITATION_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).", "Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #CITATION_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001;Hwa et al. 2005;Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).", "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #CITATION_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).", "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #CITATION_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001;Hwa et al. 2005;Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008)."], "CC652": ["We used a standard implementation of IBM Model 4 ( #CITATION_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves", "Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements. We used a standard implementation of IBM Model 4 ( #CITATION_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves", "We used a standard implementation of IBM Model 4 ( #CITATION_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves We trained IBM Model 4 using the default configuration of the", "However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference. Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements. We used a standard implementation of IBM Model 4 ( #CITATION_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves", "Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements. We used a standard implementation of IBM Model 4 ( #CITATION_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves We trained IBM Model 4 using the default configuration of the", "We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference. However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference. Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements. We used a standard implementation of IBM Model 4 ( #CITATION_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves", "However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference. Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements. We used a standard implementation of IBM Model 4 ( #CITATION_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves We trained IBM Model 4 using the default configuration of the"], "CC653": ["But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #CITATION_TAG ) .", "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #CITATION_TAG ) .", "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #CITATION_TAG ) ."], "CC654": ["The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.", "In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.", "The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: \u2212 \u2192 p ( \u2212 \u2192 z ) (source-target) and \u2190 \u2212 p ( \u2190 \u2212 z ) (target-source).", "The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations. In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.", "In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: \u2212 \u2192 p ( \u2212 \u2192 z ) (source-target) and \u2190 \u2212 p ( \u2190 \u2212 z ) (target-source).", "The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: \u2212 \u2192 p ( \u2212 \u2192 z ) (source-target) and \u2190 \u2212 p ( \u2190 \u2212 z ) (target-source). We suppress dependence on x and y for brevity.", "The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations. In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: \u2212 \u2192 p ( \u2212 \u2192 z ) (source-target) and \u2190 \u2212 p ( \u2190 \u2212 z ) (target-source).", "In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: \u2212 \u2192 p ( \u2212 \u2192 z ) (source-target) and \u2190 \u2212 p ( \u2190 \u2212 z ) (target-source). We suppress dependence on x and y for brevity.", "The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: \u2212 \u2192 p ( \u2212 \u2192 z ) (source-target) and \u2190 \u2212 p ( \u2190 \u2212 z ) (target-source). We suppress dependence on x and y for brevity. Define z to range over the union of all possible directional alignments \u2212 \u2192 Z \u222a \u2190 \u2212 Z", "The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations. In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: \u2212 \u2192 p ( \u2212 \u2192 z ) (source-target) and \u2190 \u2212 p ( \u2190 \u2212 z ) (target-source). We suppress dependence on x and y for brevity.", "In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #CITATION_TAG ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: \u2212 \u2192 p ( \u2212 \u2192 z ) (source-target) and \u2190 \u2212 p ( \u2190 \u2212 z ) (target-source). We suppress dependence on x and y for brevity. Define z to range over the union of all possible directional alignments \u2212 \u2192 Z \u222a \u2190 \u2212 Z"], "CC655": ["EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #CITATION_TAG ) :"], "CC656": ["This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.", "One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.", "This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors.", "Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.", "One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors.", "This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).", "Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors.", "One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).", "This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model). Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.", "Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).", "One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #CITATION_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Gra\u00e7a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model\"s posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model). Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior."], "CC657": ["In the context of word alignment , #CITATION_TAG use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states.", "The idea of introducing constraints over a model to better guide the learning process has appeared before. In the context of word alignment , #CITATION_TAG use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states.", "In the context of word alignment , #CITATION_TAG use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1.", "The idea of introducing constraints over a model to better guide the learning process has appeared before. In the context of word alignment , #CITATION_TAG use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1.", "In the context of word alignment , #CITATION_TAG use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases.", "The idea of introducing constraints over a model to better guide the learning process has appeared before. In the context of word alignment , #CITATION_TAG use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases.", "In the context of word alignment , #CITATION_TAG use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing, Smith and Eisner (2006) add a constraint of the form \"the average length of dependencies should be X\" to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing."], "CC658": ["results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( #CITATION_TAG )", "Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( #CITATION_TAG )", "results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( #CITATION_TAG ) We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.", "Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( #CITATION_TAG )", "Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( #CITATION_TAG ) We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.", "We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish. Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( #CITATION_TAG )", "Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En\u2192Bg) and from English to Spanish (En\u2192Es). results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \u00e2\\x86\\x92 Es results are based on a corpus of parliamentary proceedings ( #CITATION_TAG ) We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM."], "CC659": ["This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.", "We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.", "This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.", "In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.", "We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.", "This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.", "Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance. In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.", "In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.", "We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.", "This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time. Applying the symmetrization to the model with symmetry constraints does not affect performance.", "Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance. In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.", "In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.", "We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time. Applying the symmetrization to the model with symmetry constraints does not affect performance.", "Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance. In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.", "In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #CITATION_TAG ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time. Applying the symmetrization to the model with symmetry constraints does not affect performance."], "CC661": ["Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair.", "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est all\u00e9), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair.", "Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.", "A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b). There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est all\u00e9), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair.", "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est all\u00e9), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.", "Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns. Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.", "A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b). There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est all\u00e9), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.", "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est all\u00e9), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns. Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.", "Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns. Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link. Sure links are represented as squares with borders, and possible links", "A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b). There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est all\u00e9), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns. Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.", "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est all\u00e9), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #CITATION_TAG ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns. Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link. Sure links are represented as squares with borders, and possible links"], "CC662": ["For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG )", "Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG )", "For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG ) This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.", "As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair. Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG )", "Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG ) This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.", "For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG ) This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.", "As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair. Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG ) This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.", "Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG ) This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.", "For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG ) This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The alignment produced has high recall relative to the intersection and only slightly lower recall than the union. In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.", "As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair. Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG ) This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.", "Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #CITATION_TAG ) This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The alignment produced has high recall relative to the intersection and only slightly lower recall than the union. In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages."], "CC663": ["PR is closely related to the work of #CITATION_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning", "PR is closely related to the work of #CITATION_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning They call their method generalized expectation (GE) constraints or alternatively expectation regularization.", "PR is closely related to the work of #CITATION_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning They call their method generalized expectation (GE) constraints or alternatively expectation regularization. In the original GE framework, the posteriors of the model on unlabeled data are regularized directly."], "CC664": ["But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #CITATION_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .", "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #CITATION_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .", "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #CITATION_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) ."], "CC665": ["Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #CITATION_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).", "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #CITATION_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).", "Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #CITATION_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).", "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1\ufffd5) for statistical machine translation and the concept of \ufffdword-by- word\ufffd alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #CITATION_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).", "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #CITATION_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008)."], "CC666": ["For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing", "This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing", "For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.", "Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing", "This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.", "For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training. These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.", "The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing", "Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.", "This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training. These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.", "For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training. These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take. However, the approaches differ substantially from PR.", "The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.", "Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training. These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.", "This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training. These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take. However, the approaches differ substantially from PR.", "The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training. These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.", "Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant \u03b7 > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing , #CITATION_TAG add a constraint of the form `` the average length of dependencies should be X \"\" to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing They modify the model\"s distribution over trees p \u03b8 (y) by a penalty term as: p \u03b8 (y) \u221d p \u03b8 (y)e (\u03b4 e\u2208y length(e)) , where length(e) is the surface length of edge e. The factor \u03b4 changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training. These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take. However, the approaches differ substantially from PR."], "CC667": ["We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not.", "We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns. We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not.", "We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which meet the agreement criteria.", "Grammaticality of parse trees. We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns. We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not.", "We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns. We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which meet the agreement criteria.", "We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which meet the agreement criteria. Note that we use the syntax given by the tree, not the gold syntax.", "Grammaticality of parse trees. We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns. We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which meet the agreement criteria.", "We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns. We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which meet the agreement criteria. Note that we use the syntax given by the tree, not the gold syntax.", "We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which meet the agreement criteria. Note that we use the syntax given by the tree, not the gold syntax. For all three trees, however, we used gold morphological features for this evaluation even when those features were not used in the parsing task.", "Grammaticality of parse trees. We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns. We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which meet the agreement criteria. Note that we use the syntax given by the tree, not the gold syntax.", "We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns. We use the agreement checker code developed by #CITATION_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference . The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which meet the agreement criteria. Note that we use the syntax given by the tree, not the gold syntax. For all three trees, however, we used gold morphological features for this evaluation even when those features were not used in the parsing task."], "CC668": ["In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.", "Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.", "In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension. 8 Cross-linguistically, a core set containing around 12 tags is often assumed as a \ufffduniversal tag set\ufffd (Rambow et al. 2006; Petrov, Das, and McDonald 2012).", "Linguistically, words have associated POS tags, e.g., \ufffdverb\ufffd or \ufffdnoun,\ufffd which further abstract over morphologically and syntactically similar lexemes. Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.", "Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension. 8 Cross-linguistically, a core set containing around 12 tags is often assumed as a \ufffduniversal tag set\ufffd (Rambow et al. 2006; Petrov, Das, and McDonald 2012).", "In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension. 8 Cross-linguistically, a core set containing around 12 tags is often assumed as a \ufffduniversal tag set\ufffd (Rambow et al. 2006; Petrov, Das, and McDonald 2012). We have adapted the list from Rambow et al. (2006) for Arabic, and call it here CORE12.", "Linguistically, words have associated POS tags, e.g., \ufffdverb\ufffd or \ufffdnoun,\ufffd which further abstract over morphologically and syntactically similar lexemes. Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension. 8 Cross-linguistically, a core set containing around 12 tags is often assumed as a \ufffduniversal tag set\ufffd (Rambow et al. 2006; Petrov, Das, and McDonald 2012).", "Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension. 8 Cross-linguistically, a core set containing around 12 tags is often assumed as a \ufffduniversal tag set\ufffd (Rambow et al. 2006; Petrov, Das, and McDonald 2012). We have adapted the list from Rambow et al. (2006) for Arabic, and call it here CORE12.", "In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension. 8 Cross-linguistically, a core set containing around 12 tags is often assumed as a \ufffduniversal tag set\ufffd (Rambow et al. 2006; Petrov, Das, and McDonald 2012). We have adapted the list from Rambow et al. (2006) for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX).", "Linguistically, words have associated POS tags, e.g., \ufffdverb\ufffd or \ufffdnoun,\ufffd which further abstract over morphologically and syntactically similar lexemes. Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension. 8 Cross-linguistically, a core set containing around 12 tags is often assumed as a \ufffduniversal tag set\ufffd (Rambow et al. 2006; Petrov, Das, and McDonald 2012). We have adapted the list from Rambow et al. (2006) for Arabic, and call it here CORE12.", "Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #CITATION_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension. 8 Cross-linguistically, a core set containing around 12 tags is often assumed as a \ufffduniversal tag set\ufffd (Rambow et al. 2006; Petrov, Das, and McDonald 2012). We have adapted the list from Rambow et al. (2006) for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX)."], "CC669": ["For better comparison with work of others , we adopt the suggestion made by #CITATION_TAG to evaluate the parsing quality on sentences up to 70 tokens long", "For better comparison with work of others , we adopt the suggestion made by #CITATION_TAG to evaluate the parsing quality on sentences up to 70 tokens long We report these filtered results in Table 14.", "For better comparison with work of others , we adopt the suggestion made by #CITATION_TAG to evaluate the parsing quality on sentences up to 70 tokens long We report these filtered results in Table 14. Filtered results are consistently higher (as expected)."], "CC671": ["We use the Columbia Arabic Treebank ( CATiB ) ( #CITATION_TAG ) . Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.", "We use the Columbia Arabic Treebank ( CATiB ) ( #CITATION_TAG ) . Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information. CATiB\"s dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations.", "We use the Columbia Arabic Treebank ( CATiB ) ( #CITATION_TAG ) . Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information. CATiB\"s dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations. It has a reduced POS tag set consisting of six tags only (henceforth CATIB6)."], "CC672": ["For example , modeling CASE in Czech improves Czech parsing ( #CITATION_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy", "In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example , modeling CASE in Czech improves Czech parsing ( #CITATION_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy", "For example , modeling CASE in Czech improves Czech parsing ( #CITATION_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;.", "Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example , modeling CASE in Czech improves Czech parsing ( #CITATION_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy", "In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example , modeling CASE in Czech improves Czech parsing ( #CITATION_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;."], "CC673": ["For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #CITATION_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5", "For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #CITATION_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set.", "For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #CITATION_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set."], "CC674": [], "CC675": ["In this article , we use an in-house system which provides functional gender , number , and rationality features ( #CITATION_TAG ) . See Section 5.2 for more details.", "The Elixir-FM analyzer (Smr\u017e 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article , we use an in-house system which provides functional gender , number , and rationality features ( #CITATION_TAG ) . See Section 5.2 for more details.", "6 ost available Arabic NLP tools and resources model morphology using formbased (\"surface\") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;Habash, Rambow, and Roth 2012). The Elixir-FM analyzer (Smr\u017e 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article , we use an in-house system which provides functional gender , number , and rationality features ( #CITATION_TAG ) . See Section 5.2 for more details."], "CC676": ["As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #CITATION_TAG ) . Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #CITATION_TAG ) . Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT."], "CC677": ["Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.", "Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.", "Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.", "Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.", "Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.", "Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs. 9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise \"inverse\" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).", "We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.", "Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.", "Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs. 9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise \"inverse\" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).", "We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.", "Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #CITATION_TAG ) ( Section 6 ) . Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs. 9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise \"inverse\" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too)."], "CC678": ["To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #CITATION_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender )", "The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #CITATION_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender )", "To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #CITATION_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012).", "The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #CITATION_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012).", "To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #CITATION_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012). 19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).", "The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #CITATION_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012). 19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).", "To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #CITATION_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012). 19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts."], "CC681": ["For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; K\u00c3\u00bcbler , McDonald , and #CITATION_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering).", "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; K\u00c3\u00bcbler , McDonald , and #CITATION_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\").", "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; K\u00c3\u00bcbler , McDonald , and #CITATION_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\"). to predict the next state in the parse derivation."], "CC682": ["We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).", "18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).", "We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.", "To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011). 18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).", "18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.", "We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts. The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.", "The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011). 18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).", "To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011). 18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.", "18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts. The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.", "We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts. The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again. The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX\ufffdthe best performing tag set with predicted in- put.", "The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011). 18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.", "To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011). 18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts. The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.", "18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts. The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again. The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX\ufffdthe best performing tag set with predicted in- put.", "The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011). 18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts. The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.", "To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011). 18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #CITATION_TAG ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts. The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again. The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX\ufffdthe best performing tag set with predicted in- put."], "CC683": ["Most available Arabic NLP tools and resources model morphology using form- based (\ufffdsurface\ufffd) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #CITATION_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012).", "Most available Arabic NLP tools and resources model morphology using form- based (\ufffdsurface\ufffd) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #CITATION_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012). The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.", "Most available Arabic NLP tools and resources model morphology using form- based (\ufffdsurface\ufffd) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #CITATION_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012). The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012)."], "CC684": ["Most available Arabic NLP tools and resources model morphology using form- based (\ufffdsurface\ufffd) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #CITATION_TAG ;  Habash, Rambow, and Roth 2012).", "Most available Arabic NLP tools and resources model morphology using form- based (\ufffdsurface\ufffd) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #CITATION_TAG ;  Habash, Rambow, and Roth 2012). The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.", "Most available Arabic NLP tools and resources model morphology using form- based (\ufffdsurface\ufffd) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #CITATION_TAG ;  Habash, Rambow, and Roth 2012). The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012)."], "CC685": [], "CC687": ["As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #CITATION_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) . Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #CITATION_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) . Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT."], "CC688": ["It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) .", "For example, modeling CASE in Czech improves Czech parsing (Collins et al. 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) .", "In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al. 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) ."], "CC690": [], "CC691": ["#CITATION_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable )", "Much work has been done on the use of morphological features for parsing of morphologically rich languages. #CITATION_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable )", "#CITATION_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size \u22483000+).", "Much work has been done on the use of morphological features for parsing of morphologically rich languages. #CITATION_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size \u22483000+).", "#CITATION_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size \u22483000+). They also report that the use of gender, number, and person features did not yield any improvements.", "Much work has been done on the use of morphological features for parsing of morphologically rich languages. #CITATION_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size \u22483000+). They also report that the use of gender, number, and person features did not yield any improvements.", "#CITATION_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size \u22483000+). They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see."], "CC692": [], "CC693": ["The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #CITATION_TAG ) .", "This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations. The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #CITATION_TAG ) .", "In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations. The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #CITATION_TAG ) ."], "CC695": ["For more information on CATiB , see #CITATION_TAG and Habash , Faraj , and Roth ( 2009 ) .", "For the corpus statistics, see Table 1. For more information on CATiB , see #CITATION_TAG and Habash , Faraj , and Roth ( 2009 ) .", "An example CATiB dependency tree is shown in Figure 1. For the corpus statistics, see Table 1. For more information on CATiB , see #CITATION_TAG and Habash , Faraj , and Roth ( 2009 ) ."], "CC696": ["Some researchers , however , including #CITATION_TAG , train on predicted feature values instead", "So far, we have only evaluated models trained on gold POS tag set and morphological feature values. Some researchers , however , including #CITATION_TAG , train on predicted feature values instead", "Some researchers , however , including #CITATION_TAG , train on predicted feature values instead It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.", "So far, we have only evaluated models trained on gold POS tag set and morphological feature values. Some researchers , however , including #CITATION_TAG , train on predicted feature values instead It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.", "Some researchers , however , including #CITATION_TAG , train on predicted feature values instead It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test. But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold).", "So far, we have only evaluated models trained on gold POS tag set and morphological feature values. Some researchers , however , including #CITATION_TAG , train on predicted feature values instead It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test. But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold).", "Some researchers , however , including #CITATION_TAG , train on predicted feature values instead It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test. But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold). 21 To test our hypothesis, we start this section by comparing three variations:"], "CC697": [], "CC698": ["As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #CITATION_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) . Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #CITATION_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) . Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT."], "CC700": ["K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features.", "Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features.", "K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.", "The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features.", "Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.", "K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].", "There are default MaltParser features (in the machine learning sense),12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers. The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features.", "The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.", "Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].", "K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0]. We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).", "There are default MaltParser features (in the machine learning sense),12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers. The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.", "The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].", "Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0]. We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).", "There are default MaltParser features (in the machine learning sense),12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers. The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].", "The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K\ufffdbler, McDonald, and #CITATION_TAG describe a \ufffdtypical\ufffd MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0]. We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument)."], "CC702": ["For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #CITATION_TAG , 2008 ; K\u00c3\u00bcbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering).", "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #CITATION_TAG , 2008 ; K\u00c3\u00bcbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\").", "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #CITATION_TAG , 2008 ; K\u00c3\u00bcbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\"). to predict the next state in the parse derivation."], "CC704": ["Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations", "This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations", "Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.", "The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations", "This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.", "Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic.", "They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations", "The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.", "This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic.", "Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima\ufffdan (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.", "They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.", "The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic.", "This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima\ufffdan (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.", "They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic.", "The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic\ufffd and Vidov\ufffd-Hladk\ufffd 1998) compared with Arabic (\ufffd14.0%, see Table 3). Similarly , #CITATION_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima\ufffdan (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing."], "CC705": [], "CC706": ["In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #CITATION_TAG ) . As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.", "In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #CITATION_TAG ) . As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values. The Easy-First Parser is a shift-reduce parser (as is MaltParser).", "In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #CITATION_TAG ) . As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values. The Easy-First Parser is a shift-reduce parser (as is MaltParser). Unlike MaltParser, however, it does not attempt to attach arcs \"eagerly\" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments)."], "CC707": ["Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG )", "We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG )", "Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.", "Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG )", "We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.", "Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).", "Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.", "We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).", "Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6). Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.", "Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).", "We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #CITATION_TAG ) Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6). Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor."], "CC709": ["This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #CITATION_TAG ) . In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.", "This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #CITATION_TAG ) . In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way. As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents).", "This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #CITATION_TAG ) . In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way. As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents). In the next section, we provide additional information on how we performed the annotation of this corpus."], "CC710": ["A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #CITATION_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.", "A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #CITATION_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction."], "CC711": ["Thus , for example , it can acquire a `` script \"\" such as the one for going to a restaurant as defined in #CITATION_TAG .", "The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples. Thus , for example , it can acquire a `` script \"\" such as the one for going to a restaurant as defined in #CITATION_TAG ."], "CC712": ["There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #CITATION_TAG .", "That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #CITATION_TAG .", "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975). That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #CITATION_TAG ."], "CC713": [], "CC714": ["The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #CITATION_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.", "The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #CITATION_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here.", "The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #CITATION_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis."], "CC715": ["The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #CITATION_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.", "The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #CITATION_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here.", "The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #CITATION_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis."], "CC716": ["An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.", "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.", "An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).", "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.", "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).", "An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.", "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).", "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word. ]", "The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).", "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word. ]", "The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #CITATION_TAG ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word. ]"], "CC717": ["The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #CITATION_TAG", "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984). The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #CITATION_TAG", "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #CITATION_TAG That is, the current system learns procedures rather than data structures.", "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984). The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #CITATION_TAG That is, the current system learns procedures rather than data structures.", "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #CITATION_TAG That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982)."], "CC718": ["Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.", "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.", "Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.", "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.", "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.", "Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.", "Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.", "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.", "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.", "Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.", "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #CITATION_TAG . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable."], "CC721": ["The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #CITATION_TAG , or semantic nets as in Winston ( 1975 )", "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984). The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #CITATION_TAG , or semantic nets as in Winston ( 1975 )", "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #CITATION_TAG , or semantic nets as in Winston ( 1975 ) That is, the current system learns procedures rather than data structures.", "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984). The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #CITATION_TAG , or semantic nets as in Winston ( 1975 ) That is, the current system learns procedures rather than data structures.", "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #CITATION_TAG , or semantic nets as in Winston ( 1975 ) That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982)."], "CC722": ["A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.", "Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.", "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984).", "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.", "Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984).", "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984). However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.", "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.", "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984).", "Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984). However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.", "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984). However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.", "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984).", "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984). However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.", "Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984). However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.", "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984). However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.", "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #CITATION_TAG where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984). However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user."], "CC723": ["The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #CITATION_TAG ) , a deep parse of Si , or some other representation . A user behavior is represented by a network, or directed graph, of such meanings.", "We denote the meaning of each sentence Si with the notation M(Si). The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #CITATION_TAG ) , a deep parse of Si , or some other representation . A user behavior is represented by a network, or directed graph, of such meanings.", "The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #CITATION_TAG ) , a deep parse of Si , or some other representation . A user behavior is represented by a network, or directed graph, of such meanings. At the beginning of a task, the state of the interaction is represented by the start state of the graph."], "CC724": ["A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #CITATION_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.", "A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #CITATION_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction."], "CC725": ["A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #CITATION_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.", "A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #CITATION_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction."], "CC727": ["[ The current system should be distinguished from an earlier voice system ( VNLC , #CITATION_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]", "The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [ The current system should be distinguished from an earlier voice system ( VNLC , #CITATION_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]", "The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [ The current system should be distinguished from an earlier voice system ( VNLC , #CITATION_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]"], "CC728": ["The expectation parser uses an ATN-like representation for its grammar ( #CITATION_TAG ) . Its strategy is top-down.", "The expectation parser uses an ATN-like representation for its grammar ( #CITATION_TAG ) . Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979).", "The expectation parser uses an ATN-like representation for its grammar ( #CITATION_TAG ) . Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979). An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses."], "CC729": ["The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #CITATION_TAG ) . [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #CITATION_TAG ) . [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #CITATION_TAG ) . [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word. ]", "An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980). The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #CITATION_TAG ) . [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #CITATION_TAG ) . [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word. ]", "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980). The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #CITATION_TAG ) . [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980). The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #CITATION_TAG ) . [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word. ]"], "CC730": ["A number of speech understanding systems have been developed during the past fifteen years ( #CITATION_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.", "A number of speech understanding systems have been developed during the past fifteen years ( #CITATION_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction."], "CC732": [], "CC733": ["The problem of handling ill-formed input has been studied by #CITATION_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.", "The problem of handling ill-formed input has been studied by #CITATION_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here.", "The problem of handling ill-formed input has been studied by #CITATION_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis."], "CC734": ["The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #CITATION_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 )", "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984). The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #CITATION_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 )", "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #CITATION_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) That is, the current system learns procedures rather than data structures.", "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984). The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #CITATION_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) That is, the current system learns procedures rather than data structures.", "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #CITATION_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982)."], "CC735": ["An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.", "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.", "An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).", "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.", "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).", "An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.", "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).", "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word. ]", "The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).", "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word. ]", "The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.", "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #CITATION_TAG , Biermann and Ballard 1980 ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word. ]"], "CC736": ["A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #CITATION_TAG , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.", "A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #CITATION_TAG , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction."], "CC737": ["The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #CITATION_TAG ) . An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.", "Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #CITATION_TAG ) . An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.", "The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #CITATION_TAG ) . An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses. Sentences have the same \"meaning\" if they \"result in identical tasks being performed.", "The expectation parser uses an ATN-like representation for its grammar (Woods 1970). Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #CITATION_TAG ) . An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.", "Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #CITATION_TAG ) . An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses. Sentences have the same \"meaning\" if they \"result in identical tasks being performed.", "The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #CITATION_TAG ) . An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses. Sentences have the same \"meaning\" if they \"result in identical tasks being performed. The various sentence structures that", "The expectation parser uses an ATN-like representation for its grammar (Woods 1970). Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #CITATION_TAG ) . An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses. Sentences have the same \"meaning\" if they \"result in identical tasks being performed.", "Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #CITATION_TAG ) . An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses. Sentences have the same \"meaning\" if they \"result in identical tasks being performed. The various sentence structures that"], "CC738": ["How it is done is beyond the scope of this paper but is explained in detail in #CITATION_TAG .", "The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second. How it is done is beyond the scope of this paper but is explained in detail in #CITATION_TAG .", "Thus, both options are imperfect in terms of the error correction capabilities that they can provide. The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second. How it is done is beyond the scope of this paper but is explained in detail in #CITATION_TAG ."], "CC739": ["There is some literature on procedure acquisition such as the LISP synthesis work described in #CITATION_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .", "That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #CITATION_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .", "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975). That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #CITATION_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) ."], "CC740": ["A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #CITATION_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.", "A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #CITATION_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction."], "CC741": [") In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #CITATION_TAG describe further research in Cambridge utilising different types of information available in LDOCE .", "(Michiels (1982) contains further description and discussion of LDOCE. ) In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #CITATION_TAG describe further research in Cambridge utilising different types of information available in LDOCE .", "Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled \"core\" vocabulary in defining the words throughout the dictionary. (Michiels (1982) contains further description and discussion of LDOCE. ) In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #CITATION_TAG describe further research in Cambridge utilising different types of information available in LDOCE ."], "CC742": ["#CITATION_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description . Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.", "On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. #CITATION_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description . Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated."], "CC743": ["The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #CITATION_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.", "Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #CITATION_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.", "These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #CITATION_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser."], "CC744": ["However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #CITATION_TAG for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.", "The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth. However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #CITATION_TAG for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.", "However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #CITATION_TAG for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information. For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.", "The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth. However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #CITATION_TAG for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information. For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.", "However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #CITATION_TAG for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information. For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used. These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.", "The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth. However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #CITATION_TAG for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information. For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used. These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.", "However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #CITATION_TAG for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information. For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used. These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character. In Figure 1 above the definition of rivet as verb includes the noun definition of \"RIVET 1\\\"\\ as signalled by the font change and the numerical superscript which indicates that it is the first (i.e. noun entry) homograph; additional notation exists for word senses within homographs."], "CC745": ["#CITATION_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .", "Michiels (1982) and Akkerman et al. (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. #CITATION_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .", "On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. Michiels (1982) and Akkerman et al. (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. #CITATION_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated ."], "CC746": ["#CITATION_TAG:472 ), but these are the only ones which are explicit in the LDOCE coding system."], "CC747": ["Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #CITATION_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective."], "CC748": ["No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #CITATION_TAG ) .", "However, the LDOCE coding of verbs is more comprehensive than elsewhere, so verbs are the obvious place to start in an evaluation of the usefulness of the coding system. No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #CITATION_TAG ) .", "Extending the system to handle nouns, adjectives and adverbs would present no problems of principle. However, the LDOCE coding of verbs is more comprehensive than elsewhere, so verbs are the obvious place to start in an evaluation of the usefulness of the coding system. No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #CITATION_TAG ) ."], "CC749": [") In this paper we focus on the exploitation of the LDOCE grammar coding system ; #CITATION_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .", "(Michiels (1982) contains further description and discussion of LDOCE. ) In this paper we focus on the exploitation of the LDOCE grammar coding system ; #CITATION_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .", "Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled \"core\" vocabulary in defining the words throughout the dictionary. (Michiels (1982) contains further description and discussion of LDOCE. ) In this paper we focus on the exploitation of the LDOCE grammar coding system ; #CITATION_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE ."], "CC750": ["One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #CITATION_TAG ) . In this project, a new lexicon is being manually derived from LDOCE.", "This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment). One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #CITATION_TAG ) . In this project, a new lexicon is being manually derived from LDOCE.", "One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #CITATION_TAG ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.", "This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment). One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #CITATION_TAG ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.", "One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #CITATION_TAG ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.", "This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment). One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #CITATION_TAG ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.", "One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #CITATION_TAG ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis. In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing."], "CC751": ["To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #CITATION_TAG and references therein ) . PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.", "The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms. To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #CITATION_TAG and references therein ) . PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.", "To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #CITATION_TAG and references therein ) . PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG. We The latter employs a grammatical formalism based on GPSG; the comparatively theory neutral lexical entries that we construct from LDOCE should translate straightforwardly into this framework as well."], "CC752": ["One approach to this problem is that taken by the ASCOT project ( #CITATION_TAG ; Akkerman , 1986 ) . In this project, a new lexicon is being manually derived from LDOCE.", "This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment). One approach to this problem is that taken by the ASCOT project ( #CITATION_TAG ; Akkerman , 1986 ) . In this project, a new lexicon is being manually derived from LDOCE.", "One approach to this problem is that taken by the ASCOT project ( #CITATION_TAG ; Akkerman , 1986 ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.", "This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment). One approach to this problem is that taken by the ASCOT project ( #CITATION_TAG ; Akkerman , 1986 ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.", "One approach to this problem is that taken by the ASCOT project ( #CITATION_TAG ; Akkerman , 1986 ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.", "This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment). One approach to this problem is that taken by the ASCOT project ( #CITATION_TAG ; Akkerman , 1986 ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.", "One approach to this problem is that taken by the ASCOT project ( #CITATION_TAG ; Akkerman , 1986 ) . In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis. In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing."], "CC753": ["Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #CITATION_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective."], "CC754": [", #CITATION_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .", "Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #CITATION_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .", "In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage. Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #CITATION_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape ."], "CC755": ["In addition , #CITATION_TAG note that our Object Raising rule would assign mean to this category incorrectly . Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e. intend\"), however, when it is used in this sense it must be treated as an Object Equi verb.", "None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule. In addition , #CITATION_TAG note that our Object Raising rule would assign mean to this category incorrectly . Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e. intend\"), however, when it is used in this sense it must be treated as an Object Equi verb."], "CC756": ["In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #CITATION_TAG for details ) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #CITATION_TAG for details ) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #CITATION_TAG for details ) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #CITATION_TAG for details ) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #CITATION_TAG for details ) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #CITATION_TAG for details ) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #CITATION_TAG for details ) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective."], "CC757": ["Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #CITATION_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language . These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.", "Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #CITATION_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language . These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.", "Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #CITATION_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language . These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects (Boguraev, 1987;Russell et al., 1986;Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English."], "CC758": ["This deficiency is rectified in the verb classification system employed by #CITATION_TAG in the Brandeis verb catalogue .", "The T5 code is marked as \\\"rare\\ and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal:  This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as \"would\". This deficiency is rectified in the verb classification system employed by #CITATION_TAG in the Brandeis verb catalogue ."], "CC759": ["The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #CITATION_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.", "Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #CITATION_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.", "These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #CITATION_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser."], "CC760": [], "CC761": ["#CITATION_TAG ) . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.", "In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #CITATION_TAG ) . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.", "This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #CITATION_TAG ) . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly."], "CC762": ["#CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.", "#CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "#CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.", "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "#CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).", "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.", "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #CITATION_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective."], "CC763": ["Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #CITATION_TAG , for further discussion ) .", "For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement. Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #CITATION_TAG , for further discussion ) .", "These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement. Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #CITATION_TAG , for further discussion ) ."], "CC764": ["( #CITATION_TAG contains further description and discussion of LDOCE . ) In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al. (1985) and Alshawi (1987) describe further research in Cambridge utilising different types of information available in LDOCE.", "Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled \"core\" vocabulary in defining the words throughout the dictionary. ( #CITATION_TAG contains further description and discussion of LDOCE . ) In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al. (1985) and Alshawi (1987) describe further research in Cambridge utilising different types of information available in LDOCE."], "CC765": ["Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.", "In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.", "Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.", "In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence. In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.", "In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.", "Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement. Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see Williams (1980), for further discussion).", "The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense. In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence. In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.", "In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence. In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.", "In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement. Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see Williams (1980), for further discussion).", "The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense. In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence. In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.", "In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence. In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #CITATION_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement. Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see Williams (1980), for further discussion)."], "CC766": ["In addition to headwords , dictionary search through the pronunciation field is available ; #CITATION_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).", "From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords , dictionary search through the pronunciation field is available ; #CITATION_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).", "In addition to headwords , dictionary search through the pronunciation field is available ; #CITATION_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample."], "CC769": ["Michiels ( 1982 ) and #CITATION_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description . Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.", "On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. Michiels ( 1982 ) and #CITATION_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description . Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated."], "CC770": ["Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #CITATION_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language . These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.", "Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #CITATION_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language . These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.", "Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #CITATION_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language . These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects (Boguraev, 1987;Russell et al., 1986;Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English."], "CC771": ["In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #CITATION_TAG ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).", "From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #CITATION_TAG ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).", "In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #CITATION_TAG ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample."], "CC772": ["The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word.", "Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary. The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word.", "The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word. Patterns are descriptive, and are used to convey a range of information: eg.", "Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring. Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary. The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word.", "Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary. The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word. Patterns are descriptive, and are used to convey a range of information: eg.", "The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word. Patterns are descriptive, and are used to convey a range of information: eg. distinctions between count and mass nouns (dog vs. desire), predicative, postpositive and attributive adjectives (asleep vs. elect vs. jokular), noun complementation (fondness, fact) and, most importantly, verb complementation and valency.", "In what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an LDOCE entry and therefore, a small proportion of the overall restructuring task. Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring. Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary. The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word.", "Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring. Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary. The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word. Patterns are descriptive, and are used to convey a range of information: eg.", "Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary. The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word. Patterns are descriptive, and are used to convey a range of information: eg. distinctions between count and mass nouns (dog vs. desire), predicative, postpositive and attributive adjectives (asleep vs. elect vs. jokular), noun complementation (fondness, fact) and, most importantly, verb complementation and valency.", "In what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an LDOCE entry and therefore, a small proportion of the overall restructuring task. Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring. Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary. The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word. Patterns are descriptive, and are used to convey a range of information: eg.", "Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring. Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary. The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #CITATION_TAG ) . A grammar code describes a particular pattern of behaviour of a word. Patterns are descriptive, and are used to convey a range of information: eg. distinctions between count and mass nouns (dog vs. desire), predicative, postpositive and attributive adjectives (asleep vs. elect vs. jokular), noun complementation (fondness, fact) and, most importantly, verb complementation and valency."], "CC773": ["Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #CITATION_TAG:460 ff . ) However, only two of these criteria are explicit in the coding system.", "Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system. Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #CITATION_TAG:460 ff . ) However, only two of these criteria are explicit in the coding system.", "This is the primary source of error in the case of the Object Raising rule. Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system. Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #CITATION_TAG:460 ff . ) However, only two of these criteria are explicit in the coding system."], "CC774": ["In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #CITATION_TAG ) . Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.", "In addition to headwords, dictionary search through the pronunciation field is available; Carter (1987) has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure (Huttenlocher and Zue, 1983). In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #CITATION_TAG ) . Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample."], "CC775": [], "CC776": ["As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.", "A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.", "As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.", "They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.", "A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.", "As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.", "A series of systems in Cambridge are implemented in Lisp running under UnixTM. They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.", "They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.", "A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.", "As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system. To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.", "A series of systems in Cambridge are implemented in Lisp running under UnixTM. They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.", "They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.", "A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system. To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.", "A series of systems in Cambridge are implemented in Lisp running under UnixTM. They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.", "They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #CITATION_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system. To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls."], "CC777": ["The research described below is taking place in the context of three collaborative projects ( #CITATION_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.", "Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects ( #CITATION_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.", "These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects ( #CITATION_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English . One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser."], "CC778": ["Many investigators (e.g. Many investigators ( e.g. #CITATION_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.", "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #CITATION_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.", "Many investigators (e.g. Many investigators ( e.g. #CITATION_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.", "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #CITATION_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.", "Many investigators (e.g. Many investigators ( e.g. #CITATION_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses.", "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #CITATION_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses.", "Many investigators (e.g. Many investigators ( e.g. #CITATION_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."], "CC779": ["In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #CITATION_TAG ) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence.", "In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #CITATION_TAG ) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence. While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.", "In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #CITATION_TAG ) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence. While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system. Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing."], "CC780": ["Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #CITATION_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.", "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #CITATION_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.", "Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #CITATION_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.", "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #CITATION_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.", "Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #CITATION_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses.", "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #CITATION_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses.", "Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #CITATION_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."], "CC781": ["We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #CITATION_TAG ) . Two concerns motivated our implementation.", "We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #CITATION_TAG ) . Two concerns motivated our implementation. First, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences."], "CC782": ["#CITATION_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing", "To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse. #CITATION_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing", "#CITATION_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.", "). To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse. #CITATION_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing", "To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse. #CITATION_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.", "3. ). To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse. #CITATION_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing", "). To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse. #CITATION_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing."], "CC783": ["Sentences like 12 , from #CITATION_TAG , are frequently cited . (Square brackets mark off the NP constituents that contain embed- ded sentences.", "For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12 , from #CITATION_TAG , are frequently cited . (Square brackets mark off the NP constituents that contain embed- ded sentences.", "Sentences like 12 , from #CITATION_TAG , are frequently cited . (Square brackets mark off the NP constituents that contain embed- ded sentences. )", "However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12 , from #CITATION_TAG , are frequently cited . (Square brackets mark off the NP constituents that contain embed- ded sentences.", "For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12 , from #CITATION_TAG , are frequently cited . (Square brackets mark off the NP constituents that contain embed- ded sentences. )", "This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e. However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12 , from #CITATION_TAG , are frequently cited . (Square brackets mark off the NP constituents that contain embed- ded sentences.", "However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12 , from #CITATION_TAG , are frequently cited . (Square brackets mark off the NP constituents that contain embed- ded sentences. )"], "CC784": ["In previous work ( #CITATION_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence.", "In previous work ( #CITATION_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence. While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.", "In previous work ( #CITATION_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence. While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system. Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing."], "CC785": ["#CITATION_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model", "Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper. #CITATION_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model", "#CITATION_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.", "372). Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper. #CITATION_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model", "Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper. #CITATION_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.", "a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p. 372). Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper. #CITATION_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model", "372). Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper. #CITATION_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology."], "CC787": ["3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #CITATION_TAG , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p.", "Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase. 3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #CITATION_TAG , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p.", "3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #CITATION_TAG , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251)", "Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase. 3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #CITATION_TAG , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251)", "3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #CITATION_TAG , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251) This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing."], "CC788": ["In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"...", "The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"...", "In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p.", "Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited. The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"...", "The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p.", "In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p. 372).", "For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited. The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"...", "Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited. The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p.", "The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p. 372).", "In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p. 372). Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.", "For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited. The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p.", "Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited. The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p. 372).", "The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p. 372). Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.", "For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited. The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p. 372).", "Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited. The result is a flattened structure that more accurately reflects the prosodic phrasing. In #CITATION_TAG , this flattening process is not part of the grammar . Rather, it is viewed as \"... a performance factor, related to the difficulty of producing right branching structures such as [ 12]\" (p. 372). Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper."], "CC789": ["An alternative representation based on #CITATION_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree . Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing."], "CC790": ["#CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.", "Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.", "#CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.", "The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations. Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.", "Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.", "#CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure. The problem here is that the phrasing in observed data often ignores the argument status of constituents.", "The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations. Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.", "Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure. The problem here is that the phrasing in observed data often ignores the argument status of constituents.", "#CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure. The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.", "The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations. Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure. The problem here is that the phrasing in observed data often ignores the argument status of constituents.", "Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #CITATION_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure. The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts."], "CC791": ["The psycholinguistic studies of Martin ( 1970 ) , #CITATION_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.", "The psycholinguistic studies of Martin ( 1970 ) , #CITATION_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.", "The psycholinguistic studies of Martin ( 1970 ) , #CITATION_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly."], "CC792": ["Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #CITATION_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.", "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #CITATION_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.", "Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #CITATION_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.", "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #CITATION_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.", "Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #CITATION_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses.", "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #CITATION_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses.", "Many investigators (e.g. Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #CITATION_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O\"Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."], "CC793": ["Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #CITATION_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .", "An alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #CITATION_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing ."], "CC794": ["Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents.", "Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents.", "Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.", "Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents.", "Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.", "Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.", "Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents.", "Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.", "Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.", "Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized.", "Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.", "Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.", "Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized.", "Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.", "Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #CITATION_TAG also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized."], "CC795": ["This observation has led some researchers , e.g. , #CITATION_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase . However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing.", "When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes. This observation has led some researchers , e.g. , #CITATION_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase . However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing.", "This observation has led some researchers , e.g. , #CITATION_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase . However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.", "When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes. This observation has led some researchers , e.g. , #CITATION_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase . However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.", "This observation has led some researchers , e.g. , #CITATION_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase . However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.", "When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes. This observation has led some researchers , e.g. , #CITATION_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase . However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.", "This observation has led some researchers , e.g. , #CITATION_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase . However, this claim is controversial because of the misa\"dgnments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12, from Chomsky (1965) To account for such mismatches, \"readjustment rules\" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited. The result is a flattened structure that more accurately reflects the prosodic phrasing."], "CC796": ["Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #CITATION_TAG ( henceforth G&G ) .", "Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly. Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #CITATION_TAG ( henceforth G&G ) .", "Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly. Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #CITATION_TAG ( henceforth G&G ) ."], "CC797": [") The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.", "(The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.", ") The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.", "All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.", "(The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.", ") The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1. 3.", "In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.", "All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.", "(The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1. 3.", ") The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1. 3. ).", "In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.", "All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1. 3.", "(The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1. 3. ).", "In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1. 3.", "All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized. ) The relation between discourse and prosodic phrasing has been examined in some detail by #CITATION_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1. 3. )."], "CC798": ["Our rules for phonological word formation are adopted , for the most part , from G & G , #CITATION_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) . Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.", "Our rules for phonological word formation are adopted , for the most part , from G & G , #CITATION_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) . Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree. If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.", "Our rules for phonological word formation are adopted , for the most part , from G & G , #CITATION_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) . Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree. If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own. Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries."], "CC799": ["Secondly , the cooperative principle of #CITATION_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader . Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.", "First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly , the cooperative principle of #CITATION_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader . Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.", "Secondly , the cooperative principle of #CITATION_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader . Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.", "However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly , the cooperative principle of #CITATION_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader . Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.", "First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly , the cooperative principle of #CITATION_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader . Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."], "CC800": ["We are going to make such a comparison with the theories proposed by J. #CITATION_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence . The quoted works seem to be good representatives for each of the directions; they also point to related literature."], "CC801": ["), \"domain circumscription\" (cf. #CITATION_TAG), and their kin.", "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987), \"domain closure as- sumption\" (ibid. ), \"domain circumscription\" (cf. #CITATION_TAG), and their kin.", "), \"domain circumscription\" (cf. #CITATION_TAG), and their kin. Similarly, the notion of R+ M-abduction is spiritually related to the \"abduc- tive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of Berwick (1986).", "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987), \"domain closure as- sumption\" (ibid. ), \"domain circumscription\" (cf. #CITATION_TAG), and their kin. Similarly, the notion of R+ M-abduction is spiritually related to the \"abduc- tive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of Berwick (1986).", "), \"domain circumscription\" (cf. #CITATION_TAG), and their kin. Similarly, the notion of R+ M-abduction is spiritually related to the \"abduc- tive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987), \"domain closure as- sumption\" (ibid. ), \"domain circumscription\" (cf. #CITATION_TAG), and their kin. Similarly, the notion of R+ M-abduction is spiritually related to the \"abduc- tive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "), \"domain circumscription\" (cf. #CITATION_TAG), and their kin. Similarly, the notion of R+ M-abduction is spiritually related to the \"abduc- tive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of Berwick (1986). But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming)."], "CC803": ["also #CITATION_TAG , p. 672 ) .", "However, \"but\" does not behave quite like the other two--semantically, \"but\" signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition (called \"adversative\" or \"contrary-to-expectation\" by Halliday and Hasan 1976;cf. also #CITATION_TAG , p. 672 ) .", "And,\" \"or,\" and \"but\" are the three main coordinating connectives in English. However, \"but\" does not behave quite like the other two--semantically, \"but\" signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition (called \"adversative\" or \"contrary-to-expectation\" by Halliday and Hasan 1976;cf. also #CITATION_TAG , p. 672 ) ."], "CC804": ["Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #CITATION_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them."], "CC805": ["\"\" ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #CITATION_TAG , p. 546 ) gives 10 definitions of a sentence .", "This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... \"\" ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #CITATION_TAG , p. 546 ) gives 10 definitions of a sentence ."], "CC806": ["This Principle of Finitism is also assumed by #CITATION_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981).", "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #CITATION_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981).", "Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #CITATION_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981)."], "CC807": ["But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.", "So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.", "But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar.", "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.", "So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar.", "But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.", "We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.", "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar.", "So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.", "But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.", "We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar.", "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.", "So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.", "We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.", "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS \"s ) of #CITATION_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics."], "CC808": ["Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #CITATION_TAG ) must play a role , too . Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.", "They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #CITATION_TAG ) must play a role , too . Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself."], "CC809": ["According to #CITATION_TAG , p. 67 ) , these two sentences are incoherent . However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country...", "According to #CITATION_TAG , p. 67 ) , these two sentences are incoherent . However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.", "According to #CITATION_TAG , p. 67 ) , these two sentences are incoherent . However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid. ) suddenly (for Hobbs) becomes coherent."], "CC810": ["For instance , relating \"they\" to \"apples\" in the sentence ( cfXXX #CITATION_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap", "It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance , relating \"they\" to \"apples\" in the sentence ( cfXXX #CITATION_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap", "This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance , relating \"they\" to \"apples\" in the sentence ( cfXXX #CITATION_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap"], "CC811": ["Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #CITATION_TAG .", "Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #CITATION_TAG .", "These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #CITATION_TAG ."], "CC812": ["Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #CITATION_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph . Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.", "Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #CITATION_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph . Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.", "First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #CITATION_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph . Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."], "CC814": ["Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of #CITATION_TAG , the `` diagnosis from first principles \"\" of Reiter ( 1987 ) , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "), domain circumscription\" (cf. Etherington and Mercer 1987), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of #CITATION_TAG , the `` diagnosis from first principles \"\" of Reiter ( 1987 ) , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of #CITATION_TAG , the `` diagnosis from first principles \"\" of Reiter ( 1987 ) , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).", "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987), \"domain closure assumption\" (ibid. ), domain circumscription\" (cf. Etherington and Mercer 1987), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of #CITATION_TAG , the `` diagnosis from first principles \"\" of Reiter ( 1987 ) , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "), domain circumscription\" (cf. Etherington and Mercer 1987), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of #CITATION_TAG , the `` diagnosis from first principles \"\" of Reiter ( 1987 ) , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming)."], "CC815": ["The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.", "The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.", "We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.", "The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid. ).", "Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents. We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.", "We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid. ).", "Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents. We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.", "We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #CITATION_TAG ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid. )."], "CC816": ["According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences.", "These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences.", "According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.", "He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences.", "These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.", "According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).", "His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences.", "He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.", "These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).", "According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).", "His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.", "He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).", "These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).", "His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).", "He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to #CITATION_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined)."], "CC817": ["Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of Reggia ( 1985 ) , the `` diagnosis from first principles \"\" of #CITATION_TAG , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "), domain circumscription\" (cf. Etherington and Mercer 1987), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of Reggia ( 1985 ) , the `` diagnosis from first principles \"\" of #CITATION_TAG , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of Reggia ( 1985 ) , the `` diagnosis from first principles \"\" of #CITATION_TAG , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).", "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987), \"domain closure assumption\" (ibid. ), domain circumscription\" (cf. Etherington and Mercer 1987), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of Reggia ( 1985 ) , the `` diagnosis from first principles \"\" of #CITATION_TAG , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "), domain circumscription\" (cf. Etherington and Mercer 1987), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference \"\" of Reggia ( 1985 ) , the `` diagnosis from first principles \"\" of #CITATION_TAG , `` explainability \"\" of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming)."], "CC818": ["Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #CITATION_TAG ; Patel-Schneider 1985 ) . Levesque 1984;Frisch 1987;Patel-Schneider 1985)."], "CC819": ["This means that natural language expressions such as `` A is B , \"\" `` A is the same as B , \"\" etc. are not directly represented by logical equality ; similarly , `` not \"\" is often not treated as logical negation ; cfXXX #CITATION_TAG . Hintikka (1985)."], "CC820": ["Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing \"there exists\" by \"all\" in the above definitions (cf. #CITATION_TAGb ).", "Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing \"there exists\" by \"all\" in the above definitions (cf. #CITATION_TAGb ). We will have, however, no need for \"strong\" notions in this paper."], "CC821": [], "CC822": ["The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.", "The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.", "We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.", "The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid. ).", "Naturally, we will use a logical notation in which formulas may have temporal and event components. We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.", "We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid. ).", "Naturally, we will use a logical notation in which formulas may have temporal and event components. We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.", "We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject ( e.g. #CITATION_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like . Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid. )."], "CC823": ["Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #CITATION_TAGa , 1987b ) . Zadrozny 1987aZadrozny , 1987b.", "Since it is the \"highest\" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence. Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #CITATION_TAGa , 1987b ) . Zadrozny 1987aZadrozny , 1987b.", "Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #CITATION_TAGa , 1987b ) . Zadrozny 1987aZadrozny , 1987b. Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering.", "Since it is the \"highest\" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence. Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #CITATION_TAGa , 1987b ) . Zadrozny 1987aZadrozny , 1987b. Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering.", "Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #CITATION_TAGa , 1987b ) . Zadrozny 1987aZadrozny , 1987b. Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering. As it turns out, f~ is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story."], "CC824": ["Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #CITATION_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them."], "CC825": ["As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #CITATION_TAG ) . Mycielski 1981).", "This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983, Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #CITATION_TAG ) . Mycielski 1981).", "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983, Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #CITATION_TAG ) . Mycielski 1981)."], "CC826": ["We have shown elsewhere ( #CITATION_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment"], "CC827": [], "CC828": ["The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar.", "But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar.", "The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.", "So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar.", "But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.", "The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.", "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar.", "So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.", "But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.", "The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5).", "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.", "So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.", "But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5).", "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.", "So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS\"s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #CITATION_TAG is more sophisticated , and may be considered another possibility . Jackendoff\"s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys. Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5)."], "CC829": ["This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #CITATION_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981).", "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #CITATION_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981).", "Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #CITATION_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981)."], "CC830": ["He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs.", "His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs.", "He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.", "The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs.", "His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.", "He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences.", "The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.", "His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences.", "He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.", "The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences.", "His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #CITATION_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) . These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs. According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types."], "CC831": ["However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #CITATION_TAG ) . Woods 1987).", "We investigate here only the \"grounding\" in logical theories. However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #CITATION_TAG ) . Woods 1987).", "It is precisely this \"grounding\" of logical predicates in other conceptual structures that we would like to capture. We investigate here only the \"grounding\" in logical theories. However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #CITATION_TAG ) . Woods 1987)."], "CC833": ["An example of psycholinguistically oriented research work can be found in #CITATION_TAG . These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.", "An example of psycholinguistically oriented research work can be found in #CITATION_TAG . These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information."], "CC834": ["#CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ...", "It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ...", "#CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two.", ") This type of consultation uses existing natural language texts as a referential level for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ...", "It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two.", "#CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation.", "(Other reference works could be treated as additional sources of world knowledge. ) This type of consultation uses existing natural language texts as a referential level for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ...", ") This type of consultation uses existing natural language texts as a referential level for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two.", "It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation.", "#CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation. In other words, we recognize it as a separate logical level--the referential level.", "(Other reference works could be treated as additional sources of world knowledge. ) This type of consultation uses existing natural language texts as a referential level for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two.", ") This type of consultation uses existing natural language texts as a referential level for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation.", "It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation. In other words, we recognize it as a separate logical level--the referential level.", "(Other reference works could be treated as additional sources of world knowledge. ) This type of consultation uses existing natural language texts as a referential level for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation.", ") This type of consultation uses existing natural language texts as a referential level for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. #CITATION_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , \" which could be invoked to generate inferences ... \"\" . With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation. In other words, we recognize it as a separate logical level--the referential level."], "CC835": ["Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #CITATION_TAG ) , or the metarules of Section 5.2?", "1 Was the Use of a Gricean Maxim Necessary? Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #CITATION_TAG ) , or the metarules of Section 5.2?", "Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #CITATION_TAG ) , or the metarules of Section 5.2? It seems to us that the answer is no.", "6.1. 1 Was the Use of a Gricean Maxim Necessary? Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #CITATION_TAG ) , or the metarules of Section 5.2?", "1 Was the Use of a Gricean Maxim Necessary? Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #CITATION_TAG ) , or the metarules of Section 5.2? It seems to us that the answer is no."], "CC836": ["This semantics was constructed ( #CITATION_TAGa , 1987b ) as a formal framework for default and commonsense reasoning", "We adopt the three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed ( #CITATION_TAGa , 1987b ) as a formal framework for default and commonsense reasoning", "This semantics was constructed ( #CITATION_TAGa , 1987b ) as a formal framework for default and commonsense reasoning It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.", "We adopt the three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed ( #CITATION_TAGa , 1987b ) as a formal framework for default and commonsense reasoning It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.", "This semantics was constructed ( #CITATION_TAGa , 1987b ) as a formal framework for default and commonsense reasoning It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance, relating \"they\" to \"apples\" in the sentence (cf. Haugeland 1985 p. 195;Zadrozny 1987a):"], "CC837": ["Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of #CITATION_TAG . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "), domain circumscription\" (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of #CITATION_TAG . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of #CITATION_TAG . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).", "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987), \"domain closure assumption\" (ibid. ), domain circumscription\" (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of #CITATION_TAG . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.", "), domain circumscription\" (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985), the \"diagnosis from first principles\" of Reiter (1987), \"explainability\" of Poole (1988), and the subset principle of #CITATION_TAG . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming)."], "CC838": ["Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #CITATION_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them."], "CC839": ["Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #CITATION_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too . Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.", "They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #CITATION_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too . Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself."], "CC841": ["`` Coherence , \"\" as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #CITATION_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text ."], "CC842": ["Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #CITATION_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) . Levesque 1984;Frisch 1987;Patel-Schneider 1985)."], "CC844": ["This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #CITATION_TAG , p. 329 ) .", "This means that the \"it\" that brought the disease in P1 will not be considered to refer to the infection \"i\" or the death \"d\" in P3. This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #CITATION_TAG , p. 329 ) ."], "CC845": ["This approach is taken in computational syntactic grammars (e.g. #CITATION_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language ."], "CC847": ["We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #CITATION_TAG .", "Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #CITATION_TAG ."], "CC848": ["However, \"but\" does not behave quite like the other two--semantically, \"but\" signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition ( called \"adversative\" or \"contrary-to-expectation\" by #CITATION_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .", "And,\" \"or,\" and \"but\" are the three main coordinating connectives in English. However, \"but\" does not behave quite like the other two--semantically, \"but\" signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition ( called \"adversative\" or \"contrary-to-expectation\" by #CITATION_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) ."], "CC849": ["Although there are other discussions of the paragraph as a central element of discourse (e.g. #CITATION_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them."], "CC850": ["This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #CITATION_TAG , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981).", "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #CITATION_TAG , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981).", "Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #CITATION_TAG , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981)."], "CC851": ["Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #CITATION_TAG and Haberlandt et al. ( 1980 ) .", "Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #CITATION_TAG and Haberlandt et al. ( 1980 ) .", "These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #CITATION_TAG and Haberlandt et al. ( 1980 ) ."], "CC852": ["We have shown elsewhere ( Jensen and Binot 1988 ; #CITATION_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment"], "CC853": ["It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #CITATION_TAGb ) . Zadrozny 1987b).", "This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #CITATION_TAGb ) . Zadrozny 1987b).", "The partial theories pick up from the referential level the most obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #CITATION_TAGb ) . Zadrozny 1987b)."], "CC854": ["Later , #CITATION_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience \"\" in choosing facts from this knowledge base ."], "CC855": [], "CC856": ["Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid.", "The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid.", "Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ).", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid.", "The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ).", "Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ). However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.", "We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid.", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ).", "The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ). However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.", "Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ). However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives. After these remarks we can begin constructing the model of the example paragraph.", "We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ).", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ). However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.", "The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ). However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives. After these remarks we can begin constructing the model of the example paragraph.", "We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ). However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.", "So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #CITATION_TAG describes how first order logic can be augmented with such an operator . Extending and revising Jackendoff\\\"s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (\"that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon\"---ibid. ). However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives. After these remarks we can begin constructing the model of the example paragraph."], "CC857": ["For instance , #CITATION_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , \"\" which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger \"s favorite fruit ?", "A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance , #CITATION_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , \"\" which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger \"s favorite fruit ?", "For instance , #CITATION_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , \"\" which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger \"s favorite fruit ? The pairing of these two sentences may be said to create a small paragraph.", "A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance , #CITATION_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , \"\" which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger \"s favorite fruit ? The pairing of these two sentences may be said to create a small paragraph.", "For instance , #CITATION_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , \"\" which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger \"s favorite fruit ? The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs.", "A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance , #CITATION_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , \"\" which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger \"s favorite fruit ? The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs.", "For instance , #CITATION_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , \"\" which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger \"s favorite fruit ? The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs. We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task."], "CC858": ["Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #CITATION_TAG , and Young ( 1989 ) .", "Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #CITATION_TAG , and Young ( 1989 ) .", "In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #CITATION_TAG , and Young ( 1989 ) ."], "CC859": [], "CC860": ["The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #CITATION_TAG ) . The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.", "The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #CITATION_TAG ) . The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.", "The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #CITATION_TAG ) . The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.", "The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986). The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #CITATION_TAG ) . The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.", "The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #CITATION_TAG ) . The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.", "To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986). The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #CITATION_TAG ) . The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.", "The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986). The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #CITATION_TAG ) . The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community."], "CC861": ["Subsequent processing by the natural language and response generation components was done automatically by the computer ( #CITATION_TAG ) .", "Instead, an experimenter in a separate room typed in the utterances as spoken by the subject. Subsequent processing by the natural language and response generation components was done automatically by the computer ( #CITATION_TAG ) .", "Their speech was recorded in a simulation mode in which the speech recognition component was excluded. Instead, an experimenter in a separate room typed in the utterances as spoken by the subject. Subsequent processing by the natural language and response generation components was done automatically by the computer ( #CITATION_TAG ) ."], "CC863": ["This approach resembles the work by Grishman et al. ( 1986 ) and #CITATION_TAG on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.", "Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by Grishman et al. ( 1986 ) and #CITATION_TAG on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.", "This approach resembles the work by Grishman et al. ( 1986 ) and #CITATION_TAG on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area.", "In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by Grishman et al. ( 1986 ) and #CITATION_TAG on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.", "Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by Grishman et al. ( 1986 ) and #CITATION_TAG on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area.", "This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by Grishman et al. ( 1986 ) and #CITATION_TAG on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.", "In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by Grishman et al. ( 1986 ) and #CITATION_TAG on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area."], "CC864": ["The search algorithm is the standard Viterbi search ( #CITATION_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .", "The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search ( #CITATION_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .", "The recognizer for these systems is the SUMMIT system (Zue et al. 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing. The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search ( #CITATION_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence ."], "CC865": ["Semantic filters can also be used to prevent multiple versions of the same case frame ( #CITATION_TAG ) showing up as complements . For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as \"leave.", "Semantic filters can also be used to prevent multiple versions of the same case frame ( #CITATION_TAG ) showing up as complements . For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as \"leave. Thus a flight can \"leave for Chicago from Boston at nine,\" or, equivalently, \"leave at nine for Chicago from Boston.", "Semantic filters can also be used to prevent multiple versions of the same case frame ( #CITATION_TAG ) showing up as complements . For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as \"leave. Thus a flight can \"leave for Chicago from Boston at nine,\" or, equivalently, \"leave at nine for Chicago from Boston. If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place]s, [to-place]s and [at-time]s is possible."], "CC866": ["Representative systems are described in #CITATION_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .", "Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in #CITATION_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .", "In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in #CITATION_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) ."], "CC867": ["We have not yet made use of TINA \"S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #CITATION_TAG ) . Ultimately we want to incorporate TINA\"S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.", "Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA \"S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #CITATION_TAG ) . Ultimately we want to incorporate TINA\"S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model."], "CC868": ["The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #CITATION_TAG ) . The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.", "To date, four distinct domain-specific versions of TINA have been implemented. The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #CITATION_TAG ) . The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.", "The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #CITATION_TAG ) . The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).", "To date, four distinct domain-specific versions of TINA have been implemented. The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #CITATION_TAG ) . The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).", "The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #CITATION_TAG ) . The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.", "To date, four distinct domain-specific versions of TINA have been implemented. The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #CITATION_TAG ) . The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.", "The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #CITATION_TAG ) . The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community."], "CC869": ["However , the method we are currently using in the ATIS domain ( #CITATION_TAG ) represents our most promising approach to this problem", "Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However , the method we are currently using in the ATIS domain ( #CITATION_TAG ) represents our most promising approach to this problem", "However , the method we are currently using in the ATIS domain ( #CITATION_TAG ) represents our most promising approach to this problem We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).", "Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However , the method we are currently using in the ATIS domain ( #CITATION_TAG ) represents our most promising approach to this problem We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).", "However , the method we are currently using in the ATIS domain ( #CITATION_TAG ) represents our most promising approach to this problem We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects). The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.", "Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However , the method we are currently using in the ATIS domain ( #CITATION_TAG ) represents our most promising approach to this problem We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects). The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.", "However , the method we are currently using in the ATIS domain ( #CITATION_TAG ) represents our most promising approach to this problem We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects). The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree. Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.)."], "CC870": ["The second one, ATIS ( #CITATION_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.", "One, the VOYAGER domain (Zue et al. 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS ( #CITATION_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.", "The second one, ATIS ( #CITATION_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains.", "We currently have two application domains that can carry on a spoken dialog with a user. One, the VOYAGER domain (Zue et al. 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS ( #CITATION_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.", "One, the VOYAGER domain (Zue et al. 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS ( #CITATION_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains.", "The second one, ATIS ( #CITATION_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues.", "We currently have two application domains that can carry on a spoken dialog with a user. One, the VOYAGER domain (Zue et al. 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS ( #CITATION_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains.", "One, the VOYAGER domain (Zue et al. 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS ( #CITATION_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues."], "CC871": ["This approach resembles the work by #CITATION_TAG and Hirschman et al. ( 1975 ) on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.", "Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by #CITATION_TAG and Hirschman et al. ( 1975 ) on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.", "This approach resembles the work by #CITATION_TAG and Hirschman et al. ( 1975 ) on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area.", "In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by #CITATION_TAG and Hirschman et al. ( 1975 ) on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.", "Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by #CITATION_TAG and Hirschman et al. ( 1975 ) on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area.", "This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by #CITATION_TAG and Hirschman et al. ( 1975 ) on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.", "In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by #CITATION_TAG and Hirschman et al. ( 1975 ) on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area."], "CC872": ["Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #CITATION_TAG ) .", "In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out. Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #CITATION_TAG ) .", "This is a problem to be aware of in building grammars from example sentences. In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out. Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #CITATION_TAG ) ."], "CC874": ["One , the VOYAGER domain ( #CITATION_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University . The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights.", "We_currently have two application domains that can carry on a spoken dialog with a user. One , the VOYAGER domain ( #CITATION_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University . The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights.", "One , the VOYAGER domain ( #CITATION_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University . The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights. Work continues on improving all aspects of these domains."], "CC875": ["The example used to illustrate the power of ATNs ( #CITATION_TAG ) , `` John was believed to have been shot , \"\" also parses correctly , because the [ object ] node following the verb `` believed \"\" acts as both an absorber and a ( re ) generator . Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator.", "The example used to illustrate the power of ATNs ( #CITATION_TAG ) , `` John was believed to have been shot , \"\" also parses correctly , because the [ object ] node following the verb `` believed \"\" acts as both an absorber and a ( re ) generator . Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)?", "The example used to illustrate the power of ATNs ( #CITATION_TAG ) , `` John was believed to have been shot , \"\" also parses correctly , because the [ object ] node following the verb `` believed \"\" acts as both an absorber and a ( re ) generator . Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)? The CURRENT-FOCUS slot is not restricted to nodes that represent nouns."], "CC876": ["Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #CITATION_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .", "Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #CITATION_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .", "In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #CITATION_TAG , Niemann ( 1990 ) , and Young ( 1989 ) ."], "CC877": ["The second version ( RM ) concerns the Resource Management task ( #CITATION_TAG ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).", "The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986). The second version ( RM ) concerns the Resource Management task ( #CITATION_TAG ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).", "The second version ( RM ) concerns the Resource Management task ( #CITATION_TAG ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.", "To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986). The second version ( RM ) concerns the Resource Management task ( #CITATION_TAG ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).", "The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986). The second version ( RM ) concerns the Resource Management task ( #CITATION_TAG ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.", "The second version ( RM ) concerns the Resource Management task ( #CITATION_TAG ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.", "To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986). The second version ( RM ) concerns the Resource Management task ( #CITATION_TAG ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.", "The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986). The second version ( RM ) concerns the Resource Management task ( #CITATION_TAG ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community."], "CC878": ["For the A * algorithm ( #CITATION_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion . Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.", "Some modification of this scheme is necessary when the input stream is not deterministic. For the A * algorithm ( #CITATION_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion . Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.", "For the A * algorithm ( #CITATION_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion . Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied. With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found."], "CC879": ["The recognizer for these systems is the SUMMIT system ( #CITATION_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing . The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.", "This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system ( #CITATION_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing . The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.", "The recognizer for these systems is the SUMMIT system ( #CITATION_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing . The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence.", "In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system ( #CITATION_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing . The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.", "This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system ( #CITATION_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing . The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence.", "In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area. In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system ( #CITATION_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing . The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.", "In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system ( #CITATION_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing . The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence."], "CC880": ["The gap mechanism resembles the Hold register idea of ATNs ( #CITATION_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.", ") (Chomsky 1977). The gap mechanism resembles the Hold register idea of ATNs ( #CITATION_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.", "These include agreement constraints, semantic restrictions, subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in \"(which article)/do you think I should read (ti)\"? ) (Chomsky 1977). The gap mechanism resembles the Hold register idea of ATNs ( #CITATION_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes."], "CC881": ["Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #CITATION_TAG .", "Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #CITATION_TAG .", "In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #CITATION_TAG ."], "CC882": ["We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms.", "If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms.", "We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.", "A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms.", "If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.", "We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time. That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.", "When we first integrated this recognizer with TINA, we used a \"wire\" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms.", "A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.", "If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time. That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.", "When we first integrated this recognizer with TINA, we used a \"wire\" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.", "A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #CITATION_TAG ) To produce these `` N-best \"\" alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) . Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time. That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold."], "CC883": ["Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #CITATION_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing . Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.", "Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #CITATION_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing . Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA\"Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."], "CC884": ["The formalization of DLRs provided by #CITATION_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1", "One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory. The formalization of DLRs provided by #CITATION_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1", "The formalization of DLRs provided by #CITATION_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 This description can then be given the standard set-theoretical interpretation of King (1989, 1994)."], "CC885": ["The reader is referred to #CITATION_TAG for a more detailed discussion of our use of constraint propagation.", "This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and Lassez (1988). The reader is referred to #CITATION_TAG for a more detailed discussion of our use of constraint propagation.", "Once we have computed c, we use it to make the extended lexical entry more specific. This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and Lassez (1988). The reader is referred to #CITATION_TAG for a more detailed discussion of our use of constraint propagation."], "CC886": ["This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #CITATION_TAG , 31 ) . A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).", "Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #CITATION_TAG , 31 ) . A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).", "This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #CITATION_TAG , 31 ) . A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."], "CC887": ["27 #CITATION_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry . 28 In order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.", "The way these predicates interconnect is represented in Figure 19. 27 #CITATION_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry . 28 In order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.", "27 #CITATION_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry . 28 In order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates. Since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given."], "CC888": ["html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #CITATION_TAG ) to operate on those raised elements.", "de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #CITATION_TAG ) to operate on those raised elements.", "html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #CITATION_TAG ) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.", "uni-tuebingen. de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #CITATION_TAG ) to operate on those raised elements.", "de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #CITATION_TAG ) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.", "nphil. uni-tuebingen. de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #CITATION_TAG ) to operate on those raised elements.", "uni-tuebingen. de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #CITATION_TAG ) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon."], "CC889": ["This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.", "Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.", "This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.", "Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory. Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.", "Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.", "This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule. The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.", "While this introduces the recursion necessary to permit successive lexical rule application, it also grounds the recursion in a word described by a base lexical entry. Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory. Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.", "Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory. Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.", "Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule. The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.", "This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule. The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules. 9", "While this introduces the recursion necessary to permit successive lexical rule application, it also grounds the recursion in a word described by a base lexical entry. Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory. Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.", "Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory. Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule. The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.", "Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule. The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules. 9", "While this introduces the recursion necessary to permit successive lexical rule application, it also grounds the recursion in a word described by a base lexical entry. Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory. Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule. The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.", "Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory. Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata. This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #CITATION_TAG ) . Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process. As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule. The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules. 9"], "CC890": ["As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.", "Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.", "As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.", "29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987). Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.", "Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.", "As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates. 3\u00b0 The successive unfolding steps are schematically represented in Figure 20.", "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (Tamaki and Sato 1984). 29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987). Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.", "29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987). Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.", "Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates. 3\u00b0 The successive unfolding steps are schematically represented in Figure 20.", "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (Tamaki and Sato 1984). 29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987). Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.", "29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987). Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #CITATION_TAG ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates. 3\u00b0 The successive unfolding steps are schematically represented in Figure 20."], "CC893": ["6 The Partial-VP Topicalization Lexical Rule proposed by #CITATION_TAG , 10 ) is a linguistic example . The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by #CITATION_TAG , 10 ) is a linguistic example . The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "6 The Partial-VP Topicalization Lexical Rule proposed by #CITATION_TAG , 10 ) is a linguistic example . The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.", "4 This interpretation of the signature is sometimes referred to as closed world (Gerdemann and King 1994;Gerdemann 1995). 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by #CITATION_TAG , 10 ) is a linguistic example . The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by #CITATION_TAG , 10 ) is a linguistic example . The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.", "To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world (Gerdemann and King 1994;Gerdemann 1995). 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by #CITATION_TAG , 10 ) is a linguistic example . The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "4 This interpretation of the signature is sometimes referred to as closed world (Gerdemann and King 1994;Gerdemann 1995). 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by #CITATION_TAG , 10 ) is a linguistic example . The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement."], "CC894": ["4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).", "To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).", "4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.", "Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).", "To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.", "4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "The terminology used in the literature varies. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).", "Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.", "To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.", "The terminology used in the literature varies. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.", "Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.", "The terminology used in the literature varies. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #CITATION_TAG ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement."], "CC895": ["Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #CITATION_TAG ) .", "This idea of preserving properties can be considered an instance of the well-known frame problem in AI (McCarthy and Hayes 1969), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule. Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #CITATION_TAG ) .", "(Pollard and Sag [1994, 314], following Flickinger [1987]). This idea of preserving properties can be considered an instance of the well-known frame problem in AI (McCarthy and Hayes 1969), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule. Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #CITATION_TAG ) ."], "CC896": ["As shown in #CITATION_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .", "This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. As shown in #CITATION_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .", "Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry. This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. As shown in #CITATION_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries ."], "CC897": ["The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #CITATION_TAG ) . 29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).", "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #CITATION_TAG ) . 29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987). Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.", "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #CITATION_TAG ) . 29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987). Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of the clause."], "CC898": ["In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #CITATION_TAG ; Sanfilippo 1995).", "Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #CITATION_TAG ; Sanfilippo 1995).", "In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #CITATION_TAG ; Sanfilippo 1995). The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."], "CC899": ["A similar method is included in PATR-II ( #CITATION_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.", "This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). A similar method is included in PATR-II ( #CITATION_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."], "CC900": ["A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #CITATION_TAG ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.", "This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #CITATION_TAG ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."], "CC901": ["Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #CITATION_TAG ) and the . lexical rules (DLRs; Meurers 1995).", "While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #CITATION_TAG ) and the . lexical rules (DLRs; Meurers 1995).", "Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #CITATION_TAG ) and the . lexical rules (DLRs; Meurers 1995). 5"], "CC902": ["The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #CITATION_TAG ; Gotz and Meurers 1997a )", "The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #CITATION_TAG ; Gotz and Meurers 1997a ) We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).", "The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #CITATION_TAG ; Gotz and Meurers 1997a ) We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994). This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form."], "CC903": ["Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #CITATION_TAG ; Calcagno and Pollard 1995 ) and the . lexical rules (DLRs; Meurers 1995).", "While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #CITATION_TAG ; Calcagno and Pollard 1995 ) and the . lexical rules (DLRs; Meurers 1995).", "Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #CITATION_TAG ; Calcagno and Pollard 1995 ) and the . lexical rules (DLRs; Meurers 1995). 5"], "CC904": ["html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #CITATION_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.", "de/sfb /b4home. html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #CITATION_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.", "html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #CITATION_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.", "uni-tuebingen. de/sfb /b4home. html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #CITATION_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.", "de/sfb /b4home. html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #CITATION_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.", "nphil. uni-tuebingen. de/sfb /b4home. html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #CITATION_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.", "uni-tuebingen. de/sfb /b4home. html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #CITATION_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon."], "CC905": ["A common computational treatment of lexical rules adopted , for example , in the ALE system ( #CITATION_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time . While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.", "A common computational treatment of lexical rules adopted , for example , in the ALE system ( #CITATION_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time . While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation. We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.", "A common computational treatment of lexical rules adopted , for example , in the ALE system ( #CITATION_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time . While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation. We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available. A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited."], "CC906": ["A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #CITATION_TAG , 1994 ) . The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects.", "A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #CITATION_TAG , 1994 ) . The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects. These atomic expressions can be combined using conjunction, disjunction, and negation."], "CC907": ["The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29 The unfolding transformation is also referred to as partial execution , for example , by #CITATION_TAG . Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.", "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29 The unfolding transformation is also referred to as partial execution , for example , by #CITATION_TAG . Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of the clause."], "CC908": ["A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #CITATION_TAG ; Emele 1994 ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.", "This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #CITATION_TAG ; Emele 1994 ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."], "CC909": ["This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism.", "The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism.", "This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism. We do not make the linguistic claim that passives should be analyzed using such a lexical rule.", "One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory. The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism.", "The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism. We do not make the linguistic claim that passives should be analyzed using such a lexical rule.", "This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism. We do not make the linguistic claim that passives should be analyzed using such a lexical rule. For space reasons, the SYNSEM feature is abbreviated by its first letter.", "One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory. The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism. We do not make the linguistic claim that passives should be analyzed using such a lexical rule.", "The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism. We do not make the linguistic claim that passives should be analyzed using such a lexical rule. For space reasons, the SYNSEM feature is abbreviated by its first letter.", "This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism. We do not make the linguistic claim that passives should be analyzed using such a lexical rule. For space reasons, the SYNSEM feature is abbreviated by its first letter. The traditional (First I Rest) list notation is used, and the operator \u2022 stands for the append relation in the usual way.", "One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory. The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism. We do not make the linguistic claim that passives should be analyzed using such a lexical rule. For space reasons, the SYNSEM feature is abbreviated by its first letter.", "The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. This description can then be given the standard set-theoretical interpretation of #CITATION_TAG , 1994 ) . 11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism. We do not make the linguistic claim that passives should be analyzed using such a lexical rule. For space reasons, the SYNSEM feature is abbreviated by its first letter. The traditional (First I Rest) list notation is used, and the operator \u2022 stands for the append relation in the usual way."], "CC910": ["In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #CITATION_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."], "CC911": [], "CC912": ["32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #CITATION_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation . Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup."], "CC913": [], "CC915": ["Using an accumulator passing technique ( #CITATION_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules . Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail.", "Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause. Using an accumulator passing technique ( #CITATION_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules . Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail.", "In fact, one can view the representations as notational variants of one another. Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause. Using an accumulator passing technique ( #CITATION_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules . Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail."], "CC916": ["15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention", "14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention", "15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).", "13 A more detailed presentation can be found in Minnen (in preparation). 14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention", "14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).", "15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994). Computationally, a subsumption test could equally well be used in our compiler.", "The reader interested in that language and its precise interpretation can find the relevant details in that paper. 13 A more detailed presentation can be found in Minnen (in preparation). 14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention", "13 A more detailed presentation can be found in Minnen (in preparation). 14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).", "14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994). Computationally, a subsumption test could equally well be used in our compiler.", "The reader interested in that language and its precise interpretation can find the relevant details in that paper. 13 A more detailed presentation can be found in Minnen (in preparation). 14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).", "13 A more detailed presentation can be found in Minnen (in preparation). 14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 #CITATION_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994). Computationally, a subsumption test could equally well be used in our compiler."], "CC917": ["In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #CITATION_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."], "CC918": ["In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #CITATION_TAG ; Opalka 1995 ; Sanfilippo 1995 ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."], "CC919": ["4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).", "To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).", "4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.", "Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).", "To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.", "4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "The terminology used in the literature varies. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).", "Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.", "To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.", "The terminology used in the literature varies. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.", "Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.", "The terminology used in the literature varies. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.", "Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #CITATION_TAG ; Gerdemann 1995 ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement."], "CC920": ["html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #CITATION_TAG ) to operate on those raised elements . Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.", "de/sfb / b4home. html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #CITATION_TAG ) to operate on those raised elements . Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.", "uni-tuebingen. de/sfb / b4home. html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #CITATION_TAG ) to operate on those raised elements . Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon."], "CC921": ["16 A linguistic example based on the signature given by #CITATION_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn.", "16 A linguistic example based on the signature given by #CITATION_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20)."], "CC922": ["The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #CITATION_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .", "The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup. The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #CITATION_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program ."], "CC923": ["In a number of proposals , lexical generalizations are captured using lexical underspecification ( #CITATION_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."], "CC924": ["In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #CITATION_TAG ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."], "CC925": ["html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #CITATION_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.", "de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #CITATION_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.", "html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #CITATION_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.", "uni-tuebingen. de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #CITATION_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.", "de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #CITATION_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.", "nphil. uni-tuebingen. de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #CITATION_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.", "uni-tuebingen. de/sfb /b4home. html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #CITATION_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon."], "CC926": ["Based on the research results reported in #CITATION_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG", "Based on the research results reported in #CITATION_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG We developed a compiler that takes as its input a set of lexical rules, deduces the nec- essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.", "Based on the research results reported in #CITATION_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG We developed a compiler that takes as its input a set of lexical rules, deduces the nec- essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries. Each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo."], "CC927": ["This approach is taken , for example , in LKB ( #CITATION_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) . A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).", "Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( #CITATION_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) . A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).", "This approach is taken , for example , in LKB ( #CITATION_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) . A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."], "CC929": ["Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #CITATION_TAG , ch", "written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #CITATION_TAG , ch", "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #CITATION_TAG , ch This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 \u00b0 The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.", "written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #CITATION_TAG , ch This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 \u00b0 The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.", "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #CITATION_TAG , ch This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 \u00b0 The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output. The index that the subject bore in the input is assigned to an optional prepositional complement in the output."], "CC930": ["In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #CITATION_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).", "Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #CITATION_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).", "In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #CITATION_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995). The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."], "CC931": ["Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #CITATION_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch", "written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #CITATION_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch", "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #CITATION_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 \u00b0 The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.", "written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #CITATION_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 \u00b0 The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.", "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #CITATION_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 \u00b0 The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output. The index that the subject bore in the input is assigned to an optional prepositional complement in the output."], "CC932": ["Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #CITATION_TAG )"], "CC933": ["#CITATION_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge . NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004).", "However, the phrases detected are not necessarily mentions that we need to discover. #CITATION_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge . NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004).", "#CITATION_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge . NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004). The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities.", "Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. #CITATION_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge . NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004).", "However, the phrases detected are not necessarily mentions that we need to discover. #CITATION_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge . NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004). The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities.", "They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. #CITATION_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge . NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004).", "Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. #CITATION_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge . NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004). The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities."], "CC934": ["For this mention-pair coreference model \u00cf\\x86 ( u , v ) , we use the same set of features used in #CITATION_TAG .", "Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred. For this mention-pair coreference model \u00cf\\x86 ( u , v ) , we use the same set of features used in #CITATION_TAG ."], "CC935": ["In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #CITATION_TAG ) .", "In both cases, the mention heads are sufficient to support the decisions: \"they\" refers to \"companies and \"They\" refers to \"manufacturers\". In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #CITATION_TAG ) .", "In the example above, the first \"they\" refers to \"Multinational companies investing in China\" and the second \"They\" refers to \"Domestic manufacturers, who are also suffering\". In both cases, the mention heads are sufficient to support the decisions: \"they\" refers to \"companies and \"They\" refers to \"manufacturers\". In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #CITATION_TAG ) ."], "CC936": ["For Berkeley system , we use the reported results from #CITATION_TAG .", "Results for HOTCoref are slightly different from the results reported in Bj\u00f6rkelund and Kuhn (2014). For Berkeley system , we use the reported results from #CITATION_TAG .", "12 We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input. Results for HOTCoref are slightly different from the results reported in Bj\u00f6rkelund and Kuhn (2014). For Berkeley system , we use the reported results from #CITATION_TAG ."], "CC937": ["Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.", "1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.", "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.", "3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.", "1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.", "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time.", "The nonoverlapping mention head assumption in Sec. 3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.", "3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.", "1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time.", "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0.", "The nonoverlapping mention head assumption in Sec. 3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.", "3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time.", "1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0.", "The nonoverlapping mention head assumption in Sec. 3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time.", "3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #CITATION_TAG ) and HOTCoref system ( Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0."], "CC938": ["We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #CITATION_TAG ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.", "The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #CITATION_TAG ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.", "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #CITATION_TAG ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.", "The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #CITATION_TAG ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.", "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #CITATION_TAG ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets.", "The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #CITATION_TAG ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets.", "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #CITATION_TAG ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively."], "CC939": ["We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #CITATION_TAG ) to identify their heads . When these extracted heads do not overlap with gold mention heads, we treat them as negative examples.", "3), we train on a set of candidates with precision larger than 50%. We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #CITATION_TAG ) to identify their heads . When these extracted heads do not overlap with gold mention heads, we treat them as negative examples.", "Specifically, after mention head candidate generation (described in Sec. 3), we train on a set of candidates with precision larger than 50%. We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #CITATION_TAG ) to identify their heads . When these extracted heads do not overlap with gold mention heads, we treat them as negative examples."], "CC940": ["Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #CITATION_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.", "Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception). Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #CITATION_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.", "Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #CITATION_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty. Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1.", "Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception). Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #CITATION_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty. Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1.", "Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #CITATION_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty. Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1. Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions.", "Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception). Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #CITATION_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty. Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1. Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions.", "Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #CITATION_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) . However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty. Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1. Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions. These performance gaps are worrisome, since the real goal of NLP systems is to process raw data."], "CC941": ["Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #CITATION_TAG . The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks."], "CC942": ["The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.", "We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008). The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.", "The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets.", "The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008). The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.", "We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008). The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets.", "The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively.", "The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008). The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets.", "We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008). The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively.", "The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference.", "The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008). The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively.", "We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008). The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #CITATION_TAG ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference."], "CC943": ["Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.", "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.", "Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.", "1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.", "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.", "Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time.", "3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.", "1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.", "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time.", "Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0.", "3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.", "1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time.", "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0.", "3.1. 1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time.", "1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bj\u00f6rkelund and Kuhn, 2014). Developed Systems Our developed system is built on the work by #CITATION_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0."], "CC944": ["In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #CITATION_TAG in our experiments .", "The introduction of ILP methods has influenced the coreference area too Denis and Baldridge, 2007). In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #CITATION_TAG in our experiments .", "Machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001). The introduction of ILP methods has influenced the coreference area too Denis and Baldridge, 2007). In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #CITATION_TAG in our experiments ."], "CC945": ["We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #CITATION_TAG ; Bengtson and Roth , 2008 ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.", "The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #CITATION_TAG ; Bengtson and Roth , 2008 ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.", "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #CITATION_TAG ; Bengtson and Roth , 2008 ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.", "The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #CITATION_TAG ; Bengtson and Roth , 2008 ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.", "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #CITATION_TAG ; Bengtson and Roth , 2008 ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets.", "The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #CITATION_TAG ; Bengtson and Roth , 2008 ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets.", "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #CITATION_TAG ; Bengtson and Roth , 2008 ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively."], "CC946": ["More details can be found in #CITATION_TAG et al. (2013)."], "CC947": ["Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #CITATION_TAG consider joint coreference and entity-linking . The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.", "Several recent works suggest studying coreference jointly with other tasks. Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #CITATION_TAG consider joint coreference and entity-linking . The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.", "Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #CITATION_TAG consider joint coreference and entity-linking . The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework. While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different."], "CC948": ["Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #CITATION_TAG ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.", "The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0 dataset only has mention annotations. Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #CITATION_TAG ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.", "Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #CITATION_TAG ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.", "The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0 dataset only has mention annotations. Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #CITATION_TAG ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.", "Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #CITATION_TAG ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.", "The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0 dataset only has mention annotations. Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #CITATION_TAG ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.", "Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #CITATION_TAG ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1. 1."], "CC949": ["Our work is inspired by the latent left-linking model in #CITATION_TAG and the ILP formulation from Chang et al. ( 2011 ) . The joint learning and inference model takes as input mention head candidates (Sec.", "This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in #CITATION_TAG and the ILP formulation from Chang et al. ( 2011 ) . The joint learning and inference model takes as input mention head candidates (Sec.", "Our work is inspired by the latent left-linking model in #CITATION_TAG and the ILP formulation from Chang et al. ( 2011 ) . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.", "This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in #CITATION_TAG and the ILP formulation from Chang et al. ( 2011 ) . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.", "Our work is inspired by the latent left-linking model in #CITATION_TAG and the ILP formulation from Chang et al. ( 2011 ) . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.", "This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in #CITATION_TAG and the ILP formulation from Chang et al. ( 2011 ) . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.", "Our work is inspired by the latent left-linking model in #CITATION_TAG and the ILP formulation from Chang et al. ( 2011 ) . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones."], "CC950": ["We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #CITATION_TAG ) . (Hovy et al., 2006)."], "CC953": ["In order to estimate the parameters of our model , we develop a blocked sampler based on that of #CITATION_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities . However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.", "In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model , we develop a blocked sampler based on that of #CITATION_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities . However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.", "In order to estimate the parameters of our model , we develop a blocked sampler based on that of #CITATION_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities . However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios."], "CC954": ["We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #CITATION_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) ."], "CC955": [], "CC956": ["We use the same splits as #CITATION_TAG . Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\\\X)/X rather than introducing special conjunction rules.", "Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as #CITATION_TAG . Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\\\X)/X rather than introducing special conjunction rules.", "We use the same splits as #CITATION_TAG . Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\\\X)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999).", "Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as #CITATION_TAG . Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\\\X)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999).", "We use the same splits as #CITATION_TAG . Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\\\X)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999). For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k).", "Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as #CITATION_TAG . Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\\\X)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999). For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k).", "We use the same splits as #CITATION_TAG . Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\\\X)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999). For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k). For Chinese and Italian, for training efficiency, we used only raw sentences that were 50 words or fewer (note that we did not drop tag dictionary set or test set sentences)."], "CC957": ["This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar", "If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar", "This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree.", "If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar", "If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree.", "This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t\u2192 D , v and t\u2192 v, D . Recall that in \u00a73.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.", "The parser first tries to find a valid parse that has either s dcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar", "If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree.", "If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t\u2192 D , v and t\u2192 v, D . Recall that in \u00a73.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.", "This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t\u2192 D , v and t\u2192 v, D . Recall that in \u00a73.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary. Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents.", "The parser first tries to find a valid parse that has either s dcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree.", "If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t\u2192 D , v and t\u2192 v, D . Recall that in \u00a73.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.", "If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t\u2192 D , v and t\u2192 v, D . Recall that in \u00a73.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary. Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents.", "The parser first tries to find a valid parse that has either s dcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t\u2192 D , v and t\u2192 v, D . Recall that in \u00a73.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.", "If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #CITATION_TAG , but we do it directly in the grammar We add unary rules of the form D \u2192u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t\u2192 D , v and t\u2192 v, D . Recall that in \u00a73.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary. Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents."], "CC958": ["We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #CITATION_TAG ) .", "Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #CITATION_TAG ) .", "In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015). Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #CITATION_TAG ) ."], "CC959": ["We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #CITATION_TAG ; Garrette et al. , 2015 ) .4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags."], "CC960": ["Since we are not generating from the model , this does not introduce difficulties ( #CITATION_TAG ) ."], "CC961": ["We further add rules for combining with punctuation to the left and right and allow for the merge rule X \u00e2\\x86\\x92 X X of #CITATION_TAG .", "We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X \u00e2\\x86\\x92 X X of #CITATION_TAG .", "A category (s\\\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\\\) with a noun phrase (np) that serves as its subject. We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X \u00e2\\x86\\x92 X X of #CITATION_TAG ."], "CC962": [], "CC963": ["Our strategy is based on the approach presented by #CITATION_TAG . At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.", "Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution. Our strategy is based on the approach presented by #CITATION_TAG . At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.", "Our strategy is based on the approach presented by #CITATION_TAG . At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences. To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.", "We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences. Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution. Our strategy is based on the approach presented by #CITATION_TAG . At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.", "Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution. Our strategy is based on the approach presented by #CITATION_TAG . At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences. To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.", "Our strategy is based on the approach presented by #CITATION_TAG . At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences. To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy. By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.", "We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences. Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution. Our strategy is based on the approach presented by #CITATION_TAG . At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences. To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.", "Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution. Our strategy is based on the approach presented by #CITATION_TAG . At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences. To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy. By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities."], "CC964": ["One important example is the constituentcontext model ( CCM ) of #CITATION_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear", "One important example is the constituentcontext model ( CCM ) of #CITATION_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.", "One important example is the constituentcontext model ( CCM ) of #CITATION_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts. For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB."], "CC965": ["We follow #CITATION_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules", "A category (s\\\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\\\) with a noun phrase (np) that serves as its subject. We follow #CITATION_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules", "We follow #CITATION_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules We further add rules for combining with punctuation to the left and right and allow for the merge rule X \u2192 X X of Clark and Curran (2007).", "The direction of the slash operator gives the behavior of the function. A category (s\\\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\\\) with a noun phrase (np) that serves as its subject. We follow #CITATION_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules", "A category (s\\\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\\\) with a noun phrase (np) that serves as its subject. We follow #CITATION_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules We further add rules for combining with punctuation to the left and right and allow for the merge rule X \u2192 X X of Clark and Curran (2007).", "Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1. The direction of the slash operator gives the behavior of the function. A category (s\\\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\\\) with a noun phrase (np) that serves as its subject. We follow #CITATION_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules", "The direction of the slash operator gives the behavior of the function. A category (s\\\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\\\) with a noun phrase (np) that serves as its subject. We follow #CITATION_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules We further add rules for combining with punctuation to the left and right and allow for the merge rule X \u2192 X X of Clark and Curran (2007)."], "CC966": ["Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #CITATION_TAG . The motivation for the features was to capture declaratively decisions made by the randomized SPG.", "In total, we used 3,291 features in training the SPR. Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #CITATION_TAG . The motivation for the features was to capture declaratively decisions made by the randomized SPG.", "Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #CITATION_TAG . The motivation for the features was to capture declaratively decisions made by the randomized SPG. We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times.", "Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones. In total, we used 3,291 features in training the SPR. Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #CITATION_TAG . The motivation for the features was to capture declaratively decisions made by the randomized SPG.", "In total, we used 3,291 features in training the SPR. Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #CITATION_TAG . The motivation for the features was to capture declaratively decisions made by the randomized SPG. We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times.", "Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features. Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones. In total, we used 3,291 features in training the SPR. Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #CITATION_TAG . The motivation for the features was to capture declaratively decisions made by the randomized SPG.", "Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones. In total, we used 3,291 features in training the SPR. Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #CITATION_TAG . The motivation for the features was to capture declaratively decisions made by the randomized SPG. We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times."], "CC967": ["Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #CITATION_TAG ) for a recent example with further references )", "Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #CITATION_TAG ) for a recent example with further references ) This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly."], "CC968": ["The SPR uses rules automatically learned from training data , using techniques similar to ( #CITATION_TAG ; Freund et al. , 1998 ) .", "Our primary contribution is a method for training the SPR. The SPR uses rules automatically learned from training data , using techniques similar to ( #CITATION_TAG ; Freund et al. , 1998 ) .", "In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer. Our primary contribution is a method for training the SPR. The SPR uses rules automatically learned from training data , using techniques similar to ( #CITATION_TAG ; Freund et al. , 1998 ) ."], "CC971": [], "CC972": [], "CC973": [], "CC974": [], "CC975": [], "CC976": [], "CC977": ["In our previous work ( #CITATION_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.", "The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work ( #CITATION_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.", "In our previous work ( #CITATION_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).", "In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work ( #CITATION_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.", "The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work ( #CITATION_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).", "In our previous work ( #CITATION_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data.", "In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work ( #CITATION_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).", "The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work ( #CITATION_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data."], "CC978": ["In order to obtain semantic representations of each word , we apply our previous strategy ( #CITATION_TAG ) . Rather than using a termdocument matrix, we had followed an approach akin to that of Sch\u00fctze (1993), who performed SVD on a Nx2N term-term matrix.", "In order to obtain semantic representations of each word , we apply our previous strategy ( #CITATION_TAG ) . Rather than using a termdocument matrix, we had followed an approach akin to that of Sch\u00fctze (1993), who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1."], "CC979": ["Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models.", "We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models.", "Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise).", "The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\\\"s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (\"incorrect runs\"). We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models.", "We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise).", "Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed.", "As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue. The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\\\"s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (\"incorrect runs\"). We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models.", "The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\\\"s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (\"incorrect runs\"). We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise).", "We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed.", "Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed. To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users.", "As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue. The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\\\"s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (\"incorrect runs\"). We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise).", "The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\\\"s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (\"incorrect runs\"). We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed.", "We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed. To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users.", "As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue. The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\\\"s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (\"incorrect runs\"). We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed.", "The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\\\"s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (\"incorrect runs\"). We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #CITATION_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed. To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users."], "CC981": [], "CC982": [], "CC983": [], "CC984": [], "CC986": [], "CC989": [], "CC990": [], "CC991": [], "CC992": [], "CC994": [], "CC995": [], "CC996": [], "CC997": [], "CC998": ["To quantify the relative strengths of these transitive inferences , #CITATION_TAG propose to assign a weight to each link . Say the order a, b occurs m times and the pair {a, b} occurs n times in total.", "The first ordered pairs are more frequent, as are the individual adjectives involved. To quantify the relative strengths of these transitive inferences , #CITATION_TAG propose to assign a weight to each link . Say the order a, b occurs m times and the pair {a, b} occurs n times in total.", "To quantify the relative strengths of these transitive inferences , #CITATION_TAG propose to assign a weight to each link . Say the order a, b occurs m times and the pair {a, b} occurs n times in total. Then the weight of the pair a \u2192 b is:", "Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second. The first ordered pairs are more frequent, as are the individual adjectives involved. To quantify the relative strengths of these transitive inferences , #CITATION_TAG propose to assign a weight to each link . Say the order a, b occurs m times and the pair {a, b} occurs n times in total.", "The first ordered pairs are more frequent, as are the individual adjectives involved. To quantify the relative strengths of these transitive inferences , #CITATION_TAG propose to assign a weight to each link . Say the order a, b occurs m times and the pair {a, b} occurs n times in total. Then the weight of the pair a \u2192 b is:"], "CC999": ["More generally , distributional clustering techniques ( Sch \u00c2\u00a8 utze , 1992 ; #CITATION_TAG ) could be applied to extract semantic classes from the corpus itself . Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results.", "Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available. More generally , distributional clustering techniques ( Sch \u00c2\u00a8 utze , 1992 ; #CITATION_TAG ) could be applied to extract semantic classes from the corpus itself . Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results.", "First, while semantic information is not available for all adjectives, it is clearly available for some. Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available. More generally , distributional clustering techniques ( Sch \u00c2\u00a8 utze , 1992 ; #CITATION_TAG ) could be applied to extract semantic classes from the corpus itself . Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results."], "CC1000": [], "CC1001": ["In particular , boosting ( Schapire , 1999 ; #CITATION_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .", "It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997). In particular , boosting ( Schapire , 1999 ; #CITATION_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .", "The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data. It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997). In particular , boosting ( Schapire , 1999 ; #CITATION_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly ."], "CC1002": ["#CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation", "Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a \u227a b. If the re-verse is true, and b, a is found more often than a, b , then b \u227a a. If neither order appears in the training data, then neither a \u227a b nor b \u227a a and an order must be randomly assigned. #CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation", "#CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation That is, if a \u227a c and c \u227a b, we can conclude that a \u227a b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.", "One way to think of the direct evidence method is to see that it defines a relation \u227a on the set of English adjectives. Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a \u227a b. If the re-verse is true, and b, a is found more often than a, b , then b \u227a a. If neither order appears in the training data, then neither a \u227a b nor b \u227a a and an order must be randomly assigned. #CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation", "Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a \u227a b. If the re-verse is true, and b, a is found more often than a, b , then b \u227a a. If neither order appears in the training data, then neither a \u227a b nor b \u227a a and an order must be randomly assigned. #CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation That is, if a \u227a c and c \u227a b, we can conclude that a \u227a b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.", "#CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation That is, if a \u227a c and c \u227a b, we can conclude that a \u227a b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method. However, the pairs large, new and new, green occur fairly frequently.", "One way to think of the direct evidence method is to see that it defines a relation \u227a on the set of English adjectives. Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a \u227a b. If the re-verse is true, and b, a is found more often than a, b , then b \u227a a. If neither order appears in the training data, then neither a \u227a b nor b \u227a a and an order must be randomly assigned. #CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation That is, if a \u227a c and c \u227a b, we can conclude that a \u227a b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.", "Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a \u227a b. If the re-verse is true, and b, a is found more often than a, b , then b \u227a a. If neither order appears in the training data, then neither a \u227a b nor b \u227a a and an order must be randomly assigned. #CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation That is, if a \u227a c and c \u227a b, we can conclude that a \u227a b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method. However, the pairs large, new and new, green occur fairly frequently.", "#CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation That is, if a \u227a c and c \u227a b, we can conclude that a \u227a b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method. However, the pairs large, new and new, green occur fairly frequently. Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.", "One way to think of the direct evidence method is to see that it defines a relation \u227a on the set of English adjectives. Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a \u227a b. If the re-verse is true, and b, a is found more often than a, b , then b \u227a a. If neither order appears in the training data, then neither a \u227a b nor b \u227a a and an order must be randomly assigned. #CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation That is, if a \u227a c and c \u227a b, we can conclude that a \u227a b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method. However, the pairs large, new and new, green occur fairly frequently.", "Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a \u227a b. If the re-verse is true, and b, a is found more often than a, b , then b \u227a a. If neither order appears in the training data, then neither a \u227a b nor b \u227a a and an order must be randomly assigned. #CITATION_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation That is, if a \u227a c and c \u227a b, we can conclude that a \u227a b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method. However, the pairs large, new and new, green occur fairly frequently. Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order."], "CC1003": ["One approach to this more general problem , taken by the ` Nitrogen \" generator ( #CITATION_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.", "The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system. One approach to this more general problem , taken by the ` Nitrogen \" generator ( #CITATION_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.", "One approach to this more general problem , taken by the ` Nitrogen \" generator ( #CITATION_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word. It also should be straightforwardly applicable to the more specific problem we are addressing here.", "The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system. One approach to this more general problem , taken by the ` Nitrogen \" generator ( #CITATION_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word. It also should be straightforwardly applicable to the more specific problem we are addressing here.", "One approach to this more general problem , taken by the ` Nitrogen \" generator ( #CITATION_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word. It also should be straightforwardly applicable to the more specific problem we are addressing here. To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.", "The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system. One approach to this more general problem , taken by the ` Nitrogen \" generator ( #CITATION_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word. It also should be straightforwardly applicable to the more specific problem we are addressing here. To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.", "One approach to this more general problem , taken by the ` Nitrogen \" generator ( #CITATION_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word. It also should be straightforwardly applicable to the more specific problem we are addressing here. To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability. This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood."], "CC1004": ["Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , \" including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #CITATION_TAG ) . Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources."], "CC1005": ["f\u00ce\u00b8 on demand ( #CITATION_TAG ) can pay off here , since only part of f\u00ce\u00b8 may be needed subsequently . )"], "CC1006": ["#CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths.", "We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths.", "#CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring.", "The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths.", "We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring.", "#CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring. 15 rdinary probabilities fall in the semiring (R \u22650 , +, \u00d7, * ).", "But while computing this, we will also compute the numerator. The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths.", "The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring.", "We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring. 15 rdinary probabilities fall in the semiring (R \u22650 , +, \u00d7, * ).", "#CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring. 15 rdinary probabilities fall in the semiring (R \u22650 , +, \u00d7, * ). 16 Our novel weights fall in a novel 14 Formal derivation of (1):", "But while computing this, we will also compute the numerator. The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring.", "The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring. 15 rdinary probabilities fall in the semiring (R \u22650 , +, \u00d7, * ).", "We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring. 15 rdinary probabilities fall in the semiring (R \u22650 , +, \u00d7, * ). 16 Our novel weights fall in a novel 14 Formal derivation of (1):", "But while computing this, we will also compute the numerator. The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring. 15 rdinary probabilities fall in the semiring (R \u22650 , +, \u00d7, * ).", "The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset \u03a0 must be ( \u03c0\u2208\u03a0 P (\u03c0), \u03c0\u2208\u03a0 P (\u03c0) val(\u03c0)) \u2208 R \u22650 \u00d7 V , from which (1) is trivial to compute. #CITATION_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations \u2297 and \u2295 on K. Thus \u2297 is used to combine arc weights into a path weight and \u2295 is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = \u221e i=0 k i . The usual finite-state algorithms work if (K, \u2295, \u2297, * ) has the structure of a closed semiring. 15 rdinary probabilities fall in the semiring (R \u22650 , +, \u00d7, * ). 16 Our novel weights fall in a novel 14 Formal derivation of (1):"], "CC1007": ["For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG )", "20 hen Tarjan\"s method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of \u2295 and \u2297 operations. For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG )", "For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG ) But notice that it has no backward pass.", "\u2022 In many cases of interest, T i is an acyclic graph. 20 hen Tarjan\"s method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of \u2295 and \u2297 operations. For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG )", "20 hen Tarjan\"s method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of \u2295 and \u2297 operations. For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG ) But notice that it has no backward pass.", "For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG ) But notice that it has no backward pass. In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.", "\u2022 In many cases of interest, T i is an acyclic graph. 20 hen Tarjan\"s method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of \u2295 and \u2297 operations. For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG ) But notice that it has no backward pass.", "20 hen Tarjan\"s method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of \u2295 and \u2297 operations. For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG ) But notice that it has no backward pass. In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.", "For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG ) But notice that it has no backward pass. In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities. This is slower because our \u2295 and \u2297 are vector operations, and the vectors rapidly lose sparsity as they are added together.", "\u2022 In many cases of interest, T i is an acyclic graph. 20 hen Tarjan\"s method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of \u2295 and \u2297 operations. For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG ) But notice that it has no backward pass. In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.", "20 hen Tarjan\"s method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of \u2295 and \u2297 operations. For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #CITATION_TAG ) But notice that it has no backward pass. In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities. This is slower because our \u2295 and \u2297 are vector operations, and the vectors rapidly lose sparsity as they are added together."], "CC1008": ["The availability of toolkits for this weighted case ( #CITATION_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP . Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998)."], "CC1009": ["For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #CITATION_TAG ) trains only stochastic edit distance .", "Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens. For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #CITATION_TAG ) trains only stochastic edit distance .", "Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs. Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens. For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #CITATION_TAG ) trains only stochastic edit distance ."], "CC1010": ["The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , \u00e2\\x88\\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #CITATION_TAG ) .", "This speedup also works for cyclic graphs and for any V . Write w jk as (p jk , v jk ), and let w 1 jk = (p 1 jk , v 1 jk ) denote the weight of the edge from j to k. 19 Then it can be shown that w 0n = (p 0n , j,k p 0j v 1 jk p kn ). The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , \u00e2\\x88\\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #CITATION_TAG ) ."], "CC1011": [], "CC1013": ["Per-state joint normalization ( #CITATION_TAGb , \u00c2\u00a7 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization.", "Then the predicted vector given \u03b8 is j,a dj,a \u2022 (expected feature counts on a randomly chosen arc in Dj,a). Per-state joint normalization ( #CITATION_TAGb , \u00c2\u00a7 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization.", "Per-state joint normalization ( #CITATION_TAGb , \u00c2\u00a7 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization. It arises, for example, when training a joint model of the form", "Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a. Then the predicted vector given \u03b8 is j,a dj,a \u2022 (expected feature counts on a randomly chosen arc in Dj,a). Per-state joint normalization ( #CITATION_TAGb , \u00c2\u00a7 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization.", "Then the predicted vector given \u03b8 is j,a dj,a \u2022 (expected feature counts on a randomly chosen arc in Dj,a). Per-state joint normalization ( #CITATION_TAGb , \u00c2\u00a7 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization. It arises, for example, when training a joint model of the form", "13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a \u2208 \u03a3; their weights are normalized to sum to 1. Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a. Then the predicted vector given \u03b8 is j,a dj,a \u2022 (expected feature counts on a randomly chosen arc in Dj,a). Per-state joint normalization ( #CITATION_TAGb , \u00c2\u00a7 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization.", "Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a. Then the predicted vector given \u03b8 is j,a dj,a \u2022 (expected feature counts on a randomly chosen arc in Dj,a). Per-state joint normalization ( #CITATION_TAGb , \u00c2\u00a7 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization. It arises, for example, when training a joint model of the form"], "CC1014": ["The EM algorithm ( #CITATION_TAG ) can maximize these functions . Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f \u03b8 , which FST paths stand a chance of having been the path used?", "The EM algorithm ( #CITATION_TAG ) can maximize these functions . Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f \u03b8 , which FST paths stand a chance of having been the path used? (Guessing the path also guesses the exact input and output.", "The EM algorithm ( #CITATION_TAG ) can maximize these functions . Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f \u03b8 , which FST paths stand a chance of having been the path used? (Guessing the path also guesses the exact input and output. ) The M step updates \u03b8 to make those paths more likely."], "CC1015": [], "CC1016": ["Now for some important remarks on efficiency : \u00e2\\x80\u00a2 Computing ti is an instance of the well-known algebraic path problem ( #CITATION_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).", "Now for some important remarks on efficiency : \u00e2\\x80\u00a2 Computing ti is an instance of the well-known algebraic path problem ( #CITATION_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted). It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.", "Now for some important remarks on efficiency : \u00e2\\x80\u00a2 Computing ti is an instance of the well-known algebraic path problem ( #CITATION_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted). It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version. If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b)."], "CC1018": ["In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #CITATION_TAG ) ."], "CC1019": ["For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #CITATION_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11", "Samples need not be fully observed (partly supervised training): thus xi _ __, yi _ \ufffd_ may be given as regular sets in which input and output were observed to fall. For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #CITATION_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11", "These are assumed to be independent random samples from a joint dis- tribution of the form f_\ufffd(x, y); the goal is to recover the true _\ufffd. Samples need not be fully observed (partly supervised training): thus xi _ __, yi _ \ufffd_ may be given as regular sets in which input and output were observed to fall. For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #CITATION_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11"], "CC1020": ["Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , \" including classic models for speech recognition ( #CITATION_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) . Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources."], "CC1021": ["It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #CITATION_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).", "Let T i = x i \u2022f \u2022y i . Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted). It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #CITATION_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).", "It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #CITATION_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b). For a general graph T i , Tarjan (1981b) shows how to partition into \"hard\" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.", "\u2022 Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a). Let T i = x i \u2022f \u2022y i . Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted). It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #CITATION_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).", "Let T i = x i \u2022f \u2022y i . Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted). It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #CITATION_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b). For a general graph T i , Tarjan (1981b) shows how to partition into \"hard\" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.", "It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #CITATION_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b). For a general graph T i , Tarjan (1981b) shows how to partition into \"hard\" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results. The overhead of partitioning and recombining is essentially only O(m).", "\u2022 Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a). Let T i = x i \u2022f \u2022y i . Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted). It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #CITATION_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b). For a general graph T i , Tarjan (1981b) shows how to partition into \"hard\" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.", "Let T i = x i \u2022f \u2022y i . Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted). It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #CITATION_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b). For a general graph T i , Tarjan (1981b) shows how to partition into \"hard\" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results. The overhead of partitioning and recombining is essentially only O(m)."], "CC1022": ["Undesirable consequences of this fact have been termed \"label bias\" ( #CITATION_TAG ) . Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since \"dead ends\" leak probability mass).", "Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation. Undesirable consequences of this fact have been termed \"label bias\" ( #CITATION_TAG ) . Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since \"dead ends\" leak probability mass).", "Undesirable consequences of this fact have been termed \"label bias\" ( #CITATION_TAG ) . Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since \"dead ends\" leak probability mass). 8 A better-founded approach is global normalization, which simply divides each f (x, y) by", "\u2022 An easy approach is to normalize the options at each state to make the FST Markovian. Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation. Undesirable consequences of this fact have been termed \"label bias\" ( #CITATION_TAG ) . Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since \"dead ends\" leak probability mass).", "Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation. Undesirable consequences of this fact have been termed \"label bias\" ( #CITATION_TAG ) . Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since \"dead ends\" leak probability mass). 8 A better-founded approach is global normalization, which simply divides each f (x, y) by"], "CC1023": [], "CC1025": [], "CC1026": ["Such approaches have been tried recently in restricted cases ( #CITATION_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .", "The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired. Such approaches have been tried recently in restricted cases ( #CITATION_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) ."], "CC1027": ["A more subtle example is weighted FSAs that approximate PCFGs ( #CITATION_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation . These are parameterized by the PCFG\"s parameters, but add or remove strings of the PCFG to leave an improper probability distribution.", "Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 \u2212\u2192 arcs). A more subtle example is weighted FSAs that approximate PCFGs ( #CITATION_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation . These are parameterized by the PCFG\"s parameters, but add or remove strings of the PCFG to leave an improper probability distribution.", "6,7 here are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988). Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 \u2212\u2192 arcs). A more subtle example is weighted FSAs that approximate PCFGs ( #CITATION_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation . These are parameterized by the PCFG\"s parameters, but add or remove strings of the PCFG to leave an improper probability distribution."], "CC1028": ["For example , the forward-backward algorithm ( #CITATION_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .", "Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens. For example , the forward-backward algorithm ( #CITATION_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .", "Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs. Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens. For example , the forward-backward algorithm ( #CITATION_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance ."], "CC1029": ["Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #CITATION_TAGb ; Lafferty et al. , 2001 ) .", "The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired. Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #CITATION_TAGb ; Lafferty et al. , 2001 ) ."], "CC1030": ["The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #CITATION_TAG ; Chen and For globally normalized, joint models, the predicted vector is ec f (\u03a3 * , \u2206 * ).", "\u2022 If arc probabilities (or even \u03bb, \u03bd, \u00b5, \u03c1) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f \u03b8 whose (input, output) matches (x, y). The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #CITATION_TAG ; Chen and For globally normalized, joint models, the predicted vector is ec f (\u03a3 * , \u2206 * ).", "The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #CITATION_TAG ; Chen and For globally normalized, joint models, the predicted vector is ec f (\u03a3 * , \u2206 * ). If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute).", "\u2022 If arc probabilities (or even \u03bb, \u03bd, \u00b5, \u03c1) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f \u03b8 whose (input, output) matches (x, y). The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #CITATION_TAG ; Chen and For globally normalized, joint models, the predicted vector is ec f (\u03a3 * , \u2206 * ). If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute).", "The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #CITATION_TAG ; Chen and For globally normalized, joint models, the predicted vector is ec f (\u03a3 * , \u2206 * ). If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13"], "CC1031": ["A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #CITATION_TAG ; Knight and Graehl , 1998 ) . The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (\u03bb); reading another b (\u03bd); transducing b to p rather than q (\u00b5); starting to transduce p to rather than x (\u03c1).", "A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #CITATION_TAG ; Knight and Graehl , 1998 ) . The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (\u03bb); reading another b (\u03bd); transducing b to p rather than q (\u00b5); starting to transduce p to rather than x (\u03c1). 4 To prove (1)\u21d2(3), express f as an FST and apply the well-known Kleene-Sch\u00fctzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp."], "CC1032": ["The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #CITATION_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and Additionally, several model organism databases or nomenclature databases were used.", "The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #CITATION_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and Additionally, several model organism databases or nomenclature databases were used. Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14]."], "CC1033": [], "CC1034": [], "CC1035": [], "CC1036": ["The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #CITATION_TAG", "The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #CITATION_TAG It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network.", "The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #CITATION_TAG It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network. The META provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept."], "CC1039": ["Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE.", "It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE.", "Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database.", "The PIR-NREF database is a comprehensive database for sequence searching and protein identification. It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE.", "It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database.", "Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database. RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms.", "Additionally, it links to over 70 biological databases in the world. The PIR-NREF database is a comprehensive database for sequence searching and protein identification. It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE.", "The PIR-NREF database is a comprehensive database for sequence searching and protein identification. It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database.", "It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database. RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms.", "Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database. RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms. Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI\"s Map Viewer.", "Additionally, it links to over 70 biological databases in the world. The PIR-NREF database is a comprehensive database for sequence searching and protein identification. It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database.", "The PIR-NREF database is a comprehensive database for sequence searching and protein identification. It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database. RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms.", "It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database. RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms. Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI\"s Map Viewer.", "Additionally, it links to over 70 biological databases in the world. The PIR-NREF database is a comprehensive database for sequence searching and protein identification. It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database. RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms.", "The PIR-NREF database is a comprehensive database for sequence searching and protein identification. It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #CITATION_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database. RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms. Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI\"s Map Viewer."], "CC1040": [], "CC1041": ["The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #CITATION_TAG , and Additionally, several model organism databases or nomenclature databases were used.", "The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #CITATION_TAG , and Additionally, several model organism databases or nomenclature databases were used. Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14]."], "CC1043": [], "CC1044": ["These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #CITATION_TAG ) .", "The knowledge sources we use include parts-of-speech, local collocations, and surrounding words. These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #CITATION_TAG ) ."], "CC1046": ["More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #CITATION_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.", "This provides us with a train- ing set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #CITATION_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.", "Un- like them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowl- edge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC clas- sifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and an- notated with their SCs. This provides us with a train- ing set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #CITATION_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."], "CC1047": ["In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #CITATION_TAG ) . While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.", "In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #CITATION_TAG ) . While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.", "In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #CITATION_TAG ) . While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.", "In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #CITATION_TAG ) . While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.", "In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #CITATION_TAG ) . While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004))."], "CC1048": ["Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #CITATION_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.", "Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #CITATION_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.", "Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #CITATION_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE.", "Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #CITATION_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.", "Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #CITATION_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE.", "Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #CITATION_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.", "Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #CITATION_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE.", "Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #CITATION_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."], "CC1049": ["While these approaches have been reasonably successful ( see #CITATION_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance . In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful ( see #CITATION_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance . In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.", "While these approaches have been reasonably successful ( see #CITATION_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance . In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", "In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful ( see #CITATION_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance . In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)). While these approaches have been reasonably successful ( see #CITATION_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance . In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004))."], "CC1050": ["Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.\ufffds method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #CITATION_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.", "In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.\ufffds method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner. Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.\ufffds method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #CITATION_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.", "Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.\ufffds method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #CITATION_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features. More specifically, let L = i li\\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).", "In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.\ufffds method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner. Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.\ufffds method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #CITATION_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features. More specifically, let L = i li\\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).", "Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.\ufffds method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #CITATION_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features. More specifically, let L = i li\\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2). To represent i, we generate one feature from each non-empty subset of Li."], "CC1051": ["( 4 ) NE : We use BBN \"s IdentiFinder ( #CITATION_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ . If NPi is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type.", "( 4 ) NE : We use BBN \"s IdentiFinder ( #CITATION_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ . If NPi is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type. However, if NPi is determined to be a LOCATION, we create a feature with value GPE (because most of the MUC LOCA- TION NEs are ACE GPE NEs)."], "CC1052": ["However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #CITATION_TAG )", "Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #CITATION_TAG )", "However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #CITATION_TAG ) It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.\"s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic."], "CC1053": ["We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #CITATION_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 )", "Learning algorithms. We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #CITATION_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 )", "We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #CITATION_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) We apply add-one smoothing to smooth the class posteriors."], "CC1054": ["We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #CITATION_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved . Following Ponzetto and Strube (2006), we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition.", "As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #CITATION_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved . Following Ponzetto and Strube (2006), we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition.", "We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #CITATION_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved . Following Ponzetto and Strube (2006), we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder."], "CC1055": ["Following #CITATION_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.", "After training, the decision tree classifier is used to select an antecedent for each NP in a test text. Following #CITATION_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.", "Following #CITATION_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj. If no such NP exists, no antecedent is selected for NPj."], "CC1056": ["Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #CITATION_TAG ) . Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).", "Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #CITATION_TAG ) . Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.", "Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #CITATION_TAG ) . Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE."], "CC1057": ["Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\\x0e, NPi+2, \ufffd \ufffd \ufffd, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #CITATION_TAG , as described below ."], "CC1058": [], "CC1059": ["Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #CITATION_TAG and Yang et al. (2003), as described below.", "Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\\x0e, NPi+2, \ufffd \ufffd \ufffd, NPj. Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #CITATION_TAG and Yang et al. (2003), as described below."], "CC1060": ["Following #CITATION_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition . In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.", "We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al., 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following #CITATION_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition . In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder."], "CC1061": ["As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #CITATION_TAG ) .", "In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #CITATION_TAG ) .", "While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #CITATION_TAG ) ."], "CC1062": ["However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #CITATION_TAG , Markert and Nissim ( 2005 ) )", "Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #CITATION_TAG , Markert and Nissim ( 2005 ) )", "However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #CITATION_TAG , Markert and Nissim ( 2005 ) ) It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.\"s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic."], "CC1063": ["Following previous work ( e.g. , #CITATION_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ...", "Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work ( e.g. , #CITATION_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ...", "Following previous work ( e.g. , #CITATION_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 . ., NP j\u22121 . Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below."], "CC1064": ["We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #CITATION_TAG ) and NE classification ( Collins and Singer , 1999 )", "Learning algorithms. We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #CITATION_TAG ) and NE classification ( Collins and Singer , 1999 )", "We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #CITATION_TAG ) and NE classification ( Collins and Singer , 1999 ) We apply add-one smoothing to smooth the class posteriors."], "CC1065": ["As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #CITATION_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .", "In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #CITATION_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .", "While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #CITATION_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) ."], "CC1067": ["( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #CITATION_TAGa ) ) . Motivated by this observation, we create for each of NP i \"s ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.", "We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #CITATION_TAGa ) ) . Motivated by this observation, we create for each of NP i \"s ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.", "( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #CITATION_TAGa ) ) . Motivated by this observation, we create for each of NP i \"s ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin\"s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.", "An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #CITATION_TAGa ) ) . Motivated by this observation, we create for each of NP i \"s ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.", "We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #CITATION_TAGa ) ) . Motivated by this observation, we create for each of NP i \"s ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin\"s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.", "Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #CITATION_TAGa ) ) . Motivated by this observation, we create for each of NP i \"s ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.", "An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #CITATION_TAGa ) ) . Motivated by this observation, we create for each of NP i \"s ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin\"s dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity."], "CC1068": ["These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.", "(4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.", "These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.", "Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.", "(4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.", "These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.", "(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation. Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.", "Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.", "(4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.", "These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).", "(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation. Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.", "Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.", "(4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).", "(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation. Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.", "Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN\"s IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #CITATION_TAG ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a))."], "CC1070": ["All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #CITATION_TAG ) . The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication.", "All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #CITATION_TAG ) . The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication. In this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions."], "CC1072": ["Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 )", "Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 )", "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 )", "Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 )", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #CITATION_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information."], "CC1073": ["Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #CITATION_TAG for an overview ) . Others have looked at the application of machine learning algorithms to annotated multimodal corpora.", "Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #CITATION_TAG for an overview ) . Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels."], "CC1074": ["Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice.", "A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice.", "Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds.", "However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice.", "A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds.", "Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.", "In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice.", "However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds.", "A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.", "In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds.", "However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( Cohen , 1960 ) 1 and corrected kappa ( #CITATION_TAG ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2."], "CC1075": ["Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).", "Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #CITATION_TAG ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information."], "CC1076": ["These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #CITATION_TAG )", "The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #CITATION_TAG )", "These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #CITATION_TAG ) We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005).", "The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #CITATION_TAG ) We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005).", "These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #CITATION_TAG ) We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005). Therefore, here we show the results of this classifier.", "The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #CITATION_TAG ) We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005). Therefore, here we show the results of this classifier.", "These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #CITATION_TAG ) We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005). Therefore, here we show the results of this classifier. Ten-folds crossvalidation was applied throughout."], "CC1077": ["#CITATION_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. #CITATION_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.", "#CITATION_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. #CITATION_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).", "#CITATION_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. #CITATION_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "#CITATION_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features."], "CC1078": ["Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice.", "A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice.", "Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds.", "However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice.", "A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds.", "Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.", "In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice.", "However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds.", "A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.", "In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds.", "However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen \"s kappa ( #CITATION_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2."], "CC1079": ["Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 )", "Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 )", "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 )", "Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 )", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.", "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #CITATION_TAG ; Morency et al. , 2009 ) Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information."], "CC1084": ["Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen\"s kappa figures over 60 are good while those over are excellent ( #CITATION_TAG ) . Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element.", "For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained. Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen\"s kappa figures over 60 are good while those over are excellent ( #CITATION_TAG ) . Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element."], "CC1085": ["These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #CITATION_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.", "The figures obtained are given in Table 3. These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #CITATION_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.", "In the case of gestures we also measured agreement on gesture segmentation. The figures obtained are given in Table 3. These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #CITATION_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme."], "CC1086": ["The results , which partly confirm those obtained on a smaller dataset in #CITATION_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions . The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.", "We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. The results , which partly confirm those obtained on a smaller dataset in #CITATION_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions . The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.", "Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. The results , which partly confirm those obtained on a smaller dataset in #CITATION_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions . The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category."], "CC1088": ["Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.", "For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.", "Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.", "In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.", "For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.", "Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.", "As already mentioned, all words in DanPASS are phonetically and prosodically annotated. In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.", "In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.", "For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.", "Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant. Finally, the two turn management categories Turn-Take and TurnElicit were also coded.", "As already mentioned, all words in DanPASS are phonetically and prosodically annotated. In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.", "In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.", "For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant. Finally, the two turn management categories Turn-Take and TurnElicit were also coded.", "As already mentioned, all words in DanPASS are phonetically and prosodically annotated. In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.", "In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #CITATION_TAG ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant. Finally, the two turn management categories Turn-Take and TurnElicit were also coded."], "CC1089": ["The Praat tool was used ( #CITATION_TAG ) ."], "CC1092": ["The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #CITATION_TAG ) generation system to produce the appropriate text . Templates are used to generate some stock phrases such as \"When you are ready, go on to the next slide."], "CC1093": ["Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #CITATION_TAG ) . Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.", "The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn\"t explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010). Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #CITATION_TAG ) . Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail."], "CC1094": ["Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #CITATION_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #CITATION_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #CITATION_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state."], "CC1096": ["Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #CITATION_TAG ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #CITATION_TAG ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #CITATION_TAG ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state."], "CC1097": ["The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n\"t explicitly explain the reason why different terminology is needed ( #CITATION_TAG ) . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).", "In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n\"t explicitly explain the reason why different terminology is needed ( #CITATION_TAG ) . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).", "The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n\"t explicitly explain the reason why different terminology is needed ( #CITATION_TAG ) . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006). Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail."], "CC1100": ["At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #CITATION_TAG ) . Since the system accepts unrestricted input, interpretation errors are unavoidable.", "The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #CITATION_TAG ) . Since the system accepts unrestricted input, interpretation errors are unavoidable.", "At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #CITATION_TAG ) . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue.", "The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #CITATION_TAG ) . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue.", "At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #CITATION_TAG ) . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding.", "The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #CITATION_TAG ) . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding.", "At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #CITATION_TAG ) . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding. I don\\\"t know the word power."], "CC1101": ["The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them.", "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them.", "The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm.", "We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them.", "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm.", "The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution.", "We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm.", "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution.", "The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?", "We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution.", "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #CITATION_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?"], "CC1102": ["The BEETLE II system architecture is designed to overcome these limitations ( #CITATION_TAG )", "The BEETLE II system architecture is designed to overcome these limitations ( #CITATION_TAG ) It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.", "The BEETLE II system architecture is designed to overcome these limitations ( #CITATION_TAG ) It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically. This allows the system to consistently apply the same tutorial policy across a range of questions."], "CC1104": ["Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #CITATION_TAG ) . However, most existing systems use pre-authored tutor responses for addressing student errors.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #CITATION_TAG ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #CITATION_TAG ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state."], "CC1105": ["Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #CITATION_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #CITATION_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #CITATION_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state."], "CC1106": ["The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them.", "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them.", "The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm.", "We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them.", "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm.", "The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution.", "We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm.", "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution.", "The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?", "We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution.", "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #CITATION_TAGa ) to produce a domain-specific semantic representation of the student \"s output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?"], "CC1107": [], "CC1108": ["Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #CITATION_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #CITATION_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #CITATION_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state."], "CC1109": ["Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #CITATION_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #CITATION_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.", "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #CITATION_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state."], "CC1110": ["Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding.", "1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding.", "Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding. I don\\\"t know the word power.", "At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding.", "1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding. I don\\\"t know the word power.", "Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding. I don\\\"t know the word power. The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.", "The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding.", "At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding. I don\\\"t know the word power.", "1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding. I don\\\"t know the word power. The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.", "The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding. I don\\\"t know the word power.", "At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #CITATION_TAG ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I\\\"m sorry, I\\\"m having a problem understanding. I don\\\"t know the word power. The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers."], "CC1111": ["We use the TRIPS dialogue parser ( #CITATION_TAG ) to parse the utterances . The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.", "We use the TRIPS dialogue parser ( #CITATION_TAG ) to parse the utterances . The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student\"s output.", "We use the TRIPS dialogue parser ( #CITATION_TAG ) to parse the utterances . The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student\"s output. Utterance content is represented as a set of extracted objects and relations between them."], "CC1112": [], "CC1113": ["All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #CITATION_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference . Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.", "All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #CITATION_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference . Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information.", "All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #CITATION_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference . Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable."], "CC1114": ["They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #CITATION_TAG ) . There are several methods to build phrase tables.", "Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #CITATION_TAG ) . There are several methods to build phrase tables.", "They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #CITATION_TAG ) . There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.", "Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #CITATION_TAG ) . There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.", "They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #CITATION_TAG ) . There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1", "Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #CITATION_TAG ) . There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1", "They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #CITATION_TAG ) . There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level."], "CC1116": ["All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #CITATION_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference . Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.", "All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #CITATION_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference . Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information.", "All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #CITATION_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference . Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable."], "CC1117": ["Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce.", "It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce.", "Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce. Translation jobs return one Spanish version for each hypothesis.", "The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce.", "It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce. Translation jobs return one Spanish version for each hypothesis.", "Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.", "The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce. Translation jobs return one Spanish version for each hypothesis.", "It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.", "Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.", "The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.", "It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #CITATION_TAG ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk\"s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job."], "CC1118": ["Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #CITATION_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T . The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level."], "CC1120": ["We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).", "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).", "We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.", "The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).", "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.", "We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.", "There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).", "The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.", "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.", "We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www.", "There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.", "The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.", "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www.", "There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.", "The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #CITATION_TAG ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www."], "CC1121": ["They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #CITATION_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).", "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #CITATION_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).", "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #CITATION_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #CITATION_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #CITATION_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases."], "CC1122": ["Some previous works ( #CITATION_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .", "As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works ( #CITATION_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words ."], "CC1123": [], "CC1124": ["They proved to be useful in a number of NLP applications such as natural language generation ( #CITATION_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).", "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #CITATION_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).", "They proved to be useful in a number of NLP applications such as natural language generation ( #CITATION_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #CITATION_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "They proved to be useful in a number of NLP applications such as natural language generation ( #CITATION_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases."], "CC1125": ["These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #CITATION_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.", "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #CITATION_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.", "These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #CITATION_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.", "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #CITATION_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.", "These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #CITATION_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.", "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #CITATION_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.", "These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #CITATION_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses."], "CC1126": ["Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE.", "However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE.", "Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.", "This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE.", "However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.", "Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE. However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.", "In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lex- ical matches between texts and hypotheses in different languages. This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE.", "This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.", "However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE. However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.", "Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE. However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage. In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g. bilingual dictionaries).", "In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lex- ical matches between texts and hypotheses in different languages. This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.", "This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE. However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.", "However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE. However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage. In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g. bilingual dictionaries).", "In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lex- ical matches between texts and hypotheses in different languages. This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE. However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.", "This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #CITATION_TAG ) ) have been created for several languages , with different degrees of coverage . As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet\ufffds synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE. However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage. In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g. bilingual dictionaries)."], "CC1127": ["They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #CITATION_TAG ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).", "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #CITATION_TAG ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).", "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #CITATION_TAG ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #CITATION_TAG ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #CITATION_TAG ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases."], "CC1128": ["One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #CITATION_TAG ) . With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #CITATION_TAG ) . With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #CITATION_TAG ) . With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.", "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #CITATION_TAG ) . With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #CITATION_TAG ) . With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases."], "CC1129": ["These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #CITATION_TAG ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.", "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009). These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #CITATION_TAG ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.", "These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #CITATION_TAG ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.", "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009). These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #CITATION_TAG ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.", "These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #CITATION_TAG ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.", "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009). These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #CITATION_TAG ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.", "These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #CITATION_TAG ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses."], "CC1130": ["Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #CITATION_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T . The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level."], "CC1131": ["These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #CITATION_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.", "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #CITATION_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.", "These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #CITATION_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.", "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #CITATION_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.", "These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #CITATION_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.", "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #CITATION_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.", "These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #CITATION_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses."], "CC1132": ["After the extraction , pruning techniques ( #CITATION_TAG ) can be applied to increase the precision of the extracted paraphrases .", "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction , pruning techniques ( #CITATION_TAG ) can be applied to increase the precision of the extracted paraphrases .", "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction , pruning techniques ( #CITATION_TAG ) can be applied to increase the precision of the extracted paraphrases ."], "CC1133": ["They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #CITATION_TAG ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).", "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #CITATION_TAG ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).", "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #CITATION_TAG ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #CITATION_TAG ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.", "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #CITATION_TAG ) , and TE ( Dinu and Wang , 2009 ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases."], "CC1134": ["Some previous works ( Bannard and Callison-Burch , 2005 ; #CITATION_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .", "As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works ( Bannard and Callison-Burch , 2005 ; #CITATION_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words ."], "CC1135": [], "CC1137": ["We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #CITATION_TAG ) to measure the relatedness between words in the dataset . Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).", "Wikipedia (WIKI). We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #CITATION_TAG ) to measure the relatedness between words in the dataset . Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).", "We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #CITATION_TAG ) to measure the relatedness between words in the dataset . Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009). In this way we obtained 13760 word pairs."], "CC1138": ["Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%).", "This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%).", "Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).", "First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%).", "This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).", "Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.", "The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations. First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%).", "First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).", "This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.", "Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation. Finally, it\"s worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%).", "The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations. First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).", "First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.", "This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation. Finally, it\"s worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%).", "The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations. First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.", "First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #CITATION_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation. Finally, it\"s worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%)."], "CC1139": ["Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.", "We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.", "Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.", "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.", "We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.", "Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www.", "The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.", "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.", "We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www.", "Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www. statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.", "The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.", "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www.", "We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www. statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.", "The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www.", "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #CITATION_TAG ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www. statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase."], "CC1140": ["Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #CITATION_TAG ) . These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).", "Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #CITATION_TAG ) . These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.", "Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #CITATION_TAG ) . These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates."], "CC1141": ["Such questions are typically answered by designing appropriate priming experiments ( #CITATION_TAG ) or other lexical decision tasks . The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.", "Such questions are typically answered by designing appropriate priming experiments ( #CITATION_TAG ) or other lexical decision tasks . The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain. (See Sec."], "CC1143": ["Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.", "Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.", "Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.", "Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.", "Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #CITATION_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models."], "CC1144": ["We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.", "In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.", "We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded.", "This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.", "In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded.", "We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.", "In other words non CV verbs can directly interpret from their constituent verbs. This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.", "This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded.", "In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.", "We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs. This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.", "In other words non CV verbs can directly interpret from their constituent verbs. This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded.", "This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.", "In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs. This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.", "In other words non CV verbs can directly interpret from their constituent verbs. This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.", "This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #CITATION_TAG ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs. This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed."], "CC1145": ["#CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".", "Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".", "#CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".", "Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.", "#CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.", "Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "#CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.", "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.", "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #CITATION_TAG tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible."], "CC1146": ["Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #CITATION_TAG ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.", "(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #CITATION_TAG ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.", "Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #CITATION_TAG ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole. If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.", "It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations. (Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #CITATION_TAG ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.", "(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #CITATION_TAG ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole. If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts."], "CC1147": ["The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.", "Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.", "The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).", "Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages. Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.", "Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).", "The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978). Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.", "Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages. Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).", "Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978). Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.", "The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978). Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988).", "Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages. Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978). Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.", "Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #CITATION_TAG ; Butterworth , 1983 ) . On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978). Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988)."], "CC1149": ["#CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".", "#CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.", "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.", "#CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "#CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.", "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.", "Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #CITATION_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . Bashir (1993) tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible."], "CC1150": ["With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #CITATION_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla", "With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #CITATION_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated.", "With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #CITATION_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated. These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes."], "CC1151": ["We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #CITATION_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words . Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).", "We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #CITATION_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words . Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming). The subjects were asked to make a lexical decision whether the given target is a valid word in that language.", "We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #CITATION_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words . Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming). The subjects were asked to make a lexical decision whether the given target is a valid word in that language. The same target word is again probed but with a different audio or visual probe called the control word."], "CC1152": ["There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #CITATION_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.", "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #CITATION_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.", "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #CITATION_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004)."], "CC1154": ["Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #CITATION_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .", "Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications. Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #CITATION_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency ."], "CC1155": ["On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #CITATION_TAG ) . Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.", "Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #CITATION_TAG ) . Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.", "However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #CITATION_TAG ) . Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages."], "CC1156": ["For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #CITATION_TAG ) .", "Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #CITATION_TAG ) .", "The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978). Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #CITATION_TAG ) ."], "CC1158": ["Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #CITATION_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.", "(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #CITATION_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.", "Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #CITATION_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole. If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.", "It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations. (Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #CITATION_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.", "(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #CITATION_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose . Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole. If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts."], "CC1159": ["There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #CITATION_TAG ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.", "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #CITATION_TAG ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.", "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #CITATION_TAG ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004)."], "CC1160": ["Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #CITATION_TAGc ) . The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest.", "to be confused by collocates\" (Dagan et al., 1993). Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #CITATION_TAGc ) . The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest.", "Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #CITATION_TAGc ) . The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:", "Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (Dagan et al., 1993). Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #CITATION_TAGc ) . The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest.", "to be confused by collocates\" (Dagan et al., 1993). Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #CITATION_TAGc ) . The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:", "The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (Dagan et al., 1993). Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #CITATION_TAGc ) . The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest.", "Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (Dagan et al., 1993). Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #CITATION_TAGc ) . The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:"], "CC1161": ["Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #CITATION_TAG ; Brown et al. , 1993a ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.", "Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #CITATION_TAG ; Brown et al. , 1993a ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby."], "CC1162": ["to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).", "Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).", "to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest.", "The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).", "Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest.", "to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:", "Then vk and uk+l will also co-occur more often than expected by chance. The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).", "The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest.", "Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:", "Then vk and uk+l will also co-occur more often than expected by chance. The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest.", "The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates \"\" ( #CITATION_TAG ) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v\"s translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:"], "CC1164": ["This imbalance foils thresholding strategies , clever as they might be ( #CITATION_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete.", "The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( #CITATION_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete.", "Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( #CITATION_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete."], "CC1165": ["Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #CITATION_TAG ), computer- assisted language learning, corpus linguistics (Melby.", "However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #CITATION_TAG ), computer- assisted language learning, corpus linguistics (Melby.", "Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #CITATION_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).", "Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a). However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #CITATION_TAG ), computer- assisted language learning, corpus linguistics (Melby.", "However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #CITATION_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)."], "CC1166": ["Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #CITATION_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby.", "However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #CITATION_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby.", "Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #CITATION_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).", "Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a). However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #CITATION_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby.", "However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply. Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #CITATION_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)."], "CC1167": ["2We could just as easily use other symmetric \"association\" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #CITATION_TAG ) . co-occur is called a direct association.", "2We could just as easily use other symmetric \"association\" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #CITATION_TAG ) . co-occur is called a direct association. Now, suppose that uk and Uk+z often co-occur within their language.", "2We could just as easily use other symmetric \"association\" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #CITATION_TAG ) . co-occur is called a direct association. Now, suppose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance."], "CC1168": ["Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #CITATION_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.", "Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #CITATION_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby."], "CC1169": ["The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #CITATION_TAGa ) . Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.", "Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #CITATION_TAGa ) . Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.", "The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #CITATION_TAGa ) . Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti. The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.", "A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #CITATION_TAGa ) . Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.", "Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #CITATION_TAGa ) . Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti. The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.", "Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #CITATION_TAGa ) . Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.", "A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #CITATION_TAGa ) . Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti. The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;."], "CC1170": [], "CC1171": ["It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #CITATION_TAG ; Chen , 1996 ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2.", "This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #CITATION_TAG ; Chen , 1996 ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2.", "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #CITATION_TAG ; Chen , 1996 ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.", "Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #CITATION_TAG ; Chen , 1996 ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2.", "This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #CITATION_TAG ; Chen , 1996 ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.", "1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #CITATION_TAG ; Chen , 1996 ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2.", "Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #CITATION_TAG ; Chen , 1996 ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."], "CC1172": ["#CITATION_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.", "The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports. #CITATION_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.", "#CITATION_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior.", "The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports. #CITATION_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior.", "#CITATION_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al.\"s Model 2 on 74 million words of the Canadian Hansards.", "The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports. #CITATION_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al.\"s Model 2 on 74 million words of the Canadian Hansards.", "#CITATION_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al.\"s Model 2 on 74 million words of the Canadian Hansards. These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set."], "CC1173": ["Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #CITATION_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).", "Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #CITATION_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other.", "Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #CITATION_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways."], "CC1175": ["Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a `` word \"\" to include multi-word lexical units ( #CITATION_TAG ) . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.", "Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a `` word \"\" to include multi-word lexical units ( #CITATION_TAG ) . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.", "Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a `` word \"\" to include multi-word lexical units ( #CITATION_TAG ) . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena."], "CC1176": ["For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( \u00e2\\x80\\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #CITATION_TAG ) 2 . When the L(u, v) are re-estimated, the model\"s hidden parameters come into play.", "L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( \u00e2\\x80\\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #CITATION_TAG ) 2 . When the L(u, v) are re-estimated, the model\"s hidden parameters come into play.", "The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( \u00e2\\x80\\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #CITATION_TAG ) 2 . When the L(u, v) are re-estimated, the model\"s hidden parameters come into play."], "CC1177": ["With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #CITATION_TAG ) . A bitext comprises a pair of texts in two languages, where each text is a translation of the other.", "With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #CITATION_TAG ) . A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways.", "With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #CITATION_TAG ) . A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a)."], "CC1178": ["It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #CITATION_TAG ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2.", "This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #CITATION_TAG ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2.", "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #CITATION_TAG ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.", "Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #CITATION_TAG ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2.", "This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #CITATION_TAG ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.", "1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #CITATION_TAG ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2.", "Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #CITATION_TAG ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."], "CC1179": ["By using the EM algorithm ( #CITATION_TAG ) , they can guarantee convergence towards the globally optimum parameter set . In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.", "One advantage that Brown et al.\"s Model i has over our word-to-word model is that their objective function has no local maxima. By using the EM algorithm ( #CITATION_TAG ) , they can guarantee convergence towards the globally optimum parameter set . In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.", "By using the EM algorithm ( #CITATION_TAG ) , they can guarantee convergence towards the globally optimum parameter set . In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion. We have adopted the simple heuristic that the model \"has converged\" when this probability stops increasing."], "CC1180": ["Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #CITATION_TAG ) . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (Smadja, 1992).", "Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #CITATION_TAG ) . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (Smadja, 1992).", "Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #CITATION_TAG ) . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.", "Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #CITATION_TAG ) . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.", "Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #CITATION_TAG ) . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena."], "CC1181": ["For example , frequent words are translated less consistently than rare words ( #CITATION_TAG ) .", "More accurate models can be induced by taking into account various features of the linked tokens. For example , frequent words are translated less consistently than rare words ( #CITATION_TAG ) ."], "CC1183": ["2We could just as easily use other symmetric `` association \"\" measures , such as 02 ( #CITATION_TAG ) or the Dice coefficient ( Smadja , 1992 ) . co-occur is called a direct association.", "2We could just as easily use other symmetric `` association \"\" measures , such as 02 ( #CITATION_TAG ) or the Dice coefficient ( Smadja , 1992 ) . co-occur is called a direct association. Now, suppose that uk and Uk+z often co-occur within their language.", "2We could just as easily use other symmetric `` association \"\" measures , such as 02 ( #CITATION_TAG ) or the Dice coefficient ( Smadja , 1992 ) . co-occur is called a direct association. Now, suppose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance."], "CC1184": ["Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #CITATION_TAGa ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.", "Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #CITATION_TAGa ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby."], "CC1185": ["This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #CITATION_TAG ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete.", "The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #CITATION_TAG ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete.", "Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #CITATION_TAG ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete."], "CC1187": ["The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.", "For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.", "The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.", "Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.", "For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.", "The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.", "Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.", "Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.", "For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.", "The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words. Therefore, we ignored English words that were linked to nothing.", "Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.", "Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.", "For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words. Therefore, we ignored English words that were linked to nothing.", "Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.", "Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #CITATION_TAG ) , who trained Brown et al. \"s Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words. Therefore, we ignored English words that were linked to nothing."], "CC1188": ["11 11 From ( #CITATION_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags . Thus, to be convenient, we only conduct experiments with the SAMT system.", "We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 11 From ( #CITATION_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags . Thus, to be convenient, we only conduct experiments with the SAMT system.", "We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 11 From ( #CITATION_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags . Thus, to be convenient, we only conduct experiments with the SAMT system."], "CC1189": ["Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment", "In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment", "Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.", "For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment", "In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.", "Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics.", "Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment", "For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.", "In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics.", "Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.", "For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #CITATION_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics."], "CC1190": ["The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.", "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.", "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.", "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.", "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.", "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.", "Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.", "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.", "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.", "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.", "Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.", "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.", "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.", "Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.", "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #CITATION_TAG ) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment."], "CC1191": ["#CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).", "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).", "#CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.", "Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).", "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.", "#CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.", "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).", "Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.", "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.", "#CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.", "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.", "Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.", "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.", "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.", "Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #CITATION_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG."], "CC1192": ["The system is implemented based on ( Galley et al. , 2006 ) and ( #CITATION_TAG ) . In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side."], "CC1193": ["Our previous work ( #CITATION_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models", "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work ( #CITATION_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models", "Our previous work ( #CITATION_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.", "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work ( #CITATION_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.", "Our previous work ( #CITATION_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.", "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work ( #CITATION_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.", "Our previous work ( #CITATION_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007)."], "CC1194": ["Then , we binarize the English parse trees using the head binarization approach ( #CITATION_TAG ) and use the resulting binary parse trees to build another s2t system ."], "CC1195": ["Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #CITATION_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) ."], "CC1196": ["Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #CITATION_TAG ; Zhang et al. , 2011b ) ."], "CC1197": ["#CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.", "#CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.", "Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.", "#CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.", "Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "#CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.", "Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #CITATION_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories."], "CC1198": ["Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #CITATION_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :"], "CC1200": ["Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #CITATION_TAG ) . Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.", "Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #CITATION_TAG ) . Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.", "Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #CITATION_TAG ) . Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler."], "CC1201": ["#CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.", "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.", "#CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.", "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "#CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "#CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #CITATION_TAG re-trained the linguistic parsers bilingually based on word alignment . Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes."], "CC1202": ["The statistical significance test is performed by the re-sampling approach ( #CITATION_TAG ) .", "The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach ( #CITATION_TAG ) .", "Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach ( #CITATION_TAG ) ."], "CC1203": ["The system is implemented based on ( #CITATION_TAG ) and ( Marcu et al. 2006 ) . In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side."], "CC1205": ["#CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment", "#CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.", "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.", "#CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.", "Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment", "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.", "#CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.", "Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.", "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.", "Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.", "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #CITATION_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment."], "CC1206": ["9 We only use the minimal GHKM rules ( #CITATION_TAG ) here to reduce the complexity of the sampler .", "Our final experiments verify this point and we will conduct a much detailed analysis in future. 9 We only use the minimal GHKM rules ( #CITATION_TAG ) here to reduce the complexity of the sampler .", "Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs. Our final experiments verify this point and we will conduct a much detailed analysis in future. 9 We only use the minimal GHKM rules ( #CITATION_TAG ) here to reduce the complexity of the sampler ."], "CC1207": ["In the system , we extract both the minimal GHKM rules ( #CITATION_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .", "The system is implemented based on (Galley et al., 2006) and ). In the system , we extract both the minimal GHKM rules ( #CITATION_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side ."], "CC1208": ["Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.", "Otherwise, we change its state to the right state \u020c , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of \u020c would define two different U-trees. Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.", "Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node\"s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node.", "For example, in Figure 3(a), the s-node is currently in the left VWDWH\u020c :HVDPSOHWKH\u020cRIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRI\u020cLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state. Otherwise, we change its state to the right state \u020c , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of \u020c would define two different U-trees. Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.", "Otherwise, we change its state to the right state \u020c , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of \u020c would define two different U-trees. Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node\"s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node.", "Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node\"s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node. For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (\u020c ) defines only one rule: Using these STSG rules, the two derivations are evaluated as follows (We use the value of \u020c to denote the corresponding STSG derivation):", "Our first Gibbs operator, Rotate, just works by sampling value of the \u020cparameters, one at a time, and changing the U-tree accordingly. For example, in Figure 3(a), the s-node is currently in the left VWDWH\u020c :HVDPSOHWKH\u020cRIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRI\u020cLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state. Otherwise, we change its state to the right state \u020c , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of \u020c would define two different U-trees. Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.", "For example, in Figure 3(a), the s-node is currently in the left VWDWH\u020c :HVDPSOHWKH\u020cRIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRI\u020cLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state. Otherwise, we change its state to the right state \u020c , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of \u020c would define two different U-trees. Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node\"s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node.", "Otherwise, we change its state to the right state \u020c , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of \u020c would define two different U-trees. Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node\"s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node. For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (\u020c ) defines only one rule: Using these STSG rules, the two derivations are evaluated as follows (We use the value of \u020c to denote the corresponding STSG derivation):", "Our first Gibbs operator, Rotate, just works by sampling value of the \u020cparameters, one at a time, and changing the U-tree accordingly. For example, in Figure 3(a), the s-node is currently in the left VWDWH\u020c :HVDPSOHWKH\u020cRIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRI\u020cLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state. Otherwise, we change its state to the right state \u020c , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of \u020c would define two different U-trees. Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node\"s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node.", "For example, in Figure 3(a), the s-node is currently in the left VWDWH\u020c :HVDPSOHWKH\u020cRIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRI\u020cLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state. Otherwise, we change its state to the right state \u020c , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of \u020c would define two different U-trees. Using the GHKM algorithm ( #CITATION_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment . Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node\"s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node. For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (\u020c ) defines only one rule: Using these STSG rules, the two derivations are evaluated as follows (We use the value of \u020c to denote the corresponding STSG derivation):"], "CC1209": ["Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #CITATION_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) ."], "CC1210": ["Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #CITATION_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) ."], "CC1211": [], "CC1213": ["This is because the binary structure has been verified to be very effective for tree-based translation ( #CITATION_TAG ; Zhang et al. , 2011a ) .", "Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation ( #CITATION_TAG ; Zhang et al. , 2011a ) .", "To generate frag,  used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation ( #CITATION_TAG ; Zhang et al. , 2011a ) ."], "CC1214": ["Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #CITATION_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) ."], "CC1215": ["This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #CITATION_TAG ) .", "2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs. This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #CITATION_TAG ) ."], "CC1216": ["Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment", "Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.", "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.", "Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.", "Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment", "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.", "Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.", "Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.", "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.", "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.", "Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.", "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein ( 2008 ) and #CITATION_TAG focused on joint parsing and alignment They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment."], "CC1217": ["#CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "#CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "#CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.", "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "#CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.", "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.", "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. #CITATION_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."], "CC1218": ["Inspired by ( Blunsom et al. , 2009 ) and ( #CITATION_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."], "CC1219": ["Inspired by ( #CITATION_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."], "CC1221": ["#CITATION_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #CITATION_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "#CITATION_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.", "Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #CITATION_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #CITATION_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.", "Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #CITATION_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.", "Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #CITATION_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."], "CC1222": ["This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses", "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses", "This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.", "As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses", "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.", "This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval.", "word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses", "As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.", "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval.", "This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation module to react immediately to user interruptions while the system is speaking.", "word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.", "As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval.", "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation module to react immediately to user interruptions while the system is speaking.", "word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval.", "As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992). This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #CITATION_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation module to react immediately to user interruptions while the system is speaking."], "CC1223": [], "CC1224": ["There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #CITATION_TAG ; Chu-Carroll , 1999 ) . Incorporating such techniques would deo crease the system developer workload.", "There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #CITATION_TAG ; Chu-Carroll , 1999 ) . Incorporating such techniques would deo crease the system developer workload. However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well."], "CC1225": ["They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #CITATION_TAG ."], "CC1226": ["Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG )", "As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG )", "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG ) This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.", "word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG )", "As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG ) This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.", "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG ) This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.", "word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG ) This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.", "As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG ) This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.", "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG ) This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval.", "word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG ) This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.", "As the recogn/fion engine, either VoiceRex, developed by NTI\" (Noda et al., 1998), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #CITATION_TAG ) This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval."], "CC1227": [], "CC1228": [], "CC1229": ["WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #CITATION_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.", "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #CITATION_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2.", "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #CITATION_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine."], "CC1230": [], "CC1231": ["For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description.", "Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description.", "For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).", "Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description.", "Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).", "For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT.", "For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description.", "Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).", "Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT.", "For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT.", "For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).", "Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT.", "Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT.", "For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT.", "Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #CITATION_TAG ; Nakano and Shimazu , 1999 ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT."], "CC1232": ["Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #CITATION_TAGa ) . Thus the system can respond immediately after user pauses when the user has the initiative."], "CC1233": ["WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #CITATION_TAG ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.", "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #CITATION_TAG ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2.", "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #CITATION_TAG ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine."], "CC1235": ["Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #CITATION_TAG ) . When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.", "The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules. Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #CITATION_TAG ) . When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.", "Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #CITATION_TAG ) . When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.", "The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules. Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #CITATION_TAG ) . When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.", "Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #CITATION_TAG ) . When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences. Two kinds of sentenees can be considered; domain-related ones that express the user\"s intention about the reser-vafion and dialogue-related ones that express the user\"s attitude with respect to the progress of the dialogue, such as confirmation and denial.", "The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules. Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #CITATION_TAG ) . When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences. Two kinds of sentenees can be considered; domain-related ones that express the user\"s intention about the reser-vafion and dialogue-related ones that express the user\"s attitude with respect to the progress of the dialogue, such as confirmation and denial.", "Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #CITATION_TAG ) . When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences. Two kinds of sentenees can be considered; domain-related ones that express the user\"s intention about the reser-vafion and dialogue-related ones that express the user\"s attitude with respect to the progress of the dialogue, such as confirmation and denial. Considering the meeting room reservation system, examples of domain-related sentences are \"I need to book Room 2 on Wednesday I need to book Room 2 and \"Room 2\" and dialogue-related ones are \"yes no and \"Okay\"."], "CC1236": ["The priorities are used for disambiguating interpretation in the incremental understanding method ( #CITATION_TAGb ) . When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence\"s semantic representation is the command for updating the dialogue state.", "It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper. The priorities are used for disambiguating interpretation in the incremental understanding method ( #CITATION_TAGb ) . When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence\"s semantic representation is the command for updating the dialogue state.", "The priorities are used for disambiguating interpretation in the incremental understanding method ( #CITATION_TAGb ) . When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence\"s semantic representation is the command for updating the dialogue state. The command is one of the following:", "These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied. It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper. The priorities are used for disambiguating interpretation in the incremental understanding method ( #CITATION_TAGb ) . When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence\"s semantic representation is the command for updating the dialogue state.", "It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper. The priorities are used for disambiguating interpretation in the incremental understanding method ( #CITATION_TAGb ) . When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence\"s semantic representation is the command for updating the dialogue state. The command is one of the following:"], "CC1237": ["To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #CITATION_TAG ) . One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.", "To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #CITATION_TAG ) . One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.", "To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #CITATION_TAG ) . One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model. It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain. However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics."], "CC1239": ["(Davis and Ogden, 1997; #CITATION_TAG ; Hull and Grefenstette, 1996).", "Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #CITATION_TAG ; Hull and Grefenstette, 1996).", "(Davis and Ogden, 1997; #CITATION_TAG ; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.", "Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #CITATION_TAG ; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.", "(Davis and Ogden, 1997; #CITATION_TAG ; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.", "Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #CITATION_TAG ; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.", "(Davis and Ogden, 1997; #CITATION_TAG ; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997. Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR."], "CC1240": ["There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms.", "This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms.", "There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.", "A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms.", "This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.", "There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.", "A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.", "This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.", "There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities.", "A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.", "This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #CITATION_TAG ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities."], "CC1241": ["The one-sided t-test ( #CITATION_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .", "Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries. The one-sided t-test ( #CITATION_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant ."], "CC1242": ["The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g. latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #CITATION_TAG )"], "CC1243": [], "CC1244": ["#CITATION_TAG studied the issue of disambiguation for mono-lingual M.", "Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997. #CITATION_TAG studied the issue of disambiguation for mono-lingual M.", "While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997. #CITATION_TAG studied the issue of disambiguation for mono-lingual M."], "CC1245": ["Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #CITATION_TAG . Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.", "While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #CITATION_TAG . Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.", "(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996). While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #CITATION_TAG . Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR."], "CC1246": ["One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #CITATION_TAG ) . For most languages, there are no MT systems at all.", "Many approaches to cross-lingual IR have been published. One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #CITATION_TAG ) . For most languages, there are no MT systems at all.", "One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #CITATION_TAG ) . For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived."], "CC1248": ["However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #CITATION_TAG ) .", "That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl. However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #CITATION_TAG ) .", "Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query. That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl. However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #CITATION_TAG ) ."], "CC1249": ["A cooccurrence based stemmer ( #CITATION_TAG ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.", "Each English word has around 1.5 translations on average. A cooccurrence based stemmer ( #CITATION_TAG ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.", "A cooccurrence based stemmer ( #CITATION_TAG ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon. This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.", "es) containing around 22,000 English words (16,000 English stems) and processed it similarly. Each English word has around 1.5 translations on average. A cooccurrence based stemmer ( #CITATION_TAG ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.", "Each English word has around 1.5 translations on average. A cooccurrence based stemmer ( #CITATION_TAG ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon. This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.", "arrakis. es) containing around 22,000 English words (16,000 English stems) and processed it similarly. Each English word has around 1.5 translations on average. A cooccurrence based stemmer ( #CITATION_TAG ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.", "es) containing around 22,000 English words (16,000 English stems) and processed it similarly. Each English word has around 1.5 translations on average. A cooccurrence based stemmer ( #CITATION_TAG ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon. This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon."], "CC1250": ["Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #CITATION_TAG ; Miller et al , 1999"], "CC1252": [], "CC1254": ["Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #CITATION_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #CITATION_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #CITATION_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."], "CC1255": ["The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #CITATION_TAG ) . In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.", "The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #CITATION_TAG ) . In this case, a full parse tree is represented in a flat form, producing a representation as in the example above. The goal in this case is therefore to accurately predict a collection of \u00a2 \u00a3 \u00a2 different types of phrases.", "The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #CITATION_TAG ) . In this case, a full parse tree is represented in a flat form, producing a representation as in the example above. The goal in this case is therefore to accurately predict a collection of \u00a2 \u00a3 \u00a2 different types of phrases. The chunk types are based on the syntactic category part of the bracket label in the Treebank."], "CC1256": ["Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #CITATION_TAG ; Abney , 1991 ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases.", "Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #CITATION_TAG ; Abney , 1991 ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases.", "Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #CITATION_TAG ; Abney , 1991 ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.", "Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #CITATION_TAG ; Abney , 1991 ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.", "Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #CITATION_TAG ; Abney , 1991 ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000)."], "CC1257": ["Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #CITATION_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #CITATION_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #CITATION_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."], "CC1258": ["However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #CITATION_TAG ; Ratnaparkhi , 1997 ) ."], "CC1259": ["For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #CITATION_TAG ) -- one of the most accurate full parsers around", "We perform our comparison using two state-ofthe-art parsers. For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #CITATION_TAG ) -- one of the most accurate full parsers around", "For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #CITATION_TAG ) -- one of the most accurate full parsers around It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.", "We perform our comparison using two state-ofthe-art parsers. For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #CITATION_TAG ) -- one of the most accurate full parsers around It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.", "For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #CITATION_TAG ) -- one of the most accurate full parsers around It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.", "We perform our comparison using two state-ofthe-art parsers. For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #CITATION_TAG ) -- one of the most accurate full parsers around It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.", "For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #CITATION_TAG ) -- one of the most accurate full parsers around It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output."], "CC1260": ["However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #CITATION_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) ."], "CC1261": ["SNoW ( #CITATION_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example", "The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999). SNoW ( #CITATION_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example", "SNoW ( #CITATION_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.", "The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999). SNoW ( #CITATION_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.", "SNoW ( #CITATION_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.", "The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999). SNoW ( #CITATION_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.", "SNoW ( #CITATION_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference."], "CC1262": ["Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #CITATION_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #CITATION_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #CITATION_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."], "CC1263": ["Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #CITATION_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #CITATION_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #CITATION_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."], "CC1264": ["Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #CITATION_TAG ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases.", "Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #CITATION_TAG ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases.", "Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #CITATION_TAG ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.", "Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #CITATION_TAG ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.", "Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #CITATION_TAG ; Greffenstette , 1993 ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000)."], "CC1265": ["Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #CITATION_TAG ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases.", "Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #CITATION_TAG ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases.", "Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #CITATION_TAG ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.", "Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #CITATION_TAG ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.", "Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #CITATION_TAG ) . A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000)."], "CC1266": ["First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #CITATION_TAG ) . Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.", "First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #CITATION_TAG ) . Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally. If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.", "First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #CITATION_TAG ) . Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally. If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis. This can be augmented later if more information is available."], "CC1271": ["Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #CITATION_TAG ) terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.", "We start by reporting the results in which we compare the full parser and the shallow parser on the \"clean\" WSJ data. Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #CITATION_TAG ) terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.", "Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #CITATION_TAG ) terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser. Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 . Here, again, the shallow parser exhibits significantly better performance.", "We start by reporting the results in which we compare the full parser and the shallow parser on the \"clean\" WSJ data. Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #CITATION_TAG ) terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser. Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 . Here, again, the shallow parser exhibits significantly better performance.", "Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #CITATION_TAG ) terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser. Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 . Here, again, the shallow parser exhibits significantly better performance. Table 3 shows the results of extracting atomic phrases."], "CC1272": ["Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #CITATION_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #CITATION_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #CITATION_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."], "CC1273": [], "CC1274": [], "CC1276": ["Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #CITATION_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #CITATION_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #CITATION_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."], "CC1277": ["The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #CITATION_TAG ) . SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.", "The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #CITATION_TAG ) . SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.", "The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #CITATION_TAG ) . SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes."], "CC1278": ["SNoW ( Carleson et al. , 1999 ; #CITATION_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example", "The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999). SNoW ( Carleson et al. , 1999 ; #CITATION_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example", "SNoW ( Carleson et al. , 1999 ; #CITATION_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.", "The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999). SNoW ( Carleson et al. , 1999 ; #CITATION_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.", "SNoW ( Carleson et al. , 1999 ; #CITATION_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.", "The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999). SNoW ( Carleson et al. , 1999 ; #CITATION_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.", "SNoW ( Carleson et al. , 1999 ; #CITATION_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference."], "CC1279": ["For example, the sentence \ufffdHe reckons the current account deficit will narrow to only $ 1.8 billion in September .\ufffd would be chunked as follows ( Tjong Kim #CITATION_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .", "A lot of recent work on shallow parsing has been influenced by Abney\ufffds work (Abney, 1991), who has suggested to \ufffdchunk\ufffd sentences to base level phrases. For example, the sentence \ufffdHe reckons the current account deficit will narrow to only $ 1.8 billion in September .\ufffd would be chunked as follows ( Tjong Kim #CITATION_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .", "Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993). A lot of recent work on shallow parsing has been influenced by Abney\ufffds work (Abney, 1991), who has suggested to \ufffdchunk\ufffd sentences to base level phrases. For example, the sentence \ufffdHe reckons the current account deficit will narrow to only $ 1.8 billion in September .\ufffd would be chunked as follows ( Tjong Kim #CITATION_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] ."], "CC1280": ["First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #CITATION_TAG ; Appelt et al. , 1993 ) . Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.", "First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #CITATION_TAG ; Appelt et al. , 1993 ) . Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally. If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.", "First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #CITATION_TAG ; Appelt et al. , 1993 ) . Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally. If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis. This can be augmented later if more information is available."], "CC1281": ["Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #CITATION_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #CITATION_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #CITATION_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."], "CC1283": ["Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #CITATION_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #CITATION_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .", "A lot of recent work on shallow parsing has been influenced by Abney\\\"s work (Abney, 1991), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September . would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #CITATION_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."], "CC1284": [], "CC1285": ["Training was done on the Penn Treebank ( #CITATION_TAG ) Wall Street Journal data , sections 02-21 . To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations."], "CC1286": ["Tateisi et al. also translated LTAG into HPSG ( #CITATION_TAG ) . However, their method depended on translator\ufffds intuitive analy- sis of the original grammar.", "Tateisi et al. also translated LTAG into HPSG ( #CITATION_TAG ) . However, their method depended on translator\ufffds intuitive analy- sis of the original grammar. Thus the transla- tion was manual and grammar dependent.", "Tateisi et al. also translated LTAG into HPSG ( #CITATION_TAG ) . However, their method depended on translator\ufffds intuitive analy- sis of the original grammar. Thus the transla- tion was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars."], "CC1288": ["A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #CITATION_TAG )", "We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #CITATION_TAG )", "A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #CITATION_TAG ) This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #CITATION_TAG )", "We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #CITATION_TAG ) This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #CITATION_TAG ) This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.", "In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #CITATION_TAG ) This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #CITATION_TAG ) This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing."], "CC1289": ["TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.", "In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.", "TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.", "In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.", "Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.", "TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Another paper  describes the detailed analysis on the factor of the difference of parsing performance.", "Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.", "In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser ( #CITATION_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Another paper  describes the detailed analysis on the factor of the difference of parsing performance."], "CC1290": ["This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #CITATION_TAG ) by a method of grammar conversion . The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar . Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications."], "CC1292": ["ment ( #CITATION_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) . These works are re- stricted to each closed community, and the rela- tion between them is not well discussed."], "CC1294": [], "CC1295": ["In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #CITATION_TAG )", "There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts . Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000). In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #CITATION_TAG )", "In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #CITATION_TAG ) Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000)."], "CC1296": ["Our group has developed a wide-coverage HPSG grammar for Japanese ( #CITATION_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .", "In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994). Our group has developed a wide-coverage HPSG grammar for Japanese ( #CITATION_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) ."], "CC1297": ["Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.", "The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.", "Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.", "Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.", "The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.", "Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.", "However, their method depended on translator\"s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.", "Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.", "The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.", "However, their method depended on translator\"s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.", "Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( Kasper et al. , 1995 ; #CITATION_TAG ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages."], "CC1298": ["Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #CITATION_TAG ) .", "In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994). Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #CITATION_TAG ) ."], "CC1299": ["An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #CITATION_TAG ) . A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.", "An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #CITATION_TAG ) . A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category. An ID grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.", "An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #CITATION_TAG ) . A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category. An ID grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics. Figure 6 illustrates an example of bottom-up parsing with an HPSG grammar."], "CC1300": ["We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #CITATION_TAG ) , which is a large-scale FB-LTAG grammar . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser", "In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #CITATION_TAG ) , which is a large-scale FB-LTAG grammar . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser", "We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #CITATION_TAG ) , which is a large-scale FB-LTAG grammar . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #CITATION_TAG ) , which is a large-scale FB-LTAG grammar . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #CITATION_TAG ) , which is a large-scale FB-LTAG grammar . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing."], "CC1301": ["The XTAG group ( #CITATION_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars . Development of a large-scale French grammar (Abeill\u00e9 and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.", "Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001). The XTAG group ( #CITATION_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars . Development of a large-scale French grammar (Abeill\u00e9 and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.", "In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001). The XTAG group ( #CITATION_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars . Development of a large-scale French grammar (Abeill\u00e9 and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7."], "CC1303": ["In Table 2 , lem refers to the LTAG parser ( #CITATION_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) . TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).", "Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2 , lem refers to the LTAG parser ( #CITATION_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) . TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).", "In Table 2 , lem refers to the LTAG parser ( #CITATION_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) . TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2). Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.", "Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2 , lem refers to the LTAG parser ( #CITATION_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) . TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2). Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.", "In Table 2 , lem refers to the LTAG parser ( #CITATION_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) . TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2). Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2 , lem refers to the LTAG parser ( #CITATION_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) . TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2). Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.", "In Table 2 , lem refers to the LTAG parser ( #CITATION_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) . TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2). Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing."], "CC1304": ["FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.", "Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.", "FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).", "Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.", "Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).", "FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001). The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.", "In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ). Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.", "Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).", "Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001). The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.", "FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001). The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars. Development of a large-scale French grammar (Abeill\u00e9 and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.", "In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ). Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).", "Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001). The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.", "Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001). The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars. Development of a large-scale French grammar (Abeill\u00e9 and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.", "In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ). Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001). The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.", "Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled \u00dc onto an internal node of another tree with the same symbol \u00dc (Figure 4). FBLTAG ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001). The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars. Development of a large-scale French grammar (Abeill\u00e9 and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7."], "CC1305": ["There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts . Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #CITATION_TAG ) . In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994)."], "CC1306": ["There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #CITATION_TAG ; Makino et al. , 1998 ) . These works are re-stricted to each closed community, and the relation between them is not well discussed.", "Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #CITATION_TAG ; Makino et al. , 1998 ) . These works are re-stricted to each closed community, and the relation between them is not well discussed.", "There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #CITATION_TAG ; Makino et al. , 1998 ) . These works are re-stricted to each closed community, and the relation between them is not well discussed. Investigating the relation will be apparently valuable for both communities.", "Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #CITATION_TAG ; Makino et al. , 1998 ) . These works are re-stricted to each closed community, and the relation between them is not well discussed.", "Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #CITATION_TAG ; Makino et al. , 1998 ) . These works are re-stricted to each closed community, and the relation between them is not well discussed. Investigating the relation will be apparently valuable for both communities."], "CC1307": ["The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #CITATION_TAG )", "The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #CITATION_TAG ) We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.", "The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #CITATION_TAG ) We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words)."], "CC1308": ["Another paper ( #CITATION_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .", "We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Another paper ( #CITATION_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .", "This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Another paper ( #CITATION_TAG ) describes the detailed analysis on the factor of the difference of parsing performance ."], "CC1309": ["The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #CITATION_TAG ) 6 ( the average length is 6.32 words )", "We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #CITATION_TAG ) 6 ( the average length is 6.32 words )", "The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #CITATION_TAG ) 6 ( the average length is 6.32 words ) This result empirically attested the strong equivalence of our algorithm.", "The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;. We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #CITATION_TAG ) 6 ( the average length is 6.32 words )", "We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #CITATION_TAG ) 6 ( the average length is 6.32 words ) This result empirically attested the strong equivalence of our algorithm."], "CC1310": ["The RenTAL system is implemented in LiLFeS ( #CITATION_TAG ) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.", "The RenTAL system is implemented in LiLFeS ( #CITATION_TAG ) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;. We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.", "The RenTAL system is implemented in LiLFeS ( #CITATION_TAG ) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;. We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words)."], "CC1311": ["This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #CITATION_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion . The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar . Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications."], "CC1312": ["There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #CITATION_TAG ) . These works are re-stricted to each closed community, and the relation between them is not well discussed.", "Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #CITATION_TAG ) . These works are re-stricted to each closed community, and the relation between them is not well discussed.", "There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #CITATION_TAG ) . These works are re-stricted to each closed community, and the relation between them is not well discussed. Investigating the relation will be apparently valuable for both communities.", "Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #CITATION_TAG ) . These works are re-stricted to each closed community, and the relation between them is not well discussed.", "Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #CITATION_TAG ) . These works are re-stricted to each closed community, and the relation between them is not well discussed. Investigating the relation will be apparently valuable for both communities."], "CC1313": ["There have been many studies on parsing techniques ( #CITATION_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).", "Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques ( #CITATION_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).", "There have been many studies on parsing techniques ( #CITATION_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998). These works are restricted to each closed community, and the relation between them is not well discussed.", "Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques ( #CITATION_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).", "Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques ( #CITATION_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998). These works are restricted to each closed community, and the relation between them is not well discussed.", "There have been many studies on parsing techniques ( #CITATION_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998). These works are restricted to each closed community, and the relation between them is not well discussed. Investigating the relation will be apparently valuable for both communities.", "Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques ( #CITATION_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998). These works are restricted to each closed community, and the relation between them is not well discussed.", "Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques ( #CITATION_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998). These works are restricted to each closed community, and the relation between them is not well discussed. Investigating the relation will be apparently valuable for both communities."], "CC1314": [], "CC1317": ["There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #CITATION_TAG ; Krovetz , 1993 ; Hull , 1996 ) . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary."], "CC1318": ["From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #CITATION_TAG ) .", "This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #CITATION_TAG ) .", "When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #CITATION_TAG ) ."], "CC1319": ["mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance", "mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).", "The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).", "mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain.", "The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( #CITATION_TAG ; Porter , 1980 ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain."], "CC1320": ["There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #CITATION_TAG ) . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary."], "CC1321": ["mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance", "mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).", "The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).", "mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain.", "The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers ( Lovins , 1968 ; #CITATION_TAG ) demonstrably improve retrieval performance This has been reported for other languages, too, dependent on the generality of the chosen approach (J\u00e4ppinen and Niemist\u00f6, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmek\u00e7ioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain."], "CC1322": ["Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; #CITATION_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved . In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., \"search ed search ing\") 1 , derivation (e.g., \"search er\" or \"search able\") and composition (e.g., German \"Blut hoch druck \" [\"high blood pressure\"])."], "CC1325": ["Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #CITATION_TAG ) ) one can not really recommend this method ."], "CC1326": ["From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #CITATION_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .", "This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #CITATION_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .", "When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #CITATION_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) ."], "CC1327": ["Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #CITATION_TAG ; Tzoukermann et al. , 1997 ) .", "The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #CITATION_TAG ; Tzoukermann et al. , 1997 ) ."], "CC1328": ["The retrieval process relies on the vector space model ( #CITATION_TAG ) , with the cosine measure expressing the similarity between a query and a document . The search engine produces a ranked output of documents.", "It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric. The retrieval process relies on the vector space model ( #CITATION_TAG ) , with the cosine measure expressing the similarity between a query and a document . The search engine produces a ranked output of documents."], "CC1329": ["There has been some controversy , at least for simple stemmers ( #CITATION_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary."], "CC1331": ["This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #CITATION_TAG ) ) are incorporated into our system . Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).", "It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #CITATION_TAG ) ) are incorporated into our system . Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).", "Figure 5). It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #CITATION_TAG ) ) are incorporated into our system . Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995)."], "CC1332": ["There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #CITATION_TAG ; Hull , 1996 ) . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary."], "CC1333": ["This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; #CITATION_TAG ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; #CITATION_TAG ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; #CITATION_TAG ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain.", "The efforts required for performing morphologi- cal analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; #CITATION_TAG ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; #CITATION_TAG ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain.", "This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; #CITATION_TAG ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998).", "The efforts required for performing morphologi- cal analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; #CITATION_TAG ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain.", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; #CITATION_TAG ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998)."], "CC1334": ["From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #CITATION_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .", "This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #CITATION_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .", "When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #CITATION_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) ."], "CC1335": ["From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #CITATION_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .", "This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #CITATION_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .", "When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #CITATION_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) ."], "CC1336": ["Alternatively , we may think of user-centered comparative studies ( #CITATION_TAG ) .", "This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system. Alternatively , we may think of user-centered comparative studies ( #CITATION_TAG ) .", "It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system. Alternatively , we may think of user-centered comparative studies ( #CITATION_TAG ) ."], "CC1337": ["Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #CITATION_TAG ) .", "The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #CITATION_TAG ) ."], "CC1338": ["Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #CITATION_TAG ) , turns out to be infeasible , at least for German and related languages .", "The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents. Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #CITATION_TAG ) , turns out to be infeasible , at least for German and related languages .", "This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., \ufffdBlut druck mess gera__t\ufffd translates to \ufffddevice for measuring blood pressure\ufffd). The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents. Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #CITATION_TAG ) , turns out to be infeasible , at least for German and related languages ."], "CC1339": ["This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain.", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain.", "This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).", "The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain.", "mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).", "The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain.", "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 \"\u00a1 \" denotes the string concatenation operator. mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc \u00c2\u00b8 ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #CITATION_TAG ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998)."], "CC1340": ["Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #CITATION_TAG ; J \u00c2\u00a8 appinen and Niemist \u00c2\u00a8 o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved . In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., \"search ed search ing\") 1 , derivation (e.g., \"search er\" or \"search able\") and composition (e.g., German \"Blut hoch druck \" [\"high blood pressure\"])."], "CC1341": [], "CC1342": ["For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #CITATION_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."], "CC1343": ["For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #CITATION_TAG ) ."], "CC1344": ["In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #CITATION_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two . The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation."], "CC1345": ["These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #CITATION_TAG ) . crossed dependencies (Tang & Zaharin, 1995)."], "CC1346": ["Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #CITATION_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .", "Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g. non-projective correspondence), see Figure 1. Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #CITATION_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .", "It is well known that many linguistic constructions are not projective (e.g. scrambling, cross serial dependencies, etc.). Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g. non-projective correspondence), see Figure 1. Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #CITATION_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC ."], "CC1347": ["#CITATION_TAG presented an approach for constructing a BKB based on the S-SSTC .", "S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications. #CITATION_TAG presented an approach for constructing a BKB based on the S-SSTC .", "The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages\" structures. S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications. #CITATION_TAG presented an approach for constructing a BKB based on the S-SSTC ."], "CC1349": ["It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.", "Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.", "It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component.", "S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.", "Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component.", "It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.", "For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeill\u00e9 et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990). S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.", "S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component.", "Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.", "It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict. Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).", "For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeill\u00e9 et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990). S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component.", "S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.", "Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict. Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).", "For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeill\u00e9 et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990). S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.", "S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #CITATION_TAG ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict. Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994)."], "CC1350": [], "CC1351": ["For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #CITATION_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."], "CC1352": ["In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #CITATION_TAG cases ) . Shieber (1994) cases).", "As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms. In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #CITATION_TAG cases ) . Shieber (1994) cases).", "In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #CITATION_TAG cases ) . Shieber (1994) cases). Due to lack of space we will only brief on some of these non-standard cases without going into the details."], "CC1354": ["Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #CITATION_TAG ) , such as the relation between syntax and semantic ."], "CC1355": ["For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #CITATION_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules\" extraction from parallel parsed corpus", "Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages). For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #CITATION_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules\" extraction from parallel parsed corpus", "This means allowing one-to-one, one-to-many and many-to-many, but the mappings do not overlap. Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages). For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #CITATION_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules\" extraction from parallel parsed corpus"], "CC1356": ["For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #CITATION_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."], "CC1357": ["Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #CITATION_TAG ) ."], "CC1358": ["For more details on the proprieties of SSTC , see #CITATION_TAG .", "The particle \"up\" is featurised into the verb \"pick\" and in discontinuous manner (e.g. up\" (4-5) in \"pick-up\" (1-2+4-5)) in the sentence \"He picks the box up\". For more details on the proprieties of SSTC , see #CITATION_TAG ."], "CC1360": ["For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #CITATION_TAG ) , ( Al-Adhaileh & Tang , 1999 ) ."], "CC1361": ["It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.", "Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.", "It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component.", "S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.", "Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component.", "It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.", "For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeill\u00e9 et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990). S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.", "S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component.", "Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.", "It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict. Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).", "For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeill\u00e9 et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990). S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component.", "S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.", "Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict. Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).", "For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeill\u00e9 et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990). S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.", "S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #CITATION_TAG ) , ( Harbusch & Poller , 2000 ) . As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict. Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994)."], "CC1362": ["For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #CITATION_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."], "CC1363": ["A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #CITATION_TAG . and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.", "An interval is assigned to each word in the sentence, i.e. (0-1) for \"John (1-2) for \"picks (2-3) for \"the (3-4) for \"box\" and (4-5) for \"up\". A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #CITATION_TAG . and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.", "It contains a nonprojective correspondence. An interval is assigned to each word in the sentence, i.e. (0-1) for \"John (1-2) for \"picks (2-3) for \"the (3-4) for \"box\" and (4-5) for \"up\". A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #CITATION_TAG . and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree."], "CC1364": ["However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #CITATION_TAG ) . Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.", "However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #CITATION_TAG ) . Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text databases.", "However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #CITATION_TAG ) . Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data. Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web."], "CC1367": ["Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #CITATION_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).", "There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #CITATION_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).", "Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #CITATION_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.", "There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #CITATION_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.", "Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #CITATION_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system."], "CC1368": ["There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #CITATION_TAG ) . Web services will allow components developed by different researchers in different locations to be composed to build larger systems.", "This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #CITATION_TAG ) . Web services will allow components developed by different researchers in different locations to be composed to build larger systems.", "Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #CITATION_TAG ) . Web services will allow components developed by different researchers in different locations to be composed to build larger systems."], "CC1370": ["The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #CITATION_TAG )", "The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #CITATION_TAG ) We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.", "The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #CITATION_TAG ) We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger."], "CC1371": ["The implementation has been inspired by experience in extracting information from very large corpora ( #CITATION_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 )", "The implementation has been inspired by experience in extracting information from very large corpora ( #CITATION_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.", "The implementation has been inspired by experience in extracting information from very large corpora ( #CITATION_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger."], "CC1372": ["An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #CITATION_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).", "However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #CITATION_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).", "An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #CITATION_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.", "Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #CITATION_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).", "However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #CITATION_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second."], "CC1373": ["Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #CITATION_TAG ) . However, the source code for these tools is not freely available, so they cannot be extended.", "This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #CITATION_TAG ) . However, the source code for these tools is not freely available, so they cannot be extended.", "These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #CITATION_TAG ) . However, the source code for these tools is not freely available, so they cannot be extended."], "CC1374": ["Recent work ( Banko and Brill , 2001 ; #CITATION_TAG ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases.", "However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work ( Banko and Brill , 2001 ; #CITATION_TAG ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases.", "Recent work ( Banko and Brill , 2001 ; #CITATION_TAG ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.", "However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work ( Banko and Brill , 2001 ; #CITATION_TAG ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.", "Recent work ( Banko and Brill , 2001 ; #CITATION_TAG ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime.", "However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work ( Banko and Brill , 2001 ; #CITATION_TAG ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime.", "Recent work ( Banko and Brill , 2001 ; #CITATION_TAG ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime. For example, Google currently answers 250 million queries per day, thus processing time must be minimised."], "CC1375": ["For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #CITATION_TAG ) , currently used for training POS taggers", "Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #CITATION_TAG ) , currently used for training POS taggers", "For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #CITATION_TAG ) , currently used for training POS taggers This will require more efficient learning algorithms and implementations.", "NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #CITATION_TAG ) , currently used for training POS taggers", "Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #CITATION_TAG ) , currently used for training POS taggers This will require more efficient learning algorithms and implementations."], "CC1378": ["For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #CITATION_TAG ) . GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.", "Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #CITATION_TAG ) . GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.", "For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #CITATION_TAG ) . GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system.", "There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #CITATION_TAG ) . GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.", "Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #CITATION_TAG ) . GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system."], "CC1379": ["Software engineering research on Generative Programming ( #CITATION_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems"], "CC1380": ["For example , 10 million words of the American National Corpus ( #CITATION_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers", "Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus ( #CITATION_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers", "For example , 10 million words of the American National Corpus ( #CITATION_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers This will require more efficient learning algorithms and implementations.", "NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus ( #CITATION_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers", "Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus ( #CITATION_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers This will require more efficient learning algorithms and implementations."], "CC1381": ["For example , the suite of LT tools ( Mikheev et al. , 1999 ; #CITATION_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.", "A number of stand-alone tools have also been developed. For example , the suite of LT tools ( Mikheev et al. , 1999 ; #CITATION_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.", "For example , the suite of LT tools ( Mikheev et al. , 1999 ; #CITATION_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly.", "A number of stand-alone tools have also been developed. For example , the suite of LT tools ( Mikheev et al. , 1999 ; #CITATION_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly.", "For example , the suite of LT tools ( Mikheev et al. , 1999 ; #CITATION_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).", "A number of stand-alone tools have also been developed. For example , the suite of LT tools ( Mikheev et al. , 1999 ; #CITATION_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).", "For example , the suite of LT tools ( Mikheev et al. , 1999 ; #CITATION_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998). However, the source code for these tools is not freely available, so they cannot be extended."], "CC1383": ["Recent work ( #CITATION_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases.", "However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work ( #CITATION_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases.", "Recent work ( #CITATION_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.", "However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work ( #CITATION_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.", "Recent work ( #CITATION_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime.", "However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003). Recent work ( #CITATION_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime.", "Recent work ( #CITATION_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data . Also, many potential applications of NLP will involve processing very large text databases. For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web. Other potential applications must process text online or in realtime. For example, Google currently answers 250 million queries per day, thus processing time must be minimised."], "CC1384": ["For example , the suite of LT tools ( #CITATION_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.", "A number of stand-alone tools have also been developed. For example , the suite of LT tools ( #CITATION_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.", "For example , the suite of LT tools ( #CITATION_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly.", "A number of stand-alone tools have also been developed. For example , the suite of LT tools ( #CITATION_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly.", "For example , the suite of LT tools ( #CITATION_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).", "A number of stand-alone tools have also been developed. For example , the suite of LT tools ( #CITATION_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).", "For example , the suite of LT tools ( #CITATION_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly . These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998). However, the source code for these tools is not freely available, so they cannot be extended."], "CC1385": ["Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #CITATION_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method"], "CC1386": ["Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #CITATION_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) . The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.", "An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #CITATION_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) . The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.", "However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #CITATION_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) . The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second."], "CC1387": [], "CC1388": ["These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #CITATION_TAG )", "These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #CITATION_TAG )", "These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #CITATION_TAG ) We expect even faster training times when we move to conjugate gradient methods.", "We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #CITATION_TAG )", "These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #CITATION_TAG ) We expect even faster training times when we move to conjugate gradient methods.", "The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #CITATION_TAG )", "We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #CITATION_TAG ) We expect even faster training times when we move to conjugate gradient methods."], "CC1389": ["Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #CITATION_TAG ) . The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.", "An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #CITATION_TAG ) . The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.", "However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #CITATION_TAG ) . The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second."], "CC1390": [], "CC1391": ["The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #CITATION_TAG ; Clark et al. , 2003 )", "The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #CITATION_TAG ; Clark et al. , 2003 ) We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.", "The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #CITATION_TAG ; Clark et al. , 2003 ) We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger."], "CC1392": ["An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #CITATION_TAG ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).", "However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #CITATION_TAG ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).", "An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #CITATION_TAG ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.", "Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #CITATION_TAG ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).", "However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #CITATION_TAG ) . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second."], "CC1393": [], "CC1394": ["The TNT POS tagger ( #CITATION_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .", "Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger ( #CITATION_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .", "An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997). The TNT POS tagger ( #CITATION_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second ."], "CC1395": ["GATE goes beyond earlier systems by using a component-based infrastructure ( #CITATION_TAG ) which the GUI is built on top of", "For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure ( #CITATION_TAG ) which the GUI is built on top of", "GATE goes beyond earlier systems by using a component-based infrastructure ( #CITATION_TAG ) which the GUI is built on top of This allows components to be highly configurable and simplifies the addition of new components to the system.", "Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure ( #CITATION_TAG ) which the GUI is built on top of", "For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure ( #CITATION_TAG ) which the GUI is built on top of This allows components to be highly configurable and simplifies the addition of new components to the system.", "There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure ( #CITATION_TAG ) which the GUI is built on top of", "Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002). GATE goes beyond earlier systems by using a component-based infrastructure ( #CITATION_TAG ) which the GUI is built on top of This allows components to be highly configurable and simplifies the addition of new components to the system."], "CC1397": ["Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #CITATION_TAG ) . Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.", "Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #CITATION_TAG ) . Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998)."], "CC1398": ["Similarly , ( Barzilay and Lee , 2003 ) and ( #CITATION_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.", "For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , ( Barzilay and Lee , 2003 ) and ( #CITATION_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.", "Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , ( Barzilay and Lee , 2003 ) and ( #CITATION_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts."], "CC1399": ["And ( #CITATION_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .", "Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And ( #CITATION_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .", "For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And ( #CITATION_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts ."], "CC1400": ["To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG )", "Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG )", "To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information.", "For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeill\u00e9 FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG )", "Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information.", "To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way.", "A first investigation of Anne Abeill\u00e9\"s TAG for French suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward. For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeill\u00e9 FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG )", "For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeill\u00e9 FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information.", "Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way.", "To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way. For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.", "A first investigation of Anne Abeill\u00e9\"s TAG for French suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward. For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeill\u00e9 FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information.", "For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeill\u00e9 FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way.", "Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way. For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.", "A first investigation of Anne Abeill\u00e9\"s TAG for French suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward. For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeill\u00e9 FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way.", "For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeill\u00e9 FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #CITATION_TAG ) This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way. For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur."], "CC1401": ["Similarly , ( #CITATION_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.", "For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , ( #CITATION_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.", "Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , ( #CITATION_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts."], "CC1403": ["For instance , ( #CITATION_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning . Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.", "Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance , ( #CITATION_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning . Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.", "For instance , ( #CITATION_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning . Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.", "More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance , ( #CITATION_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning . Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.", "Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance , ( #CITATION_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning . Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts."], "CC1405": ["For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.", "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.", "For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.", ") While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.", "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.", "For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.", ") and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.", ") While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.", "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.", "For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology.", ") and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.", ") While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.", "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology.", ") and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.", ") While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #CITATION_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology."], "CC1406": ["The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #CITATION_TAG ; Copestake et al. , 2001 )", "The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #CITATION_TAG ; Copestake et al. , 2001 ) However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships."], "CC1407": [") While corpus driven efforts along the PARSEVAL lines ( #CITATION_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.", ") and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PARSEVAL lines ( #CITATION_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.", ") While corpus driven efforts along the PARSEVAL lines ( #CITATION_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).", "Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language? ) and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PARSEVAL lines ( #CITATION_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.", ") and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PARSEVAL lines ( #CITATION_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).", "Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar. Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language? ) and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PARSEVAL lines ( #CITATION_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.", "Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language? ) and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PARSEVAL lines ( #CITATION_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998)."], "CC1408": ["The language chosen for semantic representation is a flat semantics along the line of ( #CITATION_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 )", "The language chosen for semantic representation is a flat semantics along the line of ( #CITATION_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships."], "CC1409": ["In particular , ( #CITATION_TAG ) lists the converses of some 3 500 predicative nouns .", "For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular , ( #CITATION_TAG ) lists the converses of some 3 500 predicative nouns ."], "CC1411": ["For complementing this database and for converse constructions , the LADL tables ( #CITATION_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions . In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns."], "CC1412": ["For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.", "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.", "For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.", ") While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.", "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.", "For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.", ") and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.", ") While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.", "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.", "For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology.", ") and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.", ") While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.", "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology.", ") and its degree of overgeneration (does it generate only the sentences of the described language? ) While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.", ") While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #CITATION_TAG ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology."], "CC1414": ["Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #CITATION_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."], "CC1415": ["Semantic construction proceeds from the derived tree ( #CITATION_TAG ) rather than -- as is more common in TAG -- from the derivation tree", "Semantic construction proceeds from the derived tree ( #CITATION_TAG ) rather than -- as is more common in TAG -- from the derivation tree This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation."], "CC1416": ["Thus for instance , ( Copestake and Flickinger , 2000 ; #CITATION_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."], "CC1418": ["For shuffling paraphrases , french alternations are partially described in ( #CITATION_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs . For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions."], "CC1419": ["It compares favorably to other stemming or root extraction algorithms ( #CITATION_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process.", "In this work, we use the Arabic root extraction technique in (El Kourdi, 2004). It compares favorably to other stemming or root extraction algorithms ( #CITATION_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process.", "This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El Kourdi, 2004). It compares favorably to other stemming or root extraction algorithms ( #CITATION_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process."], "CC1420": ["It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #CITATION_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process.", "In this work, we use the Arabic root extraction technique in (El Kourdi, 2004). It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #CITATION_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process.", "This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El Kourdi, 2004). It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #CITATION_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process."], "CC1421": ["Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #CITATION_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic . (Yang and Pedersen, 1997) has found strong correlations between DF, IG and the \u03c7 2 statistic for a term.", "Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #CITATION_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic . (Yang and Pedersen, 1997) has found strong correlations between DF, IG and the \u03c7 2 statistic for a term.", "Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #CITATION_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic . (Yang and Pedersen, 1997) has found strong correlations between DF, IG and the \u03c7 2 statistic for a term. On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance.", "Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #CITATION_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic . (Yang and Pedersen, 1997) has found strong correlations between DF, IG and the \u03c7 2 statistic for a term. On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance.", "Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #CITATION_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic . (Yang and Pedersen, 1997) has found strong correlations between DF, IG and the \u03c7 2 statistic for a term. On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).", "Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #CITATION_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic . (Yang and Pedersen, 1997) has found strong correlations between DF, IG and the \u03c7 2 statistic for a term. On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).", "Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #CITATION_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic . (Yang and Pedersen, 1997) has found strong correlations between DF, IG and the \u03c7 2 statistic for a term. On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001). TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999)."], "CC1422": ["This work is a continuation of that initiated in ( #CITATION_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).", "The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. This work is a continuation of that initiated in ( #CITATION_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).", "This work is a continuation of that initiated in ( #CITATION_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories). The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.", "The present work evaluates the performance on Arabic documents of the Na\u00efve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997). The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. This work is a continuation of that initiated in ( #CITATION_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).", "The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. This work is a continuation of that initiated in ( #CITATION_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories). The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.", "Sakhr\"s marketing literature claims that this categorizer is based on Arabic morphology and some research that has been carried out on natural language processing. The present work evaluates the performance on Arabic documents of the Na\u00efve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997). The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. This work is a continuation of that initiated in ( #CITATION_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).", "The present work evaluates the performance on Arabic documents of the Na\u00efve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997). The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. This work is a continuation of that initiated in ( #CITATION_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories). The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net."], "CC1424": [], "CC1425": ["This is mainly due to the fact that Arabic is a non-concatenative language ( #CITATION_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root . The infix form (or stem) needs further to be processed in order to obtain the root.", "In Arabic, however, the use of stems will not yield satisfactory categorization. This is mainly due to the fact that Arabic is a non-concatenative language ( #CITATION_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root . The infix form (or stem) needs further to be processed in order to obtain the root.", "This is mainly due to the fact that Arabic is a non-concatenative language ( #CITATION_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root . The infix form (or stem) needs further to be processed in order to obtain the root. This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).", "In Arabic, however, the use of stems will not yield satisfactory categorization. This is mainly due to the fact that Arabic is a non-concatenative language ( #CITATION_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root . The infix form (or stem) needs further to be processed in order to obtain the root. This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).", "This is mainly due to the fact that Arabic is a non-concatenative language ( #CITATION_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root . The infix form (or stem) needs further to be processed in order to obtain the root. This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998). As an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity."], "CC1426": ["( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance.", "Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the \u03c7 2 statistic. ( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance.", "( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).", "Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the \u03c7 2 statistic. ( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance.", "Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the \u03c7 2 statistic. ( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).", "( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001). TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).", "Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the \u03c7 2 statistic. ( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).", "Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the \u03c7 2 statistic. ( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001). TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).", "( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001). TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999). Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.", "Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the \u03c7 2 statistic. ( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001). TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).", "Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the \u03c7 2 statistic. ( #CITATION_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (Rogati and Yang, 2002) reports the \u03c7 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001). TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999). Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents."], "CC1427": ["More recently , ( #CITATION_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) ."], "CC1428": ["Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping.", "As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping.", "Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping. Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).", "It consists of assigning and labeling documents using a set of predefined categories based on document contents. As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping.", "As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping. Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).", "Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping. Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003). In this paper, NB which is a statistical machine learning algorithm is used to learn to classify non-vocalized 1 Arabic web text documents.", "Automatic text (or document) categorization attempts to replace and save human effort required in performing manual categorization. It consists of assigning and labeling documents using a set of predefined categories based on document contents. As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping.", "It consists of assigning and labeling documents using a set of predefined categories based on document contents. As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping. Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).", "As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping. Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003). In this paper, NB which is a statistical machine learning algorithm is used to learn to classify non-vocalized 1 Arabic web text documents.", "Automatic text (or document) categorization attempts to replace and save human effort required in performing manual categorization. It consists of assigning and labeling documents using a set of predefined categories based on document contents. As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping. Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).", "It consists of assigning and labeling documents using a set of predefined categories based on document contents. As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #CITATION_TAG ) . Such applications have included electronic email filtering, newsgroups classification, and survey data grouping. Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003). In this paper, NB which is a statistical machine learning algorithm is used to learn to classify non-vocalized 1 Arabic web text documents."], "CC1430": ["TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #CITATION_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance . Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .", "The importance of each term is assumed to be inversely proportional to the number of documents that contain that term. TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #CITATION_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance . Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t ."], "CC1434": ["More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #CITATION_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) ."], "CC1435": ["For example , ( #CITATION_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces ."], "CC1438": ["This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #CITATION_TAG , for example ) , does not specify the relationship itself . Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\\x1berentiated."], "CC1439": [], "CC1440": ["The terms have been identified as the most specific to our corpus by a program developed by #CITATION_TAG and called TER1vloSTAT . The ten most speci\\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).", "To construct this test set, we have focused our attention on ten domain-speci\\x1cc terms: commande (command), con\\x1cguration, \\x1cchier (\\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst\ufffdme (system), utilisateur (user ). The terms have been identified as the most specific to our corpus by a program developed by #CITATION_TAG and called TER1vloSTAT . The ten most speci\\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).", "The terms have been identified as the most specific to our corpus by a program developed by #CITATION_TAG and called TER1vloSTAT . The ten most speci\\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004). Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.", "To construct this test set, we have focused our attention on ten domain-speci\\x1cc terms: commande (command), con\\x1cguration, \\x1cchier (\\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst\ufffdme (system), utilisateur (user ). The terms have been identified as the most specific to our corpus by a program developed by #CITATION_TAG and called TER1vloSTAT . The ten most speci\\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004). Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.", "The terms have been identified as the most specific to our corpus by a program developed by #CITATION_TAG and called TER1vloSTAT . The ten most speci\\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004). Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step. (They were removed from the example set.", "To construct this test set, we have focused our attention on ten domain-speci\\x1cc terms: commande (command), con\\x1cguration, \\x1cchier (\\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst\ufffdme (system), utilisateur (user ). The terms have been identified as the most specific to our corpus by a program developed by #CITATION_TAG and called TER1vloSTAT . The ten most speci\\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004). Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step. (They were removed from the example set.", "The terms have been identified as the most specific to our corpus by a program developed by #CITATION_TAG and called TER1vloSTAT . The ten most speci\\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004). Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step. (They were removed from the example set. )"], "CC1441": [], "CC1442": ["Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #CITATION_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\\x1b and Tugwell, 2001, for example).", "On the other hand, other work has been carried out in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #CITATION_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\\x1b and Tugwell, 2001, for example).", "Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #CITATION_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\\x1b and Tugwell, 2001, for example). It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.", "On the other hand, other work has been carried out in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #CITATION_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\\x1b and Tugwell, 2001, for example). It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.", "Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #CITATION_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\\x1b and Tugwell, 2001, for example). It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs. The original acquisition methodology we present in the next section will allow us to overcome this limitation."], "CC1443": ["ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #CITATION_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E \u00e2\\x88\\x92 ) of the elements one wants to acquire and their context.", "ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #CITATION_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E \u00e2\\x88\\x92 ) of the elements one wants to acquire and their context. The contextual patterns produced can then be applied to the corpus in order to retrieve new elements."], "CC1444": ["However , most strategies are based on `` internal  or `` external methods  ( #CITATION_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts . (In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process."], "CC1445": ["The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #CITATION_TAG ) ."], "CC1447": ["ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #CITATION_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) . Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs."], "CC1448": ["Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #CITATION_TAG with WoRDNET relations ) ."], "CC1449": ["ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #CITATION_TAG ) . Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs."], "CC1450": [", see below ) used for collocation acquisition ( see ( #CITATION_TAG ) for a review ) , these patterns allow :", "Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #CITATION_TAG ) for a review ) , these patterns allow :"], "CC1451": ["ASARES is presented in detail in ( #CITATION_TAG )", "The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool. ASARES is presented in detail in ( #CITATION_TAG )", "ASARES is presented in detail in ( #CITATION_TAG ) We simply give a short account of its basic principles herein."], "CC1452": ["A number of applications have relied on distributional analysis ( #CITATION_TAG ) in order to build classes of semantically related terms", "A number of applications have relied on distributional analysis ( #CITATION_TAG ) in order to build classes of semantically related terms This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself."], "CC1453": ["It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #CITATION_TAG ; Xu et al. , 2002 ) .", "In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #CITATION_TAG ; Xu et al. , 2002 ) .", "In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words). In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #CITATION_TAG ; Xu et al. , 2002 ) ."], "CC1454": ["The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #CITATION_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #CITATION_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #CITATION_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #CITATION_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #CITATION_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #CITATION_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #CITATION_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}."], "CC1455": ["Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it).", "Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it).", "Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL\"02 and CoNLL\"03 shared tasks. Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it).", "Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL\"02 and CoNLL\"03 shared tasks. Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it).", "The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL\"02 and CoNLL\"03 shared tasks. Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL\"02 and CoNLL\"03 shared tasks. Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL\"02 and CoNLL\"03 shared tasks. Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #CITATION_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}."], "CC1457": ["These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #CITATION_TAG Arabic data . The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future."], "CC1458": ["Character classes , such as punctuation , are defined according to the Unicode Standard ( #CITATION_TAG ) .", "Each punctuation symbol is considered a separate token. Character classes , such as punctuation , are defined according to the Unicode Standard ( #CITATION_TAG ) .", "We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters. Each punctuation symbol is considered a separate token. Character classes , such as punctuation , are defined according to the Unicode Standard ( #CITATION_TAG ) ."], "CC1459": ["It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #CITATION_TAG ) .", "In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #CITATION_TAG ) .", "In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words). In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #CITATION_TAG ) ."], "CC1460": [], "CC1462": ["The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #CITATION_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #CITATION_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #CITATION_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #CITATION_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #CITATION_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #CITATION_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #CITATION_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}."], "CC1463": ["The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #CITATION_TAG ) . One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.", "We use a maximum entropy Markov model (MEMM) classifier. The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #CITATION_TAG ) . One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.", "We assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. We use a maximum entropy Markov model (MEMM) classifier. The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #CITATION_TAG ) . One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision."], "CC1464": [], "CC1465": ["The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #CITATION_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #CITATION_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #CITATION_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #CITATION_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #CITATION_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #CITATION_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #CITATION_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}."], "CC1466": [], "CC1467": ["Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in . Both systems are built around from the maximum-entropy technique ( #CITATION_TAG )", "Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in . Both systems are built around from the maximum-entropy technique ( #CITATION_TAG ) We formulate the mention detection task as a sequence classification problem.", "Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in . Both systems are built around from the maximum-entropy technique ( #CITATION_TAG ) We formulate the mention detection task as a sequence classification problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language."], "CC1468": ["Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #CITATION_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) . Both systems are built around from the maximum-entropy technique (Berger et al., 1996).", "Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #CITATION_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) . Both systems are built around from the maximum-entropy technique (Berger et al., 1996). We formulate the mention detection task as a sequence classi cation problem.", "Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #CITATION_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) . Both systems are built around from the maximum-entropy technique (Berger et al., 1996). We formulate the mention detection task as a sequence classi cation problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language."], "CC1469": ["The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #CITATION_TAG )", "The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #CITATION_TAG ) We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively."], "CC1470": [], "CC1471": ["The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #CITATION_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #CITATION_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #CITATION_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #CITATION_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #CITATION_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #CITATION_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #CITATION_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}."], "CC1472": ["As in ( #CITATION_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems . In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.", "We seeked to exploit this ability to generalize to improve the dictionary based model. As in ( #CITATION_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems . In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.", "As in ( #CITATION_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems . In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus. From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus.", "However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data. We seeked to exploit this ability to generalize to improve the dictionary based model. As in ( #CITATION_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems . In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.", "We seeked to exploit this ability to generalize to improve the dictionary based model. As in ( #CITATION_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems . In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus. From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus."], "CC1473": ["The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #CITATION_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #CITATION_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #CITATION_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #CITATION_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #CITATION_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #CITATION_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #CITATION_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}."], "CC1474": ["We introduce here a clearly defined and replicable split of the #CITATION_TAG data , so that future investigations can accurately and correctly compare against the results presented here ."], "CC1477": ["The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #CITATION_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #CITATION_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #CITATION_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #CITATION_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #CITATION_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #CITATION_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.", "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #CITATION_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL \"02 and CoNLL \"03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}."], "CC1478": ["ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #CITATION_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric . The result is shown in Table 4: the baseline numbers without stem features are listed under \\\\Base,\" and the results of the coreference system with stem features are listed under \\\\Base+Stem.", "We report results with two metrics: ECM-F and ACE- Value. ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #CITATION_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric . The result is shown in Table 4: the baseline numbers without stem features are listed under \\\\Base,\" and the results of the coreference system with stem features are listed under \\\\Base+Stem.", "True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system. We report results with two metrics: ECM-F and ACE- Value. ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #CITATION_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric . The result is shown in Table 4: the baseline numbers without stem features are listed under \\\\Base,\" and the results of the coreference system with stem features are listed under \\\\Base+Stem."], "CC1479": ["Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #CITATION_TAG ) . The formation of broken plurals is common, more complex and often irregular.", "Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #CITATION_TAG ) . The formation of broken plurals is common, more complex and often irregular. As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix . The plural form of the noun (book) is (books), which is formed by deleting the infix . The plural form and the singular form may also be completely different (e.g. for woman, but for women)."], "CC1480": ["Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #CITATION_TAG ) . Both systems are built around from the maximum-entropy technique (Berger et al., 1996).", "Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #CITATION_TAG ) . Both systems are built around from the maximum-entropy technique (Berger et al., 1996). We formulate the mention detection task as a sequence classification problem.", "Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #CITATION_TAG ) . Both systems are built around from the maximum-entropy technique (Berger et al., 1996). We formulate the mention detection task as a sequence classification problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language."], "CC1481": ["As stated before , the experiments are run in the ACE \"04 framework ( #CITATION_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.", "We want to investigate the usefulness of stem n- gram features in the mention detection system. As stated before , the experiments are run in the ACE \"04 framework ( #CITATION_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.", "As stated before , the experiments are run in the ACE \"04 framework ( #CITATION_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system. The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.", "We want to investigate the usefulness of stem n- gram features in the mention detection system. As stated before , the experiments are run in the ACE \"04 framework ( #CITATION_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system. The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.", "As stated before , the experiments are run in the ACE \"04 framework ( #CITATION_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system. The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class. Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.", "We want to investigate the usefulness of stem n- gram features in the mention detection system. As stated before , the experiments are run in the ACE \"04 framework ( #CITATION_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system. The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class. Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.", "As stated before , the experiments are run in the ACE \"04 framework ( #CITATION_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system. The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class. Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type. In this paper, we report the results in terms of precision, recall, and F-measure3."], "CC1482": [], "CC1483": ["#CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness", "Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness", "#CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs.", "Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness", "Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs.", "#CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points.", "We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair. Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness", "Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs.", "Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points.", "#CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points. The manual was presented to the subjects before the experiment and could be re-accessed at any time.", "We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair. Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs.", "Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points.", "Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points. The manual was presented to the subjects before the experiment and could be re-accessed at any time.", "We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair. Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points.", "Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. #CITATION_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points. The manual was presented to the subjects before the experiment and could be re-accessed at any time."], "CC1484": ["It is defined on different kinds of textual units, e.g. documents, parts of a document (e.g. words and their surrounding context), words or concepts ( #CITATION_TAG ).", "Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction. It is defined on different kinds of textual units, e.g. documents, parts of a document (e.g. words and their surrounding context), words or concepts ( #CITATION_TAG ).", "It is defined on different kinds of textual units, e.g. documents, parts of a document (e.g. words and their surrounding context), words or concepts ( #CITATION_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness."], "CC1485": ["In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments.", "They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments.", "In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\".", "Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments.", "They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\".", "In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\". This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.", "This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments.", "Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\".", "They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\". This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.", "In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\". This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly. Therefore, we automatically create word pairs using a corpus-based approach.", "This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\".", "Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\". This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.", "They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\". This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly. Therefore, we automatically create word pairs using a corpus-based approach.", "This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\". This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.", "Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #CITATION_TAG ) . Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\". This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly. Therefore, we automatically create word pairs using a corpus-based approach."], "CC1486": [], "CC1487": ["#CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs", "However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs", "#CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here.", "To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs", "However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here.", "#CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).", "The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts. To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs", "To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here.", "However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).", "#CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8). The plot clearly shows an empty horizontal band with no judgments.", "The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts. To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here.", "To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).", "However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8). The plot clearly shows an empty horizontal band with no judgments.", "The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts. To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).", "To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #CITATION_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8). The plot clearly shows an empty horizontal band with no judgments."], "CC1488": ["This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.", "She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.", "This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.", "Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.", "She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.", "This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.", "Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.", "Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.", "She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.", "This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\".", "Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.", "Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.", "She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\".", "Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.", "Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #CITATION_TAG . Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\"."], "CC1490": ["In particular , the `` Semantic Information Retrieval \"\" project ( SIR #CITATION_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .", "This is motivated by the aim to enable research in information retrieval incorporating SR measures. In particular , the `` Semantic Information Retrieval \"\" project ( SIR #CITATION_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems ."], "CC1491": [], "CC1492": ["Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #CITATION_TAG ) , the German equivalent to WordNet , as a sense inventory for each word", "We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #CITATION_TAG ) , the German equivalent to WordNet , as a sense inventory for each word", "Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #CITATION_TAG ) , the German equivalent to WordNet , as a sense inventory for each word It is the most complete resource of this type for German.", "Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set. We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #CITATION_TAG ) , the German equivalent to WordNet , as a sense inventory for each word", "We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #CITATION_TAG ) , the German equivalent to WordNet , as a sense inventory for each word It is the most complete resource of this type for German.", "Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist. Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set. We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #CITATION_TAG ) , the German equivalent to WordNet , as a sense inventory for each word", "Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set. We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #CITATION_TAG ) , the German equivalent to WordNet , as a sense inventory for each word It is the most complete resource of this type for German."], "CC1493": [], "CC1494": ["#CITATION_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly.", "647. #CITATION_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly.", "#CITATION_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.", "623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=. 647. #CITATION_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly.", "647. #CITATION_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.", "670 for the first and r=. 623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=. 647. #CITATION_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly.", "623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=. 647. #CITATION_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low."], "CC1495": ["We used the revised experimental setup ( #CITATION_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs", "In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006). We used the revised experimental setup ( #CITATION_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs", "We used the revised experimental setup ( #CITATION_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.", "In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006). We used the revised experimental setup ( #CITATION_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.", "We used the revised experimental setup ( #CITATION_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs We annotated semantic relatedness instead of similarity and included also non noun-noun pairs. Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure."], "CC1496": ["In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #CITATION_TAG", "In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #CITATION_TAG We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.", "In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #CITATION_TAG We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. We annotated semantic relatedness instead of similarity and included also non noun-noun pairs."], "CC1497": [], "CC1498": ["Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #CITATION_TAG ) .", "3 Dissimilar words can be semantically related, e.g. via functional relationships (night -dark) or when they are antonyms (high -low). Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #CITATION_TAG ) ."], "CC1499": ["In the seminal work by #CITATION_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards . Test subjects were instructed to order the cards according to the \"similarity of meaning\" and then assign a continuous similarity value (0.0 -4.0) to each card.", "In the seminal work by #CITATION_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards . Test subjects were instructed to order the cards according to the \"similarity of meaning\" and then assign a continuous similarity value (0.0 -4.0) to each card. Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.", "In the seminal work by #CITATION_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards . Test subjects were instructed to order the cards according to the \"similarity of meaning\" and then assign a continuous similarity value (0.0 -4.0) to each card. Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by Resnik (1995) with 10 subjects."], "CC1500": ["The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #CITATION_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.", "The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #CITATION_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task. But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.", "The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #CITATION_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task. But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another."], "CC1501": ["#CITATION_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task", "The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006). #CITATION_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task", "#CITATION_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.", "The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006). #CITATION_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.", "#CITATION_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another.", "The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006). #CITATION_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another.", "#CITATION_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another. Application-based evaluation can only state the fact, but give little explanation about the reasons."], "CC1502": ["idf-weighting scheme ( #CITATION_TAG ) .", "The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc \" 8 tf. idf-weighting scheme ( #CITATION_TAG ) ."], "CC1503": ["#CITATION_TAG reported a correlation of r = .69 . Test subjects were trained students of computational linguistics, and word pairs were selected analytically.", "Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. #CITATION_TAG reported a correlation of r = .69 . Test subjects were trained students of computational linguistics, and word pairs were selected analytically.", "10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. #CITATION_TAG reported a correlation of r = .69 . Test subjects were trained students of computational linguistics, and word pairs were selected analytically."], "CC1504": ["As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #CITATION_TAG )", "GermaNet contains only a few conceptual glosses. As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #CITATION_TAG )", "As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #CITATION_TAG ) We removed words which had more than three senses."], "CC1505": ["#CITATION_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .", "Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations. #CITATION_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity ."], "CC1506": ["#CITATION_TAG annotated a larger set of word pairs ( 353 ) , too", "This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too", "#CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.", "She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too", "This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.", "#CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.", "Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too", "She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.", "This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.", "#CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\".", "Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.", "She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.", "This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\".", "Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.", "She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). #CITATION_TAG annotated a larger set of word pairs ( 353 ) , too They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from \"not similar\" to \"synonymous\"."], "CC1507": ["The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #CITATION_TAG ) . The resulting list of POS-tagged lemmas is weighted using the SMART \"ltc\" 8 tf."], "CC1508": [], "CC1509": [], "CC1510": [], "CC1511": ["If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #CITATION_TAG ) . 6 Pairs containing such words are not suitable for evaluation.", "If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #CITATION_TAG ) . 6 Pairs containing such words are not suitable for evaluation. To limit their impact on the experiment, a threshold for the maximal number of senses can be defined."], "CC1513": ["#CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=.", "10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=.", "#CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=. 69.", "9026. 10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=.", "10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=. 69.", "#CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=. 69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically.", "Resnik (1995) reported a correlation of r=. 9026. 10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=.", "9026. 10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=. 69.", "10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=. 69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically.", "Resnik (1995) reported a correlation of r=. 9026. 10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=. 69.", "9026. 10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #CITATION_TAG did not report inter-subject correlation for their larger dataset . Gurevych (2006) reported a correlation of r=. 69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically."], "CC1514": ["10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.", "9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.", "10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.", "Resnik (1995) reported a correlation of r=. 9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.", "9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.", "10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=. 69.", "This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995) reported a correlation of r=. 9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.", "Resnik (1995) reported a correlation of r=. 9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.", "9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=. 69.", "10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=. 69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically.", "This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995) reported a correlation of r=. 9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.", "Resnik (1995) reported a correlation of r=. 9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=. 69.", "9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=. 69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically.", "This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. Resnik (1995) reported a correlation of r=. 9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=. 69.", "Resnik (1995) reported a correlation of r=. 9026. 10 #CITATION_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=. 69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically."], "CC1515": ["#CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German", "Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German", "#CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.", "However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German", "Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.", "#CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).", "A comprehensive evaluation of SR measures requires a higher number of word pairs. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German", "However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.", "Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).", "#CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.", "A comprehensive evaluation of SR measures requires a higher number of word pairs. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.", "However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).", "Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.", "A comprehensive evaluation of SR measures requires a higher number of word pairs. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).", "However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects. #CITATION_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too."], "CC1516": ["Therefore , inter-subject correlation is lower than the results obtained by #CITATION_TAG .", "Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore , inter-subject correlation is lower than the results obtained by #CITATION_TAG .", "that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group. Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore , inter-subject correlation is lower than the results obtained by #CITATION_TAG ."], "CC1517": [], "CC1518": ["Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #CITATION_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) ."], "CC1519": ["Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist \"s Search Engine ( #CITATION_TAG ; Resnik and Elkiss , 2003 ) .", "Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist \"s Search Engine ( #CITATION_TAG ; Resnik and Elkiss , 2003 ) .", "Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist \"s Search Engine ( #CITATION_TAG ; Resnik and Elkiss , 2003 ) ."], "CC1520": ["In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #CITATION_TAG ) . Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).", "In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #CITATION_TAG ) . Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.", "In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #CITATION_TAG ) . Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998)."], "CC1521": ["In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #CITATION_TAG ; Hughes et al , 2004 ) . While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.", "In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #CITATION_TAG ; Hughes et al , 2004 ) . While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001).", "In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #CITATION_TAG ; Hughes et al , 2004 ) . While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001). Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems."], "CC1522": ["Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist \"s Search Engine ( Kilgarriff , 2003 ; #CITATION_TAG ) .", "Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist \"s Search Engine ( Kilgarriff , 2003 ; #CITATION_TAG ) .", "Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist \"s Search Engine ( Kilgarriff , 2003 ; #CITATION_TAG ) ."], "CC1524": ["The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Existing tagging systems are \"small scale\" and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "Existing tagging systems are \"small scale\" and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "Existing tagging systems are \"small scale\" and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #CITATION_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)."], "CC1525": ["Word frequency counts in internet search engines are inconsistent and unreliable ( #CITATION_TAG ) . Tools based on static corpora do not suffer from this problem, e.g. BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.", "A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable ( #CITATION_TAG ) . Tools based on static corpora do not suffer from this problem, e.g. BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.", "Word frequency counts in internet search engines are inconsistent and unreliable ( #CITATION_TAG ) . Tools based on static corpora do not suffer from this problem, e.g. BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus. Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags.", "A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable ( #CITATION_TAG ) . Tools based on static corpora do not suffer from this problem, e.g. BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus. Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags.", "Word frequency counts in internet search engines are inconsistent and unreliable ( #CITATION_TAG ) . Tools based on static corpora do not suffer from this problem, e.g. BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus. Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags. In addition, PIE 9 (Phrases in English), developed at USNA, which performs searches on n-grams (based on words, parts-ofspeech and characters), is currently restricted to the British National Corpus as well, although other static corpora are being added to its database.", "A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable ( #CITATION_TAG ) . Tools based on static corpora do not suffer from this problem, e.g. BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus. Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags. In addition, PIE 9 (Phrases in English), developed at USNA, which performs searches on n-grams (based on words, parts-ofspeech and characters), is currently restricted to the British National Corpus as well, although other static corpora are being added to its database.", "Word frequency counts in internet search engines are inconsistent and unreliable ( #CITATION_TAG ) . Tools based on static corpora do not suffer from this problem, e.g. BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus. Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags. In addition, PIE 9 (Phrases in English), developed at USNA, which performs searches on n-grams (based on words, parts-ofspeech and characters), is currently restricted to the British National Corpus as well, although other static corpora are being added to its database. In contrast, little progress has been made toward annotating sizable sample corpora from the web."], "CC1526": ["The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems. This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Existing tagging systems are \ufffdsmall scale\ufffd and typically impose some limitation to prevent over- load (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems. This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems. This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\ufffds Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).", "Existing tagging systems are \ufffdsmall scale\ufffd and typically impose some limitation to prevent over- load (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems. This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems. This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\ufffds Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).", "Existing tagging systems are \ufffdsmall scale\ufffd and typically impose some limitation to prevent over- load (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems. This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems. This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #CITATION_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\ufffds Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003)."], "CC1527": ["In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #CITATION_TAG ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.", "This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #CITATION_TAG ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.", "In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #CITATION_TAG ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.", "Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #CITATION_TAG ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.", "This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #CITATION_TAG ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.", "In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #CITATION_TAG ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.", "Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #CITATION_TAG ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus."], "CC1528": ["In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #CITATION_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) . Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).", "In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #CITATION_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) . Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.", "In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #CITATION_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) . Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998)."], "CC1530": ["In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #CITATION_TAG ) . Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.", "While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #CITATION_TAG ) . Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.", "In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #CITATION_TAG ) . Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems. Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).", "In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for e-Science (Carroll et al., 2005;Hughes et al, 2004). While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #CITATION_TAG ) . Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.", "While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #CITATION_TAG ) . Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems. Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals)."], "CC1531": ["This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG )", "We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG )", "This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin.", "We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun\"s P2P API). We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG )", "We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin.", "This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin. This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ).", "The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun\"s P2P API). We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG )", "We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun\"s P2P API). We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin.", "We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin. This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ).", "This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin. This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ). The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.", "The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun\"s P2P API). We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin.", "We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun\"s P2P API). We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin. This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ).", "We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin. This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ). The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.", "The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun\"s P2P API). We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin. This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ).", "We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun\"s P2P API). We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #CITATION_TAG ) It is our intention to implement our distributed corpus annotation framework as a plugin. This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ). The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it."], "CC1532": ["The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Existing tagging systems are \"small scale\" and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "Existing tagging systems are \"small scale\" and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "Existing tagging systems are \"small scale\" and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #CITATION_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)."], "CC1533": ["The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Existing tagging systems are \"small scale\" and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "Existing tagging systems are \"small scale\" and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "Existing tagging systems are \"small scale\" and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #CITATION_TAG ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)."], "CC1534": ["#CITATION_TAG built a corpus by iteratively searching Google for a small set of seed terms . Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. #CITATION_TAG built a corpus by iteratively searching Google for a small set of seed terms . Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. #CITATION_TAG built a corpus by iteratively searching Google for a small set of seed terms . Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)."], "CC1536": ["Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #CITATION_TAG )", "In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #CITATION_TAG )", "Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #CITATION_TAG ) This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.", "In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #CITATION_TAG ) This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.", "Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #CITATION_TAG ) This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).", "In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #CITATION_TAG ) This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).", "Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #CITATION_TAG ) This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text."], "CC1537": ["The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.", "A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.", "The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.", "The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.", "A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.", "The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word\"s grammatical and collocational behaviour.", "archive.org) or build their own collections from AltaVista search engine results. The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.", "The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.", "A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word\"s grammatical and collocational behaviour.", "The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word\"s grammatical and collocational behaviour. Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell).", "archive.org) or build their own collections from AltaVista search engine results. The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.", "The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word\"s grammatical and collocational behaviour.", "A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word\"s grammatical and collocational behaviour. Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell).", "archive.org) or build their own collections from AltaVista search engine results. The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word\"s grammatical and collocational behaviour.", "The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #CITATION_TAG ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word\"s grammatical and collocational behaviour. Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell)."], "CC1538": ["Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #CITATION_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages ."], "CC1539": ["The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Existing tagging systems are \ufffdsmall scale\ufffd and typically impose some limitation to prevent over- load (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\ufffds Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).", "Existing tagging systems are \ufffdsmall scale\ufffd and typically impose some limitation to prevent over- load (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\ufffds Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).", "Existing tagging systems are \ufffdsmall scale\ufffd and typically impose some limitation to prevent over- load (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #CITATION_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) . Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\ufffds Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003)."], "CC1540": ["#CITATION_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler . Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Studies have used several different methods to mine web data. #CITATION_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler . Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "#CITATION_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler . Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. #CITATION_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler . Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "Studies have used several different methods to mine web data. #CITATION_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler . Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).", "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. #CITATION_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler . Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.", "The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. #CITATION_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler . Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist\"s Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003)."], "CC1541": ["Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #CITATION_TAGa ) . Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.", "Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust. Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #CITATION_TAGa ) . Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.", "We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general. Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust. Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #CITATION_TAGa ) . Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here."], "CC1542": ["In addition , the advantages of using linguistically annotated data over raw data are well documented ( #CITATION_TAG ; Granger and Rayson , 1998 ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.", "This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( #CITATION_TAG ; Granger and Rayson , 1998 ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.", "In addition , the advantages of using linguistically annotated data over raw data are well documented ( #CITATION_TAG ; Granger and Rayson , 1998 ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.", "Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( #CITATION_TAG ; Granger and Rayson , 1998 ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.", "This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( #CITATION_TAG ; Granger and Rayson , 1998 ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.", "In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( #CITATION_TAG ; Granger and Rayson , 1998 ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.", "Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented ( #CITATION_TAG ; Granger and Rayson , 1998 ) . As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus."], "CC1544": ["We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #CITATION_TAG ) . Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001)."], "CC1546": ["Following the example of #CITATION_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon \" ) , a word coined by Roland Barthes ( 1970 ) . Consequently, a hypertext is a set of lexias.", "Following the example of #CITATION_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon \" ) , a word coined by Roland Barthes ( 1970 ) . Consequently, a hypertext is a set of lexias. In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational."], "CC1547": ["For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.", "Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.", "For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962).", "For example, when books shouldn\"t be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (Eisenstein, 1983). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.", "Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962).", "For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness.", "When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example, when books shouldn\"t be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (Eisenstein, 1983). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.", "For example, when books shouldn\"t be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (Eisenstein, 1983). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962).", "Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness.", "For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.", "When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example, when books shouldn\"t be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (Eisenstein, 1983). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962).", "For example, when books shouldn\"t be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (Eisenstein, 1983). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness.", "Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.", "When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example, when books shouldn\"t be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (Eisenstein, 1983). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness.", "For example, when books shouldn\"t be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (Eisenstein, 1983). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999). For example , a ` web page \" is more similar to an infinite canvas than a written page ( #CITATION_TAG ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing."], "CC1548": ["Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #CITATION_TAG ) .", "We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004). Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #CITATION_TAG ) ."], "CC1550": ["Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain.", "The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain.", "Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain.", "In the second one, the last user, who has edited the lexia, may claim the attribution for himself. The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain.", "The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain.", "Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain. If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author\"s work.", "In the first instance, the edited version simply moves ahead the document history. In the second one, the last user, who has edited the lexia, may claim the attribution for himself. The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain.", "In the second one, the last user, who has edited the lexia, may claim the attribution for himself. The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain.", "The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain. If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author\"s work.", "Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain. If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author\"s work. So as to come to terms with this idea, we need a concept invented by Nelson (1992), i.e. transclusion.", "In the first instance, the edited version simply moves ahead the document history. In the second one, the last user, who has edited the lexia, may claim the attribution for himself. The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain.", "In the second one, the last user, who has edited the lexia, may claim the attribution for himself. The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain. If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author\"s work.", "The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain. If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author\"s work. So as to come to terms with this idea, we need a concept invented by Nelson (1992), i.e. transclusion.", "In the first instance, the edited version simply moves ahead the document history. In the second one, the last user, who has edited the lexia, may claim the attribution for himself. The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain. If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author\"s work.", "In the second one, the last user, who has edited the lexia, may claim the attribution for himself. The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #CITATION_TAG ) . If nobody claims the document for himself, it will fall in the public domain. The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain. If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author\"s work. So as to come to terms with this idea, we need a concept invented by Nelson (1992), i.e. transclusion."], "CC1551": ["While wikis have spread from a detailed design ( #CITATION_TAG ) , unfortunately blogs have not been designed under a model . So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal."], "CC1552": ["We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #CITATION_TAG ) .", "We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily. We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #CITATION_TAG ) ."], "CC1553": [], "CC1554": ["The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #CITATION_TAG ) . Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.", "The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #CITATION_TAG ) . Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology. So blogs are a literary metagenre which started as authored personal diaries or journals.", "The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #CITATION_TAG ) . Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology. So blogs are a literary metagenre which started as authored personal diaries or journals. Now they try to collect themselves in so-called \"blogspheres\"."], "CC1556": ["1.1 Hypertext as a New Writing Space #CITATION_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.", "1.1 Hypertext as a New Writing Space #CITATION_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing. Terms as \"chapter page\" or \"footnote\" simply become meaningless in the new texts, or they highly change their meaning.", "1.1 Hypertext as a New Writing Space #CITATION_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing. Terms as \"chapter page\" or \"footnote\" simply become meaningless in the new texts, or they highly change their meaning. When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose."], "CC1557": ["In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #CITATION_TAG ) ."], "CC1558": ["Every arc always has a definite direction , i.e. arcs are arrows ( #CITATION_TAG ) .", "Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs. Every arc always has a definite direction , i.e. arcs are arrows ( #CITATION_TAG ) ."], "CC1559": [], "CC1560": ["Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #CITATION_TAG ) . Figure 1 shows the model.", "In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself. Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #CITATION_TAG ) . Figure 1 shows the model.", "Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #CITATION_TAG ) . Figure 1 shows the model. History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.", "In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself. Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #CITATION_TAG ) . Figure 1 shows the model. History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.", "Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #CITATION_TAG ) . Figure 1 shows the model. History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp. Consequently, except in the case of sandboxes, every change in the document cannot be erased.", "In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself. Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #CITATION_TAG ) . Figure 1 shows the model. History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp. Consequently, except in the case of sandboxes, every change in the document cannot be erased.", "Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #CITATION_TAG ) . Figure 1 shows the model. History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp. Consequently, except in the case of sandboxes, every change in the document cannot be erased. This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself."], "CC1561": ["Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing", "From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing", "Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.", "From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing", "From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.", "Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well. This situation could make new problems rise up: Who owns the text?", "Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing", "From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.", "From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well. This situation could make new problems rise up: Who owns the text?", "Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well. This situation could make new problems rise up: Who owns the text? Which role is suitable for authors?", "Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.", "From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well. This situation could make new problems rise up: Who owns the text?", "From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well. This situation could make new problems rise up: Who owns the text? Which role is suitable for authors?", "Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well. This situation could make new problems rise up: Who owns the text?", "From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an \"opera aperta\" (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis ( #CITATION_TAG ) emphasize annotation , comment , and strong editing They give more power to readers, eventually filling the gap -the so-called active readers become authors as well. This situation could make new problems rise up: Who owns the text? Which role is suitable for authors?"], "CC1562": ["The paradigm is \"write many , read many\" ( #CITATION_TAG ) .", "Generally, people avoid commenting, preferring to edit each document. The paradigm is \"write many , read many\" ( #CITATION_TAG ) .", "In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself. Generally, people avoid commenting, preferring to edit each document. The paradigm is \"write many , read many\" ( #CITATION_TAG ) ."], "CC1564": ["This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #CITATION_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B \u00c2\u00a8 ohmov \u00c2\u00b4 a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( D\u00cb\\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.", "A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #CITATION_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B \u00c2\u00a8 ohmov \u00c2\u00b4 a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( D\u00cb\\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.", "This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #CITATION_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B \u00c2\u00a8 ohmov \u00c2\u00b4 a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( D\u00cb\\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Mart\u00ed Anton\u00edn, 2002), although this cannot be explained by a high proportion of non-projective structures.", "A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #CITATION_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B \u00c2\u00a8 ohmov \u00c2\u00b4 a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( D\u00cb\\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Mart\u00ed Anton\u00edn, 2002), although this cannot be explained by a high proportion of non-projective structures.", "This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #CITATION_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B \u00c2\u00a8 ohmov \u00c2\u00b4 a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( D\u00cb\\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Mart\u00ed Anton\u00edn, 2002), although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features."], "CC1565": ["\u00e2\\x80\u00a2 Graph transformations for recovering nonprojective structures ( #CITATION_TAG ) .", "\u2022 Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). \u00e2\\x80\u00a2 Graph transformations for recovering nonprojective structures ( #CITATION_TAG ) .", "\u2022 History-based feature models for predicting the next parser action (Black et al., 1992). \u2022 Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). \u00e2\\x80\u00a2 Graph transformations for recovering nonprojective structures ( #CITATION_TAG ) ."], "CC1567": [], "CC1568": ["Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #CITATION_TAG ) . Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.", "If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #CITATION_TAG ) . Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.", "before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #CITATION_TAG ) . Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances."], "CC1569": ["Typical examples are Bulgarian ( #CITATION_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) . Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.", "If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( #CITATION_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) . Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.", "before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( #CITATION_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) . Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances."], "CC1570": ["Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #CITATION_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) . Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.", "If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #CITATION_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) . Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.", "before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #CITATION_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) . Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances."], "CC1571": ["\u00e2\\x80\u00a2 History-based feature models for predicting the next parser action ( #CITATION_TAG ) . \u2022 Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).", "\u2022 A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006). \u00e2\\x80\u00a2 History-based feature models for predicting the next parser action ( #CITATION_TAG ) . \u2022 Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).", "\u00e2\\x80\u00a2 History-based feature models for predicting the next parser action ( #CITATION_TAG ) . \u2022 Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). \u2022 Graph transformations for recovering nonprojective structures ."], "CC1572": [], "CC1573": ["More specifically , we use LIBSVM ( #CITATION_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification . Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components.", "We use support vector machines to predict the next parser action from a feature vector representing the history. More specifically , we use LIBSVM ( #CITATION_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification . Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components.", "More specifically , we use LIBSVM ( #CITATION_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification . Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4 or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003).", "We use support vector machines to predict the next parser action from a feature vector representing the history. More specifically , we use LIBSVM ( #CITATION_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification . Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4 or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003).", "More specifically , we use LIBSVM ( #CITATION_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification . Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4 or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003). To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t."], "CC1574": [], "CC1575": [], "CC1578": ["Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish . Japanese ( #CITATION_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .", "If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish . Japanese ( #CITATION_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .", "before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish . Japanese ( #CITATION_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances ."], "CC1579": ["Support vector machines for mapping histories to parser actions ( #CITATION_TAG ) . \u2022 Graph transformations for recovering nonprojective structures .", "\u2022 History-based feature models for predicting the next parser action (Black et al., 1992). Support vector machines for mapping histories to parser actions ( #CITATION_TAG ) . \u2022 Graph transformations for recovering nonprojective structures ."], "CC1580": ["By contrast , Turkish ( Oflazer et al. , 2003 ; #CITATION_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 )", "The results for Arabic (Haji\u010d et al., 2004;Smr\u017e et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast , Turkish ( Oflazer et al. , 2003 ; #CITATION_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 )", "By contrast , Turkish ( Oflazer et al. , 2003 ; #CITATION_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) It is noteworthy that Arabic and Turkish, being \"typological outliers show patterns that are different both from each other and from most of the other languages."], "CC1581": ["#CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. #CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "#CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. #CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. #CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "#CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. #CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. #CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. #CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. #CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. #CITATION_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well."], "CC1582": ["For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).", "For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).", "Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research.", "For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research.", "Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , #CITATION_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score . found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005)."], "CC1583": ["#CITATION_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space . Each sentence/vector is then mul- tiplied by this matrix, and new HMM models are re-computed from the projected data."], "CC1584": ["Such a component would serve as the first stage of a clinical question answering system ( #CITATION_TAG ) or summarization system ( McKeown et al. , 2003 )", "Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #CITATION_TAG ) or summarization system ( McKeown et al. , 2003 )", "Such a component would serve as the first stage of a clinical question answering system ( #CITATION_TAG ) or summarization system ( McKeown et al. , 2003 ) We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.", "The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #CITATION_TAG ) or summarization system ( McKeown et al. , 2003 )", "Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #CITATION_TAG ) or summarization system ( McKeown et al. , 2003 ) We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.", "In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled. The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #CITATION_TAG ) or summarization system ( McKeown et al. , 2003 )", "The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #CITATION_TAG ) or summarization system ( McKeown et al. , 2003 ) We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured."], "CC1585": ["Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #CITATION_TAG ) . However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.", "Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #CITATION_TAG ) . However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing. Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.", "Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #CITATION_TAG ) . However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing. Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training. Since HMMs are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach."], "CC1586": [], "CC1587": ["The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #CITATION_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990)."], "CC1588": ["Using the section labels , the HMM was trained using the HTK toolkit ( #CITATION_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation . For testing, we performed a Viterbi (maximum likelihood) estimation of the label of each test sentence/vector (also using the HTK toolkit).", "The output probabilities were modeled as four-dimensional Gaussians mixtures with diagonal covariance matrices. Using the section labels , the HMM was trained using the HTK toolkit ( #CITATION_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation . For testing, we performed a Viterbi (maximum likelihood) estimation of the label of each test sentence/vector (also using the HTK toolkit).", "The transition probability matrix of the HMM was initialized with uniform probabilities over a fully connected graph. The output probabilities were modeled as four-dimensional Gaussians mixtures with diagonal covariance matrices. Using the section labels , the HMM was trained using the HTK toolkit ( #CITATION_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation . For testing, we performed a Viterbi (maximum likelihood) estimation of the label of each test sentence/vector (also using the HTK toolkit)."], "CC1589": ["As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction\"  , \"methods\"  , \"results\"  , and \"conclusions\"  ( SalangerMeyer , 1990 ; #CITATION_TAG ; Or\u00cb\\x98asan , 2001 ) . The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction\"  , \"methods\"  , \"results\"  , and \"conclusions\"  ( SalangerMeyer , 1990 ; #CITATION_TAG ; Or\u00cb\\x98asan , 2001 ) . The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction\"  , \"methods\"  , \"results\"  , and \"conclusions\"  ( SalangerMeyer , 1990 ; #CITATION_TAG ; Or\u00cb\\x98asan , 2001 ) . The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction\"  , \"methods\"  , \"results\"  , and \"conclusions\"  ( SalangerMeyer , 1990 ; #CITATION_TAG ; Or\u00cb\\x98asan , 2001 ) . The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction\"  , \"methods\"  , \"results\"  , and \"conclusions\"  ( SalangerMeyer , 1990 ; #CITATION_TAG ; Or\u00cb\\x98asan , 2001 ) . The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction\"  , \"methods\"  , \"results\"  , and \"conclusions\"  ( SalangerMeyer , 1990 ; #CITATION_TAG ; Or\u00cb\\x98asan , 2001 ) . The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction\"  , \"methods\"  , \"results\"  , and \"conclusions\"  ( SalangerMeyer , 1990 ; #CITATION_TAG ; Or\u00cb\\x98asan , 2001 ) . The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. Demner-Fushman et al. (2005) found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts."], "CC1590": ["An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #CITATION_TAG )", "An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #CITATION_TAG ) This technique provides two important advantages.", "An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #CITATION_TAG ) This technique provides two important advantages. First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics."], "CC1591": ["Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #CITATION_TAG ; Ng and Jordan , 2001 ) . However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.", "Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #CITATION_TAG ; Ng and Jordan , 2001 ) . However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing. Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.", "Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #CITATION_TAG ; Ng and Jordan , 2001 ) . However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing. Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training. Since HMMs are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach."], "CC1592": ["Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.", "The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.", "The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #CITATION_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well."], "CC1594": ["The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.", "The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.", "For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #CITATION_TAG ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques."], "CC1595": ["Our task is closer to the work of #CITATION_TAG , who looked at the problem of intellectual attribution in scientific texts ."], "CC1596": ["Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #CITATION_TAG )", "Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #CITATION_TAG )", "Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #CITATION_TAG ) We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.", "The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #CITATION_TAG )", "Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #CITATION_TAG ) We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.", "In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled. The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #CITATION_TAG )", "The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #CITATION_TAG ) We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured."], "CC1598": ["The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #CITATION_TAG ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990)."], "CC1600": ["The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).", "Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.", "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction methods results and \"conclusions\" (Salanger-Meyer, 1990;Swales, 1990;Or\u0203san, 2001). The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #CITATION_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering . Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990)."], "CC1601": ["The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001", "Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001", "The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences.", "Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001", "Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences.", "The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences. Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.", "The results of our second set of experiments (with RCTs only) are shown in Tables 2(a) and 2(b). Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001", "Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences.", "Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences. Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.", "The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences. Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper. Nevertheless, our HMM- based approach is at least competitive with SVMs, perhaps better in some cases.", "The results of our second set of experiments (with RCTs only) are shown in Tables 2(a) and 2(b). Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences.", "Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences. Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.", "Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences. Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper. Nevertheless, our HMM- based approach is at least competitive with SVMs, perhaps better in some cases.", "The results of our second set of experiments (with RCTs only) are shown in Tables 2(a) and 2(b). Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences. Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.", "Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #CITATION_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences. Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper. Nevertheless, our HMM- based approach is at least competitive with SVMs, perhaps better in some cases."], "CC1603": ["Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #CITATION_TAG ) . (Barzilay and Lee, 2004).", "McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #CITATION_TAG ) . (Barzilay and Lee, 2004).", "Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #CITATION_TAG ) . (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.", "Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #CITATION_TAG ) . (Barzilay and Lee, 2004).", "McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #CITATION_TAG ) . (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #CITATION_TAG ) . (Barzilay and Lee, 2004).", "Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #CITATION_TAG ) . (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well."], "CC1604": ["This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research.", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).", "For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research.", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research.", "For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).", "For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #CITATION_TAG ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."], "CC1605": ["The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.", "The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.", "For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.", "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990). For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #CITATION_TAG ; Ely et al. , 2005 ) . Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques."], "CC1606": ["Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.", "The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.", "The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004).", "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians\" needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #CITATION_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well."], "CC1607": ["Although not the first to employ a generative approach to directly model content , the seminal work of #CITATION_TAG is a noteworthy point of reference and comparison . However, our study differs in several important respects.", "Although not the first to employ a generative approach to directly model content , the seminal work of #CITATION_TAG is a noteworthy point of reference and comparison . However, our study differs in several important respects. Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.", "Although not the first to employ a generative approach to directly model content , the seminal work of #CITATION_TAG is a noteworthy point of reference and comparison . However, our study differs in several important respects. Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques. In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach."], "CC1608": ["Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #CITATION_TAG ) .", "This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training. Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #CITATION_TAG ) .", "First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics. This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training. Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #CITATION_TAG ) ."], "CC1609": ["We work with a semi-technical text on meteorological phenomena ( #CITATION_TAG ) , meant for primary school students . The text gradually introduces concepts related to precipitation, and explains them.", "We work with a semi-technical text on meteorological phenomena ( #CITATION_TAG ) , meant for primary school students . The text gradually introduces concepts related to precipitation, and explains them. Its nature makes it appropriate for the semantic analysis task in an incremental approach."], "CC1610": ["Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #CITATION_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) . Lists of semantic relations are designed to capture salient domain information.", "In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #CITATION_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) . Lists of semantic relations are designed to capture salient domain information."], "CC1611": ["This idea was inspired by #CITATION_TAG , who used a list of arguments surrounding the main verb together with the verb \"s subcategorization information and previously processed examples to analyse semantic roles ( case relations ) . In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005)."], "CC1612": ["Most approaches rely on VerbNet ( #CITATION_TAG ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) ."], "CC1614": ["In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #CITATION_TAG ) or the system ( Gomez , 1998 ) . Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).", "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998). In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #CITATION_TAG ) or the system ( Gomez , 1998 ) . Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).", "In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #CITATION_TAG ) or the system ( Gomez , 1998 ) . Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information."], "CC1615": ["Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #CITATION_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) . Lists of semantic relations are designed to capture salient domain information.", "In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #CITATION_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) . Lists of semantic relations are designed to capture salient domain information."], "CC1617": ["In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #CITATION_TAG ) . Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).", "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998). In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #CITATION_TAG ) . Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).", "In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #CITATION_TAG ) . Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information."], "CC1618": ["He was a grammarian who analysed Sanskrit ( #CITATION_TAG ) . The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968).", "The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 He was a grammarian who analysed Sanskrit ( #CITATION_TAG ) . The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968).", "He was a grammarian who analysed Sanskrit ( #CITATION_TAG ) . The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005).", "This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 He was a grammarian who analysed Sanskrit ( #CITATION_TAG ) . The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968).", "The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 He was a grammarian who analysed Sanskrit ( #CITATION_TAG ) . The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005).", "When analysing texts, it is essential to see how elements of meaning are interconnected. This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 He was a grammarian who analysed Sanskrit ( #CITATION_TAG ) . The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968).", "This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 He was a grammarian who analysed Sanskrit ( #CITATION_TAG ) . The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005)."], "CC1619": ["Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #CITATION_TAG ; Shi and Mihalcea , 2005 ) ."], "CC1621": ["Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #CITATION_TAG ) ."], "CC1622": ["The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #CITATION_TAG ) . The parser, written in Prolog, implements a classic constituency English grammar from Quirk et al. (1985).", "The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #CITATION_TAG ) . The parser, written in Prolog, implements a classic constituency English grammar from Quirk et al. (1985). Pairs of syntactic units connected by grammatical relations are extracted from the parse trees."], "CC1623": ["Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #CITATION_TAG ) ) . Lists of semantic relations are designed to capture salient domain information.", "In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #CITATION_TAG ) ) . Lists of semantic relations are designed to capture salient domain information."], "CC1624": ["The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #CITATION_TAG ; Fillmore , 1968 ) . Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).", "He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #CITATION_TAG ; Fillmore , 1968 ) . Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).", "The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century B.C. and the work of Panini 1 He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #CITATION_TAG ; Fillmore , 1968 ) . Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)."], "CC1625": ["The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #CITATION_TAG ) . Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).", "He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #CITATION_TAG ) . Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).", "The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century B.C. and the work of Panini 1 He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #CITATION_TAG ) . Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)."], "CC1626": ["This design idea was adopted from TANKA ( #CITATION_TAGb ) . The only manually encoded knowledge is a dictionary of markers (subordinators, coordinators, prepositions).", "Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text. This design idea was adopted from TANKA ( #CITATION_TAGb ) . The only manually encoded knowledge is a dictionary of markers (subordinators, coordinators, prepositions).", "This design idea was adopted from TANKA ( #CITATION_TAGb ) . The only manually encoded knowledge is a dictionary of markers (subordinators, coordinators, prepositions). This resource does not affect the syntacticsemantic graph-matching heuristic."], "CC1627": ["Such systems extract information from some types of syntactic units ( clauses in ( #CITATION_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) . Lists of semantic relations are designed to capture salient domain information.", "In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units ( clauses in ( #CITATION_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) . Lists of semantic relations are designed to capture salient domain information."], "CC1628": ["Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #CITATION_TAG ) . In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).", "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #CITATION_TAG ) . In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002))."], "CC1629": ["The list of semantic relations with which we work is based on extensive literature study ( #CITATION_TAGa ) . Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena.", "The list of semantic relations with which we work is based on extensive literature study ( #CITATION_TAGa ) . Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena. The resulting list is the one used in the experiments we present in this paper.", "The list of semantic relations with which we work is based on extensive literature study ( #CITATION_TAGa ) . Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena. The resulting list is the one used in the experiments we present in this paper. The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier)."], "CC1630": ["Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #CITATION_TAG ) , we tried to adapt the same approach to the German-English language pair", "Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #CITATION_TAG ) , we tried to adapt the same approach to the German-English language pair", "Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #CITATION_TAG ) , we tried to adapt the same approach to the German-English language pair It turned out that there is a larger variety of long reordering patterns in this case.", "Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #CITATION_TAG ) , we tried to adapt the same approach to the German-English language pair It turned out that there is a larger variety of long reordering patterns in this case.", "Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #CITATION_TAG ) , we tried to adapt the same approach to the German-English language pair It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after the official evaluation showed promising results.", "Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #CITATION_TAG ) , we tried to adapt the same approach to the German-English language pair It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after the official evaluation showed promising results.", "Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #CITATION_TAG ) , we tried to adapt the same approach to the German-English language pair It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after the official evaluation showed promising results. We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice."], "CC1631": ["Future research should apply the work of #CITATION_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .", "This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #CITATION_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."], "CC1632": ["Future research should apply the work of Blunsom et al. ( 2008 ) and #CITATION_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .", "This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of Blunsom et al. ( 2008 ) and #CITATION_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."], "CC1633": ["In our prior work ( #CITATION_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review . While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model."], "CC1634": ["( Details of how the average-expert model performs can be found in our prior work ( #CITATION_TAG ) . )", "Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ( Details of how the average-expert model performs can be found in our prior work ( #CITATION_TAG ) . )", "To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below. Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ( Details of how the average-expert model performs can be found in our prior work ( #CITATION_TAG ) . )"], "CC1635": ["We follow our previous work ( #CITATION_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) . Each word is broken down into: stem, affixes, stem POS, and affixes POS.", "In selecting features for Korean, we have to ac- count for relatively free word order (Chung et al., 2010). We follow our previous work ( #CITATION_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) . Each word is broken down into: stem, affixes, stem POS, and affixes POS.", "We follow our previous work ( #CITATION_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) . Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.", "In selecting features for Korean, we have to ac- count for relatively free word order (Chung et al., 2010). We follow our previous work ( #CITATION_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) . Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.", "We follow our previous work ( #CITATION_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) . Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties. Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4)."], "CC1636": ["The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #CITATION_TAG ) . (Schmitt et al., 2009).", "The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #CITATION_TAG ) . (Schmitt et al., 2009). From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events."], "CC1637": ["This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #CITATION_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) . There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993).", "The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #CITATION_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) . There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993).", "Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #CITATION_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) . There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993)."], "CC1638": [], "CC1639": ["It is inspired by the system described in #CITATION_TAG . Because NER annotations are commonly not nested (for example, in the text \"the US Army US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.", "In this section we describe in detail the baseline NER system we use. It is inspired by the system described in #CITATION_TAG . Because NER annotations are commonly not nested (for example, in the text \"the US Army US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.", "It is inspired by the system described in #CITATION_TAG . Because NER annotations are commonly not nested (for example, in the text \"the US Army US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity. Following Ratinov and Roth (2009) we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity."], "CC1640": ["This choice is inspired by recent work on learning syntactic categories ( #CITATION_TAG ) , which successfully utilized such language models to represent word window contexts of target words . However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme.", "To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories ( #CITATION_TAG ) , which successfully utilized such language models to represent word window contexts of target words . However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme.", "This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories ( #CITATION_TAG ) , which successfully utilized such language models to represent word window contexts of target words . However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme."], "CC1641": ["Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #CITATION_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."], "CC1642": ["Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #CITATION_TAG , which deal with automatic generation of classic fill in the blank questions"], "CC1643": ["Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #CITATION_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."], "CC1644": ["Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #CITATION_TAG ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."], "CC1645": ["Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 )", "3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 )", "Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.", "Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 )", "3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.", "Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4.", "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 )", "Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.", "3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4.", "Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.", "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.", "Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4.", "3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.", "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4.", "Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #CITATION_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized."], "CC1646": ["raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 )", "Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 )", "raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.", "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 )", "Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.", "raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4.", "Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 )", "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.", "Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4.", "raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.", "Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.", "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4.", "Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.", "Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4.", "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #CITATION_TAG ) and create multiple features for length using a decision tree ( J48 ) We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized."], "CC1647": ["The only disambiguation metric that we used in our previous work ( #CITATION_TAGb ) was the shape-based metric , according to which the `` best \"\" trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process.", "The shape-based metric. The only disambiguation metric that we used in our previous work ( #CITATION_TAGb ) was the shape-based metric , according to which the `` best \"\" trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process.", "The only disambiguation metric that we used in our previous work ( #CITATION_TAGb ) was the shape-based metric , according to which the `` best \"\" trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process. In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.", "The shape-based metric. The only disambiguation metric that we used in our previous work ( #CITATION_TAGb ) was the shape-based metric , according to which the `` best \"\" trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process. In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.", "The only disambiguation metric that we used in our previous work ( #CITATION_TAGb ) was the shape-based metric , according to which the `` best \"\" trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process. In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels. I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches.", "The shape-based metric. The only disambiguation metric that we used in our previous work ( #CITATION_TAGb ) was the shape-based metric , according to which the `` best \"\" trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process. In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels. I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches.", "The only disambiguation metric that we used in our previous work ( #CITATION_TAGb ) was the shape-based metric , according to which the `` best \"\" trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process. In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels. I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches. According to the shape-based metric, we consider that a discourse tree A is \"better\" than another discourse tree B if A is more skewed to the right than B (see Marcu (1997c) for a mathematical formulation of the notion of skewedness)."]}