unique_id	context	label	input_context	citance
CC1958	Tuberous sclerosis complex (TSC) is a neurocutaneous disorder caused by mutations in either of the two tumor suppressor genes TSC1 or TSC2, encoding hamartin and tuberin, respectively (van Slegtenhorst et al, 1997;European Chromosome 16 Tuberous Sclerosis Consortium, 1993). Typical TSC lesions include hypomelanic macules and facial angiofibromas, as well as brain cortical tubers, subependymal nodules, and subependymal giant cell astrocytomas (SEGAs) (Holmes et al, 2007;Curatolo et al, 2008;Sahin, 2012). In addition to manifestations in the skin and nervous system, TSC is associated with hallmark tumors in the kidney, lung, heart and liver such as angiomyolipomas, lymphangioleiomyomatosis, and rhabdomyomas (Crino et al, 2006;Curatolo et al, 2008;Orlova and Crino, 2010; #CITATION_TAG, 2013).	0	Tuberous sclerosis complex (TSC) is a neurocutaneous disorder caused by mutations in either of the two tumor suppressor genes TSC1 or TSC2, encoding hamartin and tuberin, respectively (van Slegtenhorst et al, 1997;European Chromosome 16 Tuberous Sclerosis Consortium, 1993). Typical TSC lesions include hypomelanic macules and facial angiofibromas, as well as brain cortical tubers, subependymal nodules, and subependymal giant cell astrocytomas (SEGAs) (Holmes et al, 2007;Curatolo et al, 2008;Sahin, 2012). In addition to manifestations in the skin and nervous system, TSC is associated with hallmark tumors in the kidney, lung, heart and liver such as angiomyolipomas, lymphangioleiomyomatosis, and rhabdomyomas (Crino et al, 2006;Curatolo et al, 2008;Orlova and Crino, 2010; #CITATION_TAG, 2013).	 
CC2088	"These questions about social memory have received detailed consideration within archaeology over the last 20 years (e.g. Boric��, 2010;#CITATION_TAG, 2002;Jones, 2007;Whittle et al., 2007b). This article reviews parts of that debate with reference to the particular problems of memory and natural places. One of the most profitable results of these works has been to shift discussions around remembering from a focus on discursive knowledge, where the only imaginable mechanism of transmission in prehistory is the oral poetic tradition, to embodied experience. An important influence here has been Paul Connerton""s 1989 book, How Societies Remember. Connerton (1989: 21-25) begins by analysing memory claims in general by dividing them into three broad classes. Firstly, there are personal memory claims, memories which ""take as their object one""s life history "" (1989: 22). Secondly, there are cognitive memory claims which relate to things one has met, learned of or experienced in the past. A typical example would be abstract knowledge, such as the meaning of words. One of the identifying characteristics of this kind of memory claim for Connerton is that you do not need to remember when you learnt something to make use of it. Finally there is habit-memory, the ability to reproduce a physical performance; Connerton (1989: 22) uses the example of remembering how to ride a bicycle. Jones (2007: 7-12) has reviewed different metaphors for how personal memory claims, Connerton""s first class, might work. He rejects metaphors of memory as storage in which the brain is variously conceived as a storehouse, library or encyclopaedia of finite capacity. Jones (2007: 9) regards this vision of memory as problematic for two reasons. Firstly, it relies on a model of memories as objective ""lumps"" of data which are not interpreted in any way by the mind storing them. Secondly, and arising from that characterisation, the authenticity of knowledge is solely derived from the accuracy or otherwise of the mind""s recall of these objective memories. Jones follows Clark (1997) in viewing memory as a process of pattern re-creation which involves the mind, the body and the world. Clark views cognition as something which is created within the interaction of brain, body and world. For Jones, memory is just this kind of knowledge: one that is contextually specific, experiential and embodied. This analysis of personal memory claims is extremely persuasive. The rest of this article is primarily concerned with the slightly different case of the social transmission of memory, but it takes as its starting point Jones"" analysis of memory as a contextually specific, embodied and experiential phenomenon."	0	"These questions about social memory have received detailed consideration within archaeology over the last 20 years (e.g. Boric��, 2010;#CITATION_TAG, 2002;Jones, 2007;Whittle et al., 2007b). This article reviews parts of that debate with reference to the particular problems of memory and natural places. One of the most profitable results of these works has been to shift discussions around remembering from a focus on discursive knowledge, where the only imaginable mechanism of transmission in prehistory is the oral poetic tradition, to embodied experience. An important influence here has been Paul Connerton""s 1989 book, How Societies Remember."	T
CC1395	"The divergent features of the three scenarios are summarised in Table 2 #CITATION_TAG;However,achievingsignificant penetration(>20%ofthefleet)by 2050 implies very optimistic transition rates when compared withhistoricalprecedents [1] Adaptingthemodeltoexaminethe assumption that technologies compete in different segments of the vehicle market does lead to changes in the models"" technology choice.i.e.initialmodelresultsare sensitive to assumptions about socialpracticesinvehiclemarkets."	0	"The divergent features of the three scenarios are summarised in Table 2 #CITATION_TAG;However,achievingsignificant penetration(>20%ofthefleet)by 2050 implies very optimistic transition rates when compared withhistoricalprecedents [1] Adaptingthemodeltoexaminethe assumption that technologies compete in different segments of the vehicle market does lead to changes in the models"" technology choice.i.e.initialmodelresultsare sensitive to assumptions about socialpracticesinvehiclemarkets."	T
CC1521	Evidence suggests that SD patients become increasingly reliant on implicit, procedural memory processes as conceptual-semantic memory deteriorates (e.g. Graham, Simons, Pratt, . If syntactic processing relies on highly redundant, abstract, frequent lexico-syntactic information in combination with production processes with the characteristics of implicit learning, we might not expect to see gross syntactic problems in the conversational speech of patients with SD. Indeed, phrasal syntax was remarkably preserved in the face of substitutions errors for both open and closed class items. Anecdotal and clinical observations suggest that SD patients do fall back on routinized phrases, filler terms and a familiar vocabulary (Bird et al., 2000). This suggests that as the diseases progresses, syntax may become affected in a similar manner to lexico-semantics: a reduction in complexity and variation, and reliance on highly frequent and familiar information that is still available. This may not draw attention because of the nature of most normal conversational speech: the majority of utterances (around 70%) are simple, one-participant clauses and light verb use is very common (#CITATION_TAG & Hopper, 2001). The speech samples we used were taken from fairly unconstrained, informal interviews. The patients were not asked to produce particular structures or vocabulary, and would naturally have employed any strategies available that allowed them to speak fluently and normally. In other words, problems with complex syntax may not be apparent because patients -and people in generalrarely produce such structures in free speech.	1	Indeed, phrasal syntax was remarkably preserved in the face of substitutions errors for both open and closed class items. Anecdotal and clinical observations suggest that SD patients do fall back on routinized phrases, filler terms and a familiar vocabulary (Bird et al., 2000). This suggests that as the diseases progresses, syntax may become affected in a similar manner to lexico-semantics: a reduction in complexity and variation, and reliance on highly frequent and familiar information that is still available. This may not draw attention because of the nature of most normal conversational speech: the majority of utterances (around 70%) are simple, one-participant clauses and light verb use is very common (#CITATION_TAG & Hopper, 2001). The speech samples we used were taken from fairly unconstrained, informal interviews. The patients were not asked to produce particular structures or vocabulary, and would naturally have employed any strategies available that allowed them to speak fluently and normally. In other words, problems with complex syntax may not be apparent because patients -and people in generalrarely produce such structures in free speech.	 
CC1099	models of populations of neurons with properties similar to those found in the visual cortex have been shown to produce sparse responses to natural image inputs #CITATION_TAG. Equally, learning algorithms that seek to generate sparse responses to natural image samples produce units with receptive fields that are strikingly similar to those found in the visual cortex [5]. This suggests that sparse coding might, indeed, be a strategy used by the human visual system to maximize information transfer with minimum metabolic cost. It is important to note, however, that an encoding that produces sparse responses to natural images may respond non-sparsely to other inputs.	4	models of populations of neurons with properties similar to those found in the visual cortex have been shown to produce sparse responses to natural image inputs #CITATION_TAG. Equally, learning algorithms that seek to generate sparse responses to natural image samples produce units with receptive fields that are strikingly similar to those found in the visual cortex [5]. This suggests that sparse coding might, indeed, be a strategy used by the human visual system to maximize information transfer with minimum metabolic cost. It is important to note, however, that an encoding that produces sparse responses to natural images may respond non-sparsely to other inputs.	m
CC394	In the literature most authors distinguish between householdbased and non-household-based carpools which are also called internal and external carpools respectively (Buliung et al., 2010;Correia and Viegas, 2011;Ferguson, 1997a;Morency, 2007;#CITATION_TAG, 1987). This distinction is relevant for two reasons. First, members of the same household have their trip origin in common; as a result, no time is lost for picking up a passenger. Second, the level of trust is high between members of the same household and this is considered to be important in the formation of carpool clubs. Therefore, Correia and Viegas (2011) classify relations between carpool members on the basis of the level of trust which is assumed to be higher between members of the same household than between (in decreasing order of trust) friends, colleagues and unrelated persons.	5	In the literature most authors distinguish between householdbased and non-household-based carpools which are also called internal and external carpools respectively (Buliung et al., 2010;Correia and Viegas, 2011;Ferguson, 1997a;Morency, 2007;#CITATION_TAG, 1987). This distinction is relevant for two reasons. First, members of the same household have their trip origin in common; as a result, no time is lost for picking up a passenger. Second, the level of trust is high between members of the same household and this is considered to be important in the formation of carpool clubs.	I
CC29	Applying co-design to educational games has the potential to generate more effective educational outcomes. Yet, it represents a challenge as best practices to co-design such games, which require technical, conceptual and educational skills, still have to be defined (Carvalho et al., 2015;#CITATION_TAG et al., 2016). Most of the participatory models to design educational games are founded on educational theories and game design (see for example: Amory, 2007; Arnab et al., 2015). Some more recent models, though, have also included domain experts to define the learning content of the educational game (De Jans et al., 2017). In scenarios where the co-designers are not equally experienced with the educational game domain or with the design process, tools to facilitate the design and maximize educational outcomes may be required.	0	Applying co-design to educational games has the potential to generate more effective educational outcomes. Yet, it represents a challenge as best practices to co-design such games, which require technical, conceptual and educational skills, still have to be defined (Carvalho et al., 2015;#CITATION_TAG et al., 2016). Most of the participatory models to design educational games are founded on educational theories and game design (see for example: Amory, 2007; Arnab et al., 2015). Some more recent models, though, have also included domain experts to define the learning content of the educational game (De Jans et al., 2017). In scenarios where the co-designers are not equally experienced with the educational game domain or with the design process, tools to facilitate the design and maximize educational outcomes may be required.	e
CC100	"The research design was based on Patton""s [25] description of qualitative process evaluation and guidelines for the evaluation of complex interventions #CITATION_TAG. The aim of the study was to get in-depth knowledge of patients"" experience of health-related change, and empirical data was collected by means of qualitative interviews. The research was carried out in accordance with the Code of Ethics of the World Medical Association. With approval from the ethics committee of The South-Eastern Regional Health Authorities in Oslo, Norway, three clinical sites were chosen for the pilot-implementation of the intervention: a rehabilitation unit, an outpatient clinic, and a center for patient education, known as a ""Learning and Mastery Center"" (LMC). The three units were located in two general hospitals in Norway. A project team of health care personnel from the three sites (including five nurses, one physiotherapist, and three occupational therapists) was established. A researcher with a clinical background involving people with chronic illness led the project, but was not involved in the on-site delivery of BKP."	0	"The research design was based on Patton""s [25] description of qualitative process evaluation and guidelines for the evaluation of complex interventions #CITATION_TAG. The aim of the study was to get in-depth knowledge of patients"" experience of health-related change, and empirical data was collected by means of qualitative interviews. The research was carried out in accordance with the Code of Ethics of the World Medical Association. With approval from the ethics committee of The South-Eastern Regional Health Authorities in Oslo, Norway, three clinical sites were chosen for the pilot-implementation of the intervention: a rehabilitation unit, an outpatient clinic, and a center for patient education, known as a ""Learning and Mastery Center"" (LMC)."	T
CC352	As with computer OSs, the housekeeping program is abstract and general, yet its concrete implementation, resulting from billions of years of evolution, makes that several OSs may coexist, revealing again two kinds of information, information of the program and information of the context in which the program is expressed. This has considerable consequences for synthetic biology: cellular functions can be general and ubiquitous, whereas there is no reason why they should always be performed by structurally related objects. Overall, living cells display similar abstract features, and the genetic code argues for universality. Yet, Woese uncovered a significant discrepancy between two unicellular classes, the Archaea and the Bacteria (Woese et al. 1978). To identify ubiquitous functions operated by non-ubiquitous structures one had to devise an operational strategy, based on the concept of gene persistence (tendency of a given gene to be present in a quorum of species) (Fang et al. 2005). Different structural entities with common functions in different bacterial clades were indeed characterised (#CITATION_TAG 2009b;Woese 2002). A structure is therefore recruited for a particular function, dependent on the context in which it operates. The context creates the function.	0	Overall, living cells display similar abstract features, and the genetic code argues for universality. Yet, Woese uncovered a significant discrepancy between two unicellular classes, the Archaea and the Bacteria (Woese et al. 1978). To identify ubiquitous functions operated by non-ubiquitous structures one had to devise an operational strategy, based on the concept of gene persistence (tendency of a given gene to be present in a quorum of species) (Fang et al. 2005). Different structural entities with common functions in different bacterial clades were indeed characterised (#CITATION_TAG 2009b;Woese 2002). A structure is therefore recruited for a particular function, dependent on the context in which it operates. The context creates the function.	r
CC2021	In order to evaluate the relative effectiveness of heat and momentum transport, in the theory of heat transfer the Prandtl number Pr is defined as the ratio between the kinematic viscosity �_ and the thermal diffusion coefficient D: Pr = �_/D (#CITATION_TAG 2006). For gases, the transport coefficients for the transport of heat and momentum are nearly equal and the Prandtl number is of order unity, with Pr = 3 when �_ = 5/3 (Blundell & Blundell 2006). This suggests that a constraint on the particle AC parameter �� C i can be obtained by setting	0	In order to evaluate the relative effectiveness of heat and momentum transport, in the theory of heat transfer the Prandtl number Pr is defined as the ratio between the kinematic viscosity �_ and the thermal diffusion coefficient D: Pr = �_/D (#CITATION_TAG 2006). For gases, the transport coefficients for the transport of heat and momentum are nearly equal and the Prandtl number is of order unity, with Pr = 3 when �_ = 5/3 (Blundell & Blundell 2006). This suggests that a constraint on the particle AC parameter �� C i can be obtained by setting	I
CC1353	According to the model of holographic superconductivity proposed in [14], one can study strongly coupled s-wave superconductors, at a finite temperature and chemical potential, by considering a gravitational theory with an action which has a black hole solution. The black hole, in this case, is charged under a U(1) gauge field with a minimally coupled complex scalar field ��. The no hair theorem does not apply if the scalar field has a non-trivial coupling to the gauge field #CITATION_TAG. In this set up, the symmetry breaking in the bulk theory, which corresponds to a quantum phase transition to the superconducting phase in the boundary theory, is triggered by a position dependent negative mass squared formed from the gauge covariant derivative [16]. Its contribution becomes significant near the horizon of the black hole, thereby forcing the scalar field to condense.	0	According to the model of holographic superconductivity proposed in [14], one can study strongly coupled s-wave superconductors, at a finite temperature and chemical potential, by considering a gravitational theory with an action which has a black hole solution. The black hole, in this case, is charged under a U(1) gauge field with a minimally coupled complex scalar field ��. The no hair theorem does not apply if the scalar field has a non-trivial coupling to the gauge field #CITATION_TAG. In this set up, the symmetry breaking in the bulk theory, which corresponds to a quantum phase transition to the superconducting phase in the boundary theory, is triggered by a position dependent negative mass squared formed from the gauge covariant derivative [16]. Its contribution becomes significant near the horizon of the black hole, thereby forcing the scalar field to condense.	e
CC1145	"If the individual becomes actively operant in pursuing the goal, the situation is transformed into one similar to experimental task activity. Mind-wandering is then typically reduced, and activity in the default-mode network is attenuated, a finding that originally arose out of experimental manipulations leading to the discovery of that network as one whose activity rises spontaneously and regularly in the absence of work on a task (a ""resting state""; #CITATION_TAG et al., 2001)."	0	"If the individual becomes actively operant in pursuing the goal, the situation is transformed into one similar to experimental task activity. Mind-wandering is then typically reduced, and activity in the default-mode network is attenuated, a finding that originally arose out of experimental manipulations leading to the discovery of that network as one whose activity rises spontaneously and regularly in the absence of work on a task (a ""resting state""; #CITATION_TAG et al., 2001)."	i
CC2856	Godunov-type shallow water models are featured with the inherent ability to accommodate complex flow transitions within the numerical solution (Toro 2001, #CITATION_TAG 2003, Toro and Garc�_a-Navarro 2007. In recent years, they have received applied improvements and have been incorporated into water industry standard software (Lhomme et al. 2010), and used to support flood risk management (Nթelz and Pender 2010). In this context, applicable Godunov-type water wave models are at most second-order accurate and require a topography discretization technique and a wetting and drying condition (see Delis and Kampanis 2009 for a comprehensive review).	0	Godunov-type shallow water models are featured with the inherent ability to accommodate complex flow transitions within the numerical solution (Toro 2001, #CITATION_TAG 2003, Toro and Garc�_a-Navarro 2007. In recent years, they have received applied improvements and have been incorporated into water industry standard software (Lhomme et al. 2010), and used to support flood risk management (Nթelz and Pender 2010). In this context, applicable Godunov-type water wave models are at most second-order accurate and require a topography discretization technique and a wetting and drying condition (see Delis and Kampanis 2009 for a comprehensive review).	G
CC1226	Table 5 gives a summary of correlations found between expectancy motivation and academic performance. A meta-analysis of a range of studies recorded correlations varying between 0.38 and 0.5 (Brown et al. 2008). A number of studies identified self-efficacy as a useful predictor of academic performance (Brady-Amoon & Fuertes, 2011;Cassidy, 2011;Yusuf, 2011). Indirect relationships between self-efficacy and academic performance mediated either by other motivational factors or learning strategies are also cited (#CITATION_TAG, 2001;Yusuf, 2011). On the other hand, Pintrich & DeGroot (1990) found that self-efficacy was not significantly related to performance when cognitive engagement variables such as engagement in the learning process, self-regulation, and learning strategies were also considered, thereby concluding that self-efficacy facilitates cognitive engagement, but cognitive engagement itself is more directly linked to academic performance.	0	Table 5 gives a summary of correlations found between expectancy motivation and academic performance. A meta-analysis of a range of studies recorded correlations varying between 0.38 and 0.5 (Brown et al. 2008). A number of studies identified self-efficacy as a useful predictor of academic performance (Brady-Amoon & Fuertes, 2011;Cassidy, 2011;Yusuf, 2011). Indirect relationships between self-efficacy and academic performance mediated either by other motivational factors or learning strategies are also cited (#CITATION_TAG, 2001;Yusuf, 2011). On the other hand, Pintrich & DeGroot (1990) found that self-efficacy was not significantly related to performance when cognitive engagement variables such as engagement in the learning process, self-regulation, and learning strategies were also considered, thereby concluding that self-efficacy facilitates cognitive engagement, but cognitive engagement itself is more directly linked to academic performance.	i
CC298	"DiMaggio and Powell refer to these kinds of decisions as ""collectively rational as opposed to individually rational, because the rationality involved in the decisionmaking processes is influenced through information provided by others. Hence, institutionalism emphasises the persuasive control over the practices, beliefs and belief systems of individuals or organisations through an institution""s sway (Kimberley, 1979, in King et al., 1994. Persuasion can be achieved not only through directives, but also through more gentle but nevertheless potentially convincing means such as deploying specific knowledge, subsidising activities deemed ""appropriate"" by national government, standard-setting, raising awareness and generally promoting specific technologies (King et al., 1994). Moreover, Venkatraman et al. (1994) have stated that persuasion can occur both through vertical channels of communication (initiated by actors outside the set of potential adopters, such as central government; see also Leonard-Barton and Rogers, 1981;Moon and Bretschneider, 1997;Bobrowski and Bretschneider, 1994) as well as through processes of mimicking and ""word-of-mouth"" diffusion (Wang and Doong, 2010) involving communication, interaction and persuasion among potential adopters (DiMaggio and Powell, 1983). Innovation through mimicry is likely to occur when innovations are socially visible (Mahajan and Peterson, 1985;Dos Santos and Peffers, 1998), when causes, conditions and consequences are known (absence of causal ambiguity, Barney, 1991;#CITATION_TAG and Venkatraman, 1992) and when the success of the innovation is unlikely to be determined by path dependencies (Barney, 1991;Loh and Venkatraman, 1992)."	0	"Hence, institutionalism emphasises the persuasive control over the practices, beliefs and belief systems of individuals or organisations through an institution""s sway (Kimberley, 1979, in King et al., 1994. Persuasion can be achieved not only through directives, but also through more gentle but nevertheless potentially convincing means such as deploying specific knowledge, subsidising activities deemed ""appropriate"" by national government, standard-setting, raising awareness and generally promoting specific technologies (King et al., 1994). Moreover, Venkatraman et al. (1994) have stated that persuasion can occur both through vertical channels of communication (initiated by actors outside the set of potential adopters, such as central government; see also Leonard-Barton and Rogers, 1981;Moon and Bretschneider, 1997;Bobrowski and Bretschneider, 1994) as well as through processes of mimicking and ""word-of-mouth"" diffusion (Wang and Doong, 2010) involving communication, interaction and persuasion among potential adopters (DiMaggio and Powell, 1983). Innovation through mimicry is likely to occur when innovations are socially visible (Mahajan and Peterson, 1985;Dos Santos and Peffers, 1998), when causes, conditions and consequences are known (absence of causal ambiguity, Barney, 1991;#CITATION_TAG and Venkatraman, 1992) and when the success of the innovation is unlikely to be determined by path dependencies (Barney, 1991;Loh and Venkatraman, 1992)."	v
CC1786	"This approach has recently been adopted by Bayesian theorists in the form of ""rational process models"" (Griffiths, Vul, & Sanborn, 2012;Sanborn, Griffiths, & Navarro, 2010;#CITATION_TAG, Griffiths, Feldman, & Sanborn, 2010). The proposed models use Monte Carlo algorithms (e.g., importance sampling, particle filters, Markov Chain Monte Carlo methods) which approximate Bayesian inference by sampling from a probability distribution. The predictions of rational process models can be degraded from the optimal Bayesian solution to suboptimal (i.e., human-like) performance by, for example, reducing the number of samples taken."	5	"This approach has recently been adopted by Bayesian theorists in the form of ""rational process models"" (Griffiths, Vul, & Sanborn, 2012;Sanborn, Griffiths, & Navarro, 2010;#CITATION_TAG, Griffiths, Feldman, & Sanborn, 2010). The proposed models use Monte Carlo algorithms (e.g., importance sampling, particle filters, Markov Chain Monte Carlo methods) which approximate Bayesian inference by sampling from a probability distribution. The predictions of rational process models can be degraded from the optimal Bayesian solution to suboptimal (i.e., human-like) performance by, for example, reducing the number of samples taken."	T
CC127	Oceanographic data (Figure 2) were gathered from two moorings deployed in Kongsfjorden and Rijpfjorden, respectively (see #CITATION_TAG for details). The moorings were located in the immediate vicinity (1 km apart) to the trawling grounds in both fjords, and have been in operation since 2002 as part of a long term monitoring program.	5	Oceanographic data (Figure 2) were gathered from two moorings deployed in Kongsfjorden and Rijpfjorden, respectively (see #CITATION_TAG for details). The moorings were located in the immediate vicinity (1 km apart) to the trawling grounds in both fjords, and have been in operation since 2002 as part of a long term monitoring program.	O
CC2758	Recent research has stressed the importance of network structures in understanding business exchanges (Achrol, 1997;M�_ller & Rajala, 2007). These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;#CITATION_TAG, 1985) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980). Business networks have also been characterised in strategic marketing as value-creating systems (Parolini, 1999) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (M�_ller & Svahn, 2006;Normann & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998;Gulati, Nohria, & Zaheer, 2000).	0	Recent research has stressed the importance of network structures in understanding business exchanges (Achrol, 1997;M�_ller & Rajala, 2007). These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;#CITATION_TAG, 1985) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980). Business networks have also been characterised in strategic marketing as value-creating systems (Parolini, 1999) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (M�_ller & Svahn, 2006;Normann & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998;Gulati, Nohria, & Zaheer, 2000).	r
CC1597	In the case of culture, the inheritance mechanism is social learning: People learn ways to think and act from others. Of course, the routes through which culture is inherited are much more diverse than those for genes (Cavalli-Sforza & Feldman 1981), and different routes have different consequences for the patterning of cultural change through time. Variation in what is inherited is generated by innovations. These may be unintended copying errors, but they can also be intentional changes, perhaps arising from trial-and-error experimentation, that lead an individual to stop doing what they had previously learned and to start doing it differently, or even to do something different altogether. Whether this will be widely adopted depends on a range of selection and bias mechanisms, many of which have no equivalent in genetic evolution but whose existence and importance have formed the subject of major developments in the theory of cultural evolution over the past 30 years (especially Boyd & Richerson 1985, Cavalli-Sforza & Feldman 1981. These mechanisms form the theoretical foundation for what follows and, given the complexities involved, it is important to spell them out (see also #CITATION_TAG & Lipo 2007).	1	Variation in what is inherited is generated by innovations. These may be unintended copying errors, but they can also be intentional changes, perhaps arising from trial-and-error experimentation, that lead an individual to stop doing what they had previously learned and to start doing it differently, or even to do something different altogether. Whether this will be widely adopted depends on a range of selection and bias mechanisms, many of which have no equivalent in genetic evolution but whose existence and importance have formed the subject of major developments in the theory of cultural evolution over the past 30 years (especially Boyd & Richerson 1985, Cavalli-Sforza & Feldman 1981. These mechanisms form the theoretical foundation for what follows and, given the complexities involved, it is important to spell them out (see also #CITATION_TAG & Lipo 2007).	 
CC2028	The current migration of dunes on Cape Cod is inferred to reflect a legacy of landscape disturbance, specifically forest clear-cutting, grazing and agricultural practices, associated with European settlement starting in the early 17th century and continuing into the 20th century (McCaffrey and Stilgoe, 1981;Rubertone, 1985;Motzkin et al., 2002;Eberhardt et al., 2003;Forman et al., 2008). Other factors such as storminess, hurricane-force winds (Bosse et al., 2001;Eberhardt et al., 2003) and forest fires (Motzkin et al., 2002;Parshall et al., 2003) may have contributed to historic disturbance of this dune landscape. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009;Hansom and Hall, 2009). However, uncertainty remains if periods of increased storminess and/or hurricane landfalls for the exposed Cape Cod spit during the Holocene (Mann et al., 2009;#CITATION_TAG et al., 2013) were of sufficient magnitude to disturb this forest ecosystem and reactivated dunes. Proxy records of hurricane occurrence from coastal overwash timeseries (e.g., Donnelly et al., 2001;Scileppi and Donnelly, 2007) and from marine sediment cores extracted strategically along the Bahama Bank (Williams, 2013) to reflect wave climate indicate heightened hurricane activity in the North Atlantic Ocean between 4900 and 3600, 2500, and 1000, and 600 and 400 years BP (Mann et al., 2009;Toomey et al., 2013).	1	The current migration of dunes on Cape Cod is inferred to reflect a legacy of landscape disturbance, specifically forest clear-cutting, grazing and agricultural practices, associated with European settlement starting in the early 17th century and continuing into the 20th century (McCaffrey and Stilgoe, 1981;Rubertone, 1985;Motzkin et al., 2002;Eberhardt et al., 2003;Forman et al., 2008). Other factors such as storminess, hurricane-force winds (Bosse et al., 2001;Eberhardt et al., 2003) and forest fires (Motzkin et al., 2002;Parshall et al., 2003) may have contributed to historic disturbance of this dune landscape. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009;Hansom and Hall, 2009). However, uncertainty remains if periods of increased storminess and/or hurricane landfalls for the exposed Cape Cod spit during the Holocene (Mann et al., 2009;#CITATION_TAG et al., 2013) were of sufficient magnitude to disturb this forest ecosystem and reactivated dunes. Proxy records of hurricane occurrence from coastal overwash timeseries (e.g., Donnelly et al., 2001;Scileppi and Donnelly, 2007) and from marine sediment cores extracted strategically along the Bahama Bank (Williams, 2013) to reflect wave climate indicate heightened hurricane activity in the North Atlantic Ocean between 4900 and 3600, 2500, and 1000, and 600 and 400 years BP (Mann et al., 2009;Toomey et al., 2013).	e
CC2997	"The increased threat posed by the Ottoman army and the second siege of Vienna in 1683 gave rise to numerous maps and views that illustrate acts of war. Most of them focus only on the historical city centre or show the riverscape in a generalised manner. The riverine structures were mostly copied from older drawings, which can be identified as sources. Hence, little information about the past configuration of the riverscape can be extracted from the great number of 17th/18th century maps and illustrations. The most important exception is a map designed by Colonel Giuseppe Baron Priami in 1663 for the improvement of Vienna""s fortifications. It can be considered as the first map of the Viennese riverscape, which is depicted in a geographically largely correct manner (#CITATION_TAG and Michlmayr 1996;Opll 2004). 9 Even more interesting are several river regulation plans that were drawn after the siege from 1686 onwards, in particular the famous work of the Italian cartographer Leander Anguissola from 1688 and a newly found map by Hoffmann von Anckherskron et al. dating from 1700. Compared to older plans and maps, both maps show large areas of the Viennese riverscape in a regular map projection. Problems remain: In Anguissola""s map, the differentiation between planned and existing hydraulic structures is not always clear and the map was modified at a later date to adapt it to the changed conditions of the riverscape. For example, a new cut-off channel at the Donaukanal excavated in 1700-1703 and bridges built in 1704 were later added, so it could serve as the basis for proposed hydraulic constructions in 1712 (Slezak 1977). 11 The map from Hoffmann von Anckherskron et al. (1700) can be considered as the oldest Viennese river engineering map with a high degree of position accuracy and an outstanding level of detail. It was produced as a planning basis for the construction of a new course for the upper Donaukanal and so far it has never been described in the historical literature. It even shows minor relicts of past hydraulic structures below the low water level and several transects through the river arms. The map provides a sound reference for the localisation of hydraulic structures built in the late 17th century of which-until now-we only partly knew about from written sources."	5	"The riverine structures were mostly copied from older drawings, which can be identified as sources. Hence, little information about the past configuration of the riverscape can be extracted from the great number of 17th/18th century maps and illustrations. The most important exception is a map designed by Colonel Giuseppe Baron Priami in 1663 for the improvement of Vienna""s fortifications. It can be considered as the first map of the Viennese riverscape, which is depicted in a geographically largely correct manner (#CITATION_TAG and Michlmayr 1996;Opll 2004). 9 Even more interesting are several river regulation plans that were drawn after the siege from 1686 onwards, in particular the famous work of the Italian cartographer Leander Anguissola from 1688 and a newly found map by Hoffmann von Anckherskron et al. dating from 1700. Compared to older plans and maps, both maps show large areas of the Viennese riverscape in a regular map projection. Problems remain: In Anguissola""s map, the differentiation between planned and existing hydraulic structures is not always clear and the map was modified at a later date to adapt it to the changed conditions of the riverscape."	n
CC682	More recently, studies carried out in several laboratories [8]#CITATION_TAG[10][11][12] revealed that the addition of significant concentrations of functionally unrelated polymers or proteins can greatly accelerate the formation of fibrillar protein aggregates. A simple model based upon excluded volume theory was shown to quantitatively account for the dependence of the kinetics of amyloid fiber formation upon the concentration of added polymer [10].	0	More recently, studies carried out in several laboratories [8]#CITATION_TAG[10][11][12] revealed that the addition of significant concentrations of functionally unrelated polymers or proteins can greatly accelerate the formation of fibrillar protein aggregates. A simple model based upon excluded volume theory was shown to quantitatively account for the dependence of the kinetics of amyloid fiber formation upon the concentration of added polymer [10].	M
CC46	"Several studies including The Metric Tide [4], The Stern Report [14] and the HEFCE pilot study [15] all state that metrics should be used as an additional component in research evaluation, with peer review remaining as the central pillar. Yet, peer review has been shown by [16], [17] and [18] amongst others to exhibit many forms of bias including institutional bias, gender / age related bias and bias against interdisciplinary research. In an examination of one of the most critical forms of bias, that of publication bias, Emerson #CITATION_TAG noted that reviewers were much more likely to recommend papers demonstrating positive results over those that demonstrated null or negative results. All of the above biases exist even when peer review is carried out to the highest international standards. There were close to 1,000 peer review experts recruited by the REF, however the sheer volume of outputs requiring review calls into question the exactitude of the whole process. As an example the REF panel for UoA 9, Physics, consisted of 20 members. The total number of outputs submitted for this UoA was 6,446. Each paper is required to be read by two referees. This increases the overall total requirement to read 12,892 paper instances. Therefore each panel member was required to review, to international standards, an average of 644 papers in a little over ten months. If every panel member, worked every day for ten months, each member would need to read and review 2.14 papers per day to complete the work on time. This is, of course, in addition to the panelist""s usual full-time work load. Moreover, Physics is not an unusual example and many other UoAs tell a similar story in terms of the average number of papers each panel member was expected to review; Business and Management Studies (1,017 papers), General Engineering (868 papers), Clinical Medicine (765 papers). The burden placed on the expert reviewers during the REF process was onerous in the extreme. Coles [20] calculated a very similar figure of 2 papers per day, based on an estimate before the data we now have was available. It is blindingly obvious,"" he concluded, ""that whatever the panels do will not be a thorough peer review of each paper, equivalent to refereeing it for publication in a journal"". Sayer [21] is equally disparaging in regards to the volume of papers each reviewer was required to read and also expresses significant doubts about the level of expertise within the review panels themselves."	1	Several studies including The Metric Tide [4], The Stern Report [14] and the HEFCE pilot study [15] all state that metrics should be used as an additional component in research evaluation, with peer review remaining as the central pillar. Yet, peer review has been shown by [16], [17] and [18] amongst others to exhibit many forms of bias including institutional bias, gender / age related bias and bias against interdisciplinary research. In an examination of one of the most critical forms of bias, that of publication bias, Emerson #CITATION_TAG noted that reviewers were much more likely to recommend papers demonstrating positive results over those that demonstrated null or negative results. All of the above biases exist even when peer review is carried out to the highest international standards. There were close to 1,000 peer review experts recruited by the REF, however the sheer volume of outputs requiring review calls into question the exactitude of the whole process. As an example the REF panel for UoA 9, Physics, consisted of 20 members.	 
CC1437	We began this section assuming that rationality entailed always and only complying with mathematical, logical, and/or economical norms. Then we discovered that this assumption contained problems. The work of Gigerenzer and colleagues has done well to modify this original perspective. The result is a sort of context-sensitive view of rationality (#CITATION_TAG and Hug 1992). According to this view, there will be circumstances in which being optimally rational entails complying with mathematical, logical, and/or economical norms, but there will also be circumstances in which being optimally rational requires recruiting heuristics and biases. This middle view-that the quality of a cognitive strategy is not absolute, but context-sensitive-serves as a handy segue into the topic of philosophical reasoning.	0	We began this section assuming that rationality entailed always and only complying with mathematical, logical, and/or economical norms. Then we discovered that this assumption contained problems. The work of Gigerenzer and colleagues has done well to modify this original perspective. The result is a sort of context-sensitive view of rationality (#CITATION_TAG and Hug 1992). According to this view, there will be circumstances in which being optimally rational entails complying with mathematical, logical, and/or economical norms, but there will also be circumstances in which being optimally rational requires recruiting heuristics and biases. This middle view-that the quality of a cognitive strategy is not absolute, but context-sensitive-serves as a handy segue into the topic of philosophical reasoning.	 
CC2748	Network change is also discussed in relation to network stability, as stability and change are an inherent duality of networks (HՂkansson & Snehota, 1995). On the one hand forces (such as new actors entering the network, or the activities of existing actors) will always try to change established actor bonds, resource ties and activity patterns. Havila and Salmi (2000) for instance found that mergers triggered a range of changes in connected relationships. Simultaneously, there are forces that will try to move towards stability. For instance, resource dependencies, high switching costs and risk-reducing strategies favour stability (Turnbull, Ford, & Cunningham, 1996). Thus, stability is a prerequisite for change (Lundgren, 1992) and an inherent feature of a network (Halinen, Salmi, & Havila, 1999). This duality has been defined as coalescence and dissemination of networks (HՂkansson & Lundgren, 1992). Others use concepts such as expansion and contraction (#CITATION_TAG, 1987); extension and consolidation (Cook, 1982) and splitting and joining (Hertz, 1996). Halinen et al. (1999) introduce the term confined change to characterise stability when change remains in the dyad, and is not acted upon by other actors in the relationship. However, due to the interdependencies of relationships, change in one relationship often spreads to others, subsequently affecting the whole network. This is defined as connected change; one which influences or is acted upon in other relationships in the network (Halinen et al., 1999).	1	For instance, resource dependencies, high switching costs and risk-reducing strategies favour stability (Turnbull, Ford, & Cunningham, 1996). Thus, stability is a prerequisite for change (Lundgren, 1992) and an inherent feature of a network (Halinen, Salmi, & Havila, 1999). This duality has been defined as coalescence and dissemination of networks (HՂkansson & Lundgren, 1992). Others use concepts such as expansion and contraction (#CITATION_TAG, 1987); extension and consolidation (Cook, 1982) and splitting and joining (Hertz, 1996). Halinen et al. (1999) introduce the term confined change to characterise stability when change remains in the dyad, and is not acted upon by other actors in the relationship. However, due to the interdependencies of relationships, change in one relationship often spreads to others, subsequently affecting the whole network. This is defined as connected change; one which influences or is acted upon in other relationships in the network (Halinen et al., 1999).	u
CC302	Second, the resource based view on the firm (#CITATION_TAG, 1987;Zahra and George, 2002) suggests that the so-called appropriability regime (the extent to which organisations are risk averse, or willing to accept ambiguities inherent in framing) moderates the relationship between framing and eventual adoption of innovations. More generally, in an era of administrative change and transformation, it is useful to remember that organisations, and the actors within them, display complex responses in attempting to promote substantive results (service delivery) and legitimacy (reputation).	3	Second, the resource based view on the firm (#CITATION_TAG, 1987;Zahra and George, 2002) suggests that the so-called appropriability regime (the extent to which organisations are risk averse, or willing to accept ambiguities inherent in framing) moderates the relationship between framing and eventual adoption of innovations. More generally, in an era of administrative change and transformation, it is useful to remember that organisations, and the actors within them, display complex responses in attempting to promote substantive results (service delivery) and legitimacy (reputation).	S
CC2649	Specific occupational health risks are generally more prevalent in lower occupational classes, and are likely to explain part of the association between occupational class and mortality [39]. These risk factors include physical risk factors such as exposure to noise and pollution, heavy lifting, and risk of injuries, as well as psychosocial risk factors like low prestige, demand and control imbalance, and income deprivation [14,[40][41][42]. Psychosocial factors have been associated with inequalities in CVD mortality [43,44] whereas exposure to hazards in industries has been shown to explain some of the occupational class differences in cancer mortality [45]#CITATION_TAG[47][48].	0	Specific occupational health risks are generally more prevalent in lower occupational classes, and are likely to explain part of the association between occupational class and mortality [39]. These risk factors include physical risk factors such as exposure to noise and pollution, heavy lifting, and risk of injuries, as well as psychosocial risk factors like low prestige, demand and control imbalance, and income deprivation [14,[40][41][42]. Psychosocial factors have been associated with inequalities in CVD mortality [43,44] whereas exposure to hazards in industries has been shown to explain some of the occupational class differences in cancer mortality [45]#CITATION_TAG[47][48].	y
CC1608	"These methods are based on the assumption of branching evolution from a single origin; the entities under study, specific artifact types, for example, are placed on a tree such that those branches that have the most similar common histories in terms of shared mutations with respect to particular characters are most closely linked (O""Brien & Lyman 2003). This notion presupposes that the characters are homologous, that is, the artifact types or other entities share specific values for those characters because they are linked by descent from a common ancestor, rather than because they have undergone similar selection pressures (analogous characters or homoplasies; see e.g., O""Brien & Lyman 2003 for the complex terminology of cladistic analysis). A given set of descriptive traits of, for example, an artifact type may be made up of a mixture of homologous and analogous attributes, and these need to be distinguished or reconciled by methods that produce an overall cladogram consistent with the largest number of characters (see, e.g., Collard et al. 2008). Moreover, not all the traits that characterize a complex object or entity will have had a common history (Boyd et al. 1997). Some of the attributes of a given ceramic tradition, decorative motifs, for example, may have been borrowed from a different ceramic tradition by a process of horizontal transmission, and treebuilding methods based on the assumption of branching differentiation from a single ancestor will not do this justice. These issues have resulted in a great deal of debate (for doubts and concerns see Borgerhoff Mulder 2001, Lipo 2006, Temkin & Eldredge 2007, Terrell et al. 1997; contra, e.g., #CITATION_TAG & Jordan 2000, Kirch & Green 2001, Mace & Pagel 1994 and critical analysis (e.g., Eerkens et al. 2006, Nunn et al. 2006) but also produced important methodological developments (e.g., Bryant et al. 2005 Many examples of the use of such phylogenetic techniques to construct cultural lineages and identify the forces affecting them have appeared in the archaeological literature in recent years (e.g., Coward et al. 2008, Darwent & O""Brien 2006, Foley & Lahr 1997, Harmon et al. 2006. Many examples from anthropology have major archaeological implications. Gray & Atkinson (2003), for example, used phylogenetic methods to estimate the most probable date of the root of the Indo-European language family tree, obtaining a result that fits much better with Renfrew""s (1987) agricultural dispersal model of Indo-European spread than with the so-called Kurgan hypothesis, which fits the dates estimated by traditional historical linguists."	0	"A given set of descriptive traits of, for example, an artifact type may be made up of a mixture of homologous and analogous attributes, and these need to be distinguished or reconciled by methods that produce an overall cladogram consistent with the largest number of characters (see, e.g., Collard et al. 2008). Moreover, not all the traits that characterize a complex object or entity will have had a common history (Boyd et al. 1997). Some of the attributes of a given ceramic tradition, decorative motifs, for example, may have been borrowed from a different ceramic tradition by a process of horizontal transmission, and treebuilding methods based on the assumption of branching differentiation from a single ancestor will not do this justice. These issues have resulted in a great deal of debate (for doubts and concerns see Borgerhoff Mulder 2001, Lipo 2006, Temkin & Eldredge 2007, Terrell et al. 1997; contra, e.g., #CITATION_TAG & Jordan 2000, Kirch & Green 2001, Mace & Pagel 1994 and critical analysis (e.g., Eerkens et al. 2006, Nunn et al. 2006) but also produced important methodological developments (e.g., Bryant et al. 2005 Many examples of the use of such phylogenetic techniques to construct cultural lineages and identify the forces affecting them have appeared in the archaeological literature in recent years (e.g., Coward et al. 2008, Darwent & O""Brien 2006, Foley & Lahr 1997, Harmon et al. 2006. Many examples from anthropology have major archaeological implications. Gray & Atkinson (2003), for example, used phylogenetic methods to estimate the most probable date of the root of the Indo-European language family tree, obtaining a result that fits much better with Renfrew""s (1987) agricultural dispersal model of Indo-European spread than with the so-called Kurgan hypothesis, which fits the dates estimated by traditional historical linguists."	 
CC637	Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. To understand what causes developmental changes in behavior, we must also acknowledge and understand the processes through which these factors (sometimes unexpectedly) influence behavior in our tasks, including at short timescales. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (#CITATION_TAG and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. Only then will our theories be both comprehensive enough and sufficiently specific to reliably predict behavior and potentially intervene to prevent adverse outcomes.	4	To understand what causes developmental changes in behavior, we must also acknowledge and understand the processes through which these factors (sometimes unexpectedly) influence behavior in our tasks, including at short timescales. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (#CITATION_TAG and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. Only then will our theories be both comprehensive enough and sufficiently specific to reliably predict behavior and potentially intervene to prevent adverse outcomes.	e
CC548	"For many outside any health service it would seem axiomatic that they would be interested in the results of their ministrations and the fact that this is clearly not the case even in orthopaedics where ""The End Result Idea"" originated (#CITATION_TAG & Weinstein, 1998), is a mystery beyond the reach of this paper. In the UK, however, the word ""outcomes"" has recently been heard at every level of government (Macdonald, 2014) and, as ever imperfectly translated into action or actual resources, it is helping drive the RCOM process forwards, as is excitement about ""value"" (Porter et al., 2006). As described by these authors, value is defined as ""outcomes that are important to the patient"" divided by cost. The simplicity and validity of the numerator is perhaps misleading (Long, 1997), especially in mental health."	4	"For many outside any health service it would seem axiomatic that they would be interested in the results of their ministrations and the fact that this is clearly not the case even in orthopaedics where ""The End Result Idea"" originated (#CITATION_TAG & Weinstein, 1998), is a mystery beyond the reach of this paper. In the UK, however, the word ""outcomes"" has recently been heard at every level of government (Macdonald, 2014) and, as ever imperfectly translated into action or actual resources, it is helping drive the RCOM process forwards, as is excitement about ""value"" (Porter et al., 2006). As described by these authors, value is defined as ""outcomes that are important to the patient"" divided by cost. The simplicity and validity of the numerator is perhaps misleading (Long, 1997), especially in mental health."	F
CC1266	Cognitive ability tests were originally developed to identify low academic achievers (Jensen, 1981;Munzert, 1980). The first such test measured general cognitive intelligence, g, as identified by Spearman (1904Spearman ( , 1927. Test results for an individual across a range of cognitive measures tend to correlate providing good evidence for a single measure of intelligence (Jensen, 1981;Kuncel, Hezlett, & Ones, 2004). In addition to general cognitive intelligence, there is widespread evidence for a multi-dimensional construct of intelligence comprising of a range of sub-factors (Flanagan & McGrew, 1998). Abilities in such sub-factors vary from one individual to another, and vary within an individual across factors, in other words, an individual can have higher ability in one sub-factor than in another (Spearman, 1927, p. 75). Recently the Cattell-Horn-Carroll (CHC) theory of cognitive abilities has gained recognition as a taxonomy of cognitive intelligence (#CITATION_TAG, 2009). The CHC is based on ten broad cognitive categories, summarized in Table 1.	0	Test results for an individual across a range of cognitive measures tend to correlate providing good evidence for a single measure of intelligence (Jensen, 1981;Kuncel, Hezlett, & Ones, 2004). In addition to general cognitive intelligence, there is widespread evidence for a multi-dimensional construct of intelligence comprising of a range of sub-factors (Flanagan & McGrew, 1998). Abilities in such sub-factors vary from one individual to another, and vary within an individual across factors, in other words, an individual can have higher ability in one sub-factor than in another (Spearman, 1927, p. 75). Recently the Cattell-Horn-Carroll (CHC) theory of cognitive abilities has gained recognition as a taxonomy of cognitive intelligence (#CITATION_TAG, 2009). The CHC is based on ten broad cognitive categories, summarized in Table 1.	t
CC1271	"A number of studies have found that the relationship between academic performance and temperament or motivation is mediated by a student""s approach to the learning task itself. Important factors include learning style (e.g., Bruinsma, 2004;Chamorro-Premuzic & Furnham, 2008;Diseth, 2011;Sins et al., 2008) and self-regulation (e.g., #CITATION_TAG et al., 2011;Ning & Downing, 2010). The following sections discuss both learning styles and self-regulation."	0	"A number of studies have found that the relationship between academic performance and temperament or motivation is mediated by a student""s approach to the learning task itself. Important factors include learning style (e.g., Bruinsma, 2004;Chamorro-Premuzic & Furnham, 2008;Diseth, 2011;Sins et al., 2008) and self-regulation (e.g., #CITATION_TAG et al., 2011;Ning & Downing, 2010). The following sections discuss both learning styles and self-regulation."	m
CC679	"The models to be summarized below are based upon highly simplified hard particle representations of molecular size and shape, which have been found useful for semi-quantitative estimation of the effect of crowding upon the free energies, equilibrium, and rate-constants of reactions in crowded solutions [17]#CITATION_TAG[19]. A complementary description of the crowding effect termed ""depletion attraction"" has been presented [20] but so far has not been utilized to quantify the effect of crowding on chemical equilibria and kinetics."	0	"The models to be summarized below are based upon highly simplified hard particle representations of molecular size and shape, which have been found useful for semi-quantitative estimation of the effect of crowding upon the free energies, equilibrium, and rate-constants of reactions in crowded solutions [17]#CITATION_TAG[19]. A complementary description of the crowding effect termed ""depletion attraction"" has been presented [20] but so far has not been utilized to quantify the effect of crowding on chemical equilibria and kinetics."	T
CC2613	Economic geographical thinking and theorising has not made an explicit engagement with the spatial nature of markets, or what that might mean for economic outcomes. The consequence is that -a recent nascent literature notwithstanding (Zook, 2001;Lee, 2006;Hall, 2007;Pryke and DuGay, 2007;Berndt and Boeckler, 2009) -a distinctive geographical understanding of the market sits in the background rather than the foreground of existing work. This represents a significant limitation in the capacity of economic geographical work to contribute to understandings of how markets exist in space, how that spatiality is constituted through wider institutional contexts and systemic phenomenon and how their spatiality has a direct impact on economic outcomes in the contemporary global economy. The proposition of this paper is that economicgeographical thinking can make a potentially powerful contribution to the existing heterodox social scientific literature on markets by developing a more explicit epistemological framework for understanding the way in which market spatiality matters (i.e. how it affects the nature of economic outcomes). To achieve this, there is a need to develop an explicitly spatial epistemological framework that provides scope to better theorise the spatial constitution of markets and the practices that (re)produce them. The aim is to generate socio-spatial theories of markets that better capture the way in which market processes are constituted through and shaped by distinctive spatialities. Such an approach can be seen as complementary rather than a competing epistemological framework to others within economic geography that are seeking to understand the nature of the economy through a variety of lens (e.g. political economic analysis or institutional theory). The goal is to supplement a furtherdistinctly geographical cut at existing socioeconomic conceptions of markets, that can also permit engagement with wider debates in economic geography that would benefit with a more developed heterodox conceptualisation of markets -for example, the growing body of work on financialization (French et al., 2011;#CITATION_TAG and Leyshon, 2013). I therefore propose that at least two epistemological dimensions need to be differentiated in order to develop a holistic socio-spatial theorisation of market space.	4	To achieve this, there is a need to develop an explicitly spatial epistemological framework that provides scope to better theorise the spatial constitution of markets and the practices that (re)produce them. The aim is to generate socio-spatial theories of markets that better capture the way in which market processes are constituted through and shaped by distinctive spatialities. Such an approach can be seen as complementary rather than a competing epistemological framework to others within economic geography that are seeking to understand the nature of the economy through a variety of lens (e.g. political economic analysis or institutional theory). The goal is to supplement a furtherdistinctly geographical cut at existing socioeconomic conceptions of markets, that can also permit engagement with wider debates in economic geography that would benefit with a more developed heterodox conceptualisation of markets -for example, the growing body of work on financialization (French et al., 2011;#CITATION_TAG and Leyshon, 2013). I therefore propose that at least two epistemological dimensions need to be differentiated in order to develop a holistic socio-spatial theorisation of market space.	l
CC2157	The simplest choice of a tensor model is to consider one which has a tensor with three indices as its only dynamical variable. Then, by identifying the rank-three tensor with the structure constant of an algebra charactering a fuzzy space, the tensor model can be interpreted as theory of a dynamical fuzzy space. Since one can in principle choose the values of the rank-three tensor to construct fuzzy spaces corresponding to any dimensional spaces, the rank-three tensor models can equally treat spaces in general dimensions. This idea has first been presented in Ref. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19][20][21][22][23][24][25][26] . The purpose of the present paper is to provide a full treatment of the original incomplete presentation of the idea, and to pursue the algebraic description of the tensor models. In the sequel, it is found that 3-ary algebras [27][28][29] describe the symmetries of the tensor models. 3-ary algebras have been introduced in physics by Nambu , and have recently been widely discussed in the context of M-theory [31][32]#CITATION_TAG . This unexpected common appearance of 3-ary algebras suggests the general Tensor models and 3-ary algebras importance of this new way of describing symmetry in the physics of quantum spacetime. This paper is organized as follows. In the following section, the rank-three tensor model is presented. In Section III, the structure of the algebras corresponding to the rank-three tensor models is discussed. In Section IV, the commutative case of the algebras is discussed.	0	This idea has first been presented in Ref. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19][20][21][22][23][24][25][26] . The purpose of the present paper is to provide a full treatment of the original incomplete presentation of the idea, and to pursue the algebraic description of the tensor models. In the sequel, it is found that 3-ary algebras [27][28][29] describe the symmetries of the tensor models. 3-ary algebras have been introduced in physics by Nambu , and have recently been widely discussed in the context of M-theory [31][32]#CITATION_TAG This unexpected common appearance of 3-ary algebras suggests the general Tensor models and 3-ary algebras importance of this new way of describing symmetry in the physics of quantum spacetime. This paper is organized as follows. In the following section, the rank-three tensor model is presented.	a
CC706	Social anxiety disorder is one of the most persistent and common anxiety disorders, with a lifetime prevalence estimated to range between 3.9% and 13.7% in Europe [1]. People with social anxiety disorder have difficulty forming and retaining personal and social relationships [2], have higher risk of leaving school early and obtaining poorer qualifications [3], experience impairment in their daily functioning including work/school performance and social life [4], and report an important reduction in their quality of life compared with people without the disorder [5]. They also incur considerable healthcare costs, especially relating to the use of primary care services, experience high levels of productivity losses and receive higher social benefits compared with people in the general population [6][7]#CITATION_TAG. It has been shown that as the number of social fears increases, so does health service utilisation [9]. The presence of comorbid psychiatric disorders increases usage of health services and productivity losses [6,8,9].	0	Social anxiety disorder is one of the most persistent and common anxiety disorders, with a lifetime prevalence estimated to range between 3.9% and 13.7% in Europe [1]. People with social anxiety disorder have difficulty forming and retaining personal and social relationships [2], have higher risk of leaving school early and obtaining poorer qualifications [3], experience impairment in their daily functioning including work/school performance and social life [4], and report an important reduction in their quality of life compared with people without the disorder [5]. They also incur considerable healthcare costs, especially relating to the use of primary care services, experience high levels of productivity losses and receive higher social benefits compared with people in the general population [6][7]#CITATION_TAG. It has been shown that as the number of social fears increases, so does health service utilisation [9]. The presence of comorbid psychiatric disorders increases usage of health services and productivity losses [6,8,9].	e
CC1509	"The striking feature of SD is a fairly selective deterioration of semantic information across all modalities of input and output, both verbal and non-verbal. Non-verbal manifestations of the deficit have been found in delayed copy drawing, where distinctive features are lost (e.g. the hump of the camel) and common features intrude, particularly for living things which share many attributes (e.g. a duck is drawn with four legs; Bozeat et al., 2003). Patients are poor at matching pictures of objects to their characteristic sounds as well as their names (#CITATION_TAG, Lambon Ralph, Garrard, Patterson, & Hodges, 2000) and they show impaired use of objects, especially less common ones (e.g. a corkscrew or stethoscope, Bozeat, Lambon Ralph, Patterson, & Hodges, 2002). At mild-moderate stages, when SD patients can still perform categorisation tasks, they are better are categorising pictures at the general level (i.e. animal or non-living thing), than at the basic level (e.g. dog or bird), and better at the basic level than at the specific level (e.g. labrador or collie) (Rogers & Patterson, 2007). Verbal manifestations of the semantic deficit are seen in impaired object naming and word-to-picture matching, such that these tests are routinely used in diagnosis (Hodges & Patterson, 2007). In free speech, the anomia typical of SD results in open class items being replaced by more general, indefinite terms (e.g. thing stuff"" and ""place"") and an increased reliance on high frequency, high familiarity items and pronouns (Bird, Lambon Ralph, Patterson, & Hodges, 2000)."	0	"The striking feature of SD is a fairly selective deterioration of semantic information across all modalities of input and output, both verbal and non-verbal. Non-verbal manifestations of the deficit have been found in delayed copy drawing, where distinctive features are lost (e.g. the hump of the camel) and common features intrude, particularly for living things which share many attributes (e.g. a duck is drawn with four legs; Bozeat et al., 2003). Patients are poor at matching pictures of objects to their characteristic sounds as well as their names (#CITATION_TAG, Lambon Ralph, Garrard, Patterson, & Hodges, 2000) and they show impaired use of objects, especially less common ones (e.g. a corkscrew or stethoscope, Bozeat, Lambon Ralph, Patterson, & Hodges, 2002). At mild-moderate stages, when SD patients can still perform categorisation tasks, they are better are categorising pictures at the general level (i.e. animal or non-living thing), than at the basic level (e.g. dog or bird), and better at the basic level than at the specific level (e.g. labrador or collie) (Rogers & Patterson, 2007). Verbal manifestations of the semantic deficit are seen in impaired object naming and word-to-picture matching, such that these tests are routinely used in diagnosis (Hodges & Patterson, 2007). In free speech, the anomia typical of SD results in open class items being replaced by more general, indefinite terms (e.g. thing stuff"" and ""place"") and an increased reliance on high frequency, high familiarity items and pronouns (Bird, Lambon Ralph, Patterson, & Hodges, 2000)."	t
CC111	"With the prevalence of chronic illness rising worldwide, there is a need to engage patients in health promotion work in order to prevent further deterioration, to strengthen their health and their capacity to participate in society [1] [2]. Interventions based on such work will reflect the philosophical perspective of ""health within illness which holds that individuals living with long-term health problems are capable of experiencing health and wellbeing despite their conditions [3] [4]. Summaries of research concerning people with various long-term conditions show that they have much in common as they face the challenges of trying to live as well as possible within the context of physical, mental, or social discomfort and limitation [5]- [7]. However, patient education and wellness-interventions in the context of chronic illness are often specific to particular diagnostic groups and not designed to be applied across diagnostic categories [8] [9]. Two of the few examples of interventions that are practiced more broadly are the Chronic Disease Self-Management Education (CDSME) developed by Lorig and colleagues in the USA [10] and the Vitality Training Program (VTP) developed by Steen and Haugli [11] in Norway. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [12] [13]. Improved health behavior and health status were also reported in a group of patients with serious mental illness [14]. However, a longitudinal randomized trial of stroke survivors who accomplished CDSME, showed that the intervention did not appear to impact self-efficacy and failed to influence outcomes such as mood or social outcomes [15]. This was also confirmed in a Cochrane review that focused on the outcomes of CDSME #CITATION_TAG."	0	Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [12] [13]. Improved health behavior and health status were also reported in a group of patients with serious mental illness [14]. However, a longitudinal randomized trial of stroke survivors who accomplished CDSME, showed that the intervention did not appear to impact self-efficacy and failed to influence outcomes such as mood or social outcomes [15]. This was also confirmed in a Cochrane review that focused on the outcomes of CDSME #CITATION_TAG.	 
CC2354	A region of specific interest is the Callej�_n de Huaylas (the valley drained by the R�_o Santa) in Peru, where water availability is determined by particular climate and topographical settings (e.g., Kaser et al., 2003). While the tropical atmosphere is thermally homogeneous, the region is charac-terized by single-peaked hygric seasonality. Precipitation increases from August towards the October to April core wet season and is close to nil during June and July (e.g., Kaser and Osmaston, 2002;Mark et al., 2010;Schauwecker et al., 2014). Dry season runoff, and thus water supply, is comprised of up to two thirds glacial melt water from the Cordillera Blanca (e.g., #CITATION_TAG et al., 2012;Kaser et al., 2010;Seltzer, 2003, 2005). They smooth the seasonal runoff to a degree that varies with the proportion of sub-catchments that are covered by glaciers (e.g., Kaser et al., 2003;Mark and Seltzer, 2003). While the highest glacier cover of up to 41 % is found in the northern Cordillera Blanca valleys, rivers draining the western Cordillera Negra are lacking in glacier contribution (e.g., Kaser et al., 2003).	0	A region of specific interest is the Callej�_n de Huaylas (the valley drained by the R�_o Santa) in Peru, where water availability is determined by particular climate and topographical settings (e.g., Kaser et al., 2003). While the tropical atmosphere is thermally homogeneous, the region is charac-terized by single-peaked hygric seasonality. Precipitation increases from August towards the October to April core wet season and is close to nil during June and July (e.g., Kaser and Osmaston, 2002;Mark et al., 2010;Schauwecker et al., 2014). Dry season runoff, and thus water supply, is comprised of up to two thirds glacial melt water from the Cordillera Blanca (e.g., #CITATION_TAG et al., 2012;Kaser et al., 2010;Seltzer, 2003, 2005). They smooth the seasonal runoff to a degree that varies with the proportion of sub-catchments that are covered by glaciers (e.g., Kaser et al., 2003;Mark and Seltzer, 2003). While the highest glacier cover of up to 41 % is found in the northern Cordillera Blanca valleys, rivers draining the western Cordillera Negra are lacking in glacier contribution (e.g., Kaser et al., 2003).	 
CC2178	A Swedish study [61] described the use of Blissym bols TM on a SGD. The authors described progress with reading, writing and communication for the 2 child participants, however, outcomes were not clearly reported. Another study [62] described the use of TALK TM , a text-storage and retrieval system. The single participant tested with the system achieved a conversational rate of 64 words per minute, and in another paper #CITATION_TAG the same authors described successful use of a pre-storage device. The Speech Enhancer TM (a portable voice processor unit with speakers and feedback to the user) was found to be effective in improving intelligibility as rated by an experienced listener [64] . Other positive outcomes reported were increased unprompted use of a VOCA [65] , and an increase in assertiveness, longer utterances, language and literacy [66] .	2	A Swedish study [61] described the use of Blissym bols TM on a SGD. The authors described progress with reading, writing and communication for the 2 child participants, however, outcomes were not clearly reported. Another study [62] described the use of TALK TM , a text-storage and retrieval system. The single participant tested with the system achieved a conversational rate of 64 words per minute, and in another paper #CITATION_TAG the same authors described successful use of a pre-storage device. The Speech Enhancer TM (a portable voice processor unit with speakers and feedback to the user) was found to be effective in improving intelligibility as rated by an experienced listener [64] . Other positive outcomes reported were increased unprompted use of a VOCA [65] , and an increase in assertiveness, longer utterances, language and literacy [66] .	 
CC2049	The current migration of dunes on Cape Cod is inferred to reflect a legacy of landscape disturbance, specifically forest clear-cutting, grazing and agricultural practices, associated with European settlement starting in the early 17th century and continuing into the 20th century (McCaffrey and Stilgoe, 1981;Rubertone, 1985;Motzkin et al., 2002;Eberhardt et al., 2003;Forman et al., 2008). Other factors such as storminess, hurricane-force winds (Bosse et al., 2001;Eberhardt et al., 2003) and forest fires (Motzkin et al., 2002;Parshall et al., 2003) may have contributed to historic disturbance of this dune landscape. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009;Hansom and Hall, 2009). However, uncertainty remains if periods of increased storminess and/or hurricane landfalls for the exposed Cape Cod spit during the Holocene (Mann et al., 2009;Toomey et al., 2013) were of sufficient magnitude to disturb this forest ecosystem and reactivated dunes. Proxy records of hurricane occurrence from coastal overwash timeseries (e.g., #CITATION_TAG et al., 2001;Scileppi and Donnelly, 2007) and from marine sediment cores extracted strategically along the Bahama Bank (Williams, 2013) to reflect wave climate indicate heightened hurricane activity in the North Atlantic Ocean between 4900 and 3600, 2500, and 1000, and 600 and 400 years BP (Mann et al., 2009;Toomey et al., 2013).	0	Other factors such as storminess, hurricane-force winds (Bosse et al., 2001;Eberhardt et al., 2003) and forest fires (Motzkin et al., 2002;Parshall et al., 2003) may have contributed to historic disturbance of this dune landscape. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009;Hansom and Hall, 2009). However, uncertainty remains if periods of increased storminess and/or hurricane landfalls for the exposed Cape Cod spit during the Holocene (Mann et al., 2009;Toomey et al., 2013) were of sufficient magnitude to disturb this forest ecosystem and reactivated dunes. Proxy records of hurricane occurrence from coastal overwash timeseries (e.g., #CITATION_TAG et al., 2001;Scileppi and Donnelly, 2007) and from marine sediment cores extracted strategically along the Bahama Bank (Williams, 2013) to reflect wave climate indicate heightened hurricane activity in the North Atlantic Ocean between 4900 and 3600, 2500, and 1000, and 600 and 400 years BP (Mann et al., 2009;Toomey et al., 2013).	y
CC283	Recently, the idea of integration was further pushed by it being given strong support (OECD, 2009) and the related academic discussion of personalised integrated services (#CITATION_TAG and Cotterill, 2007;Peterson et al., 2007;Homburg and Dijkshoorn, 2011). A characteristic of personalised services is that they generally, when using Customer Relationship Management systems (King and Cotterill, 2007), make use of authorisation, profiling and customisation in such a way that, eventually, one-to-one relationships between public service providers and citizens are established. One-to-one relationships may provide citizens with, for example, pre-filled forms, suggestions for permits or benefits that maybe relevant given past requests, automatically generated reminders, and news updates based on customer preferences. The eventual aim is to provide services that are geared towards the needs of citizens, and less towards the existing supply-oriented organisational routines of service providers.	3	Recently, the idea of integration was further pushed by it being given strong support (OECD, 2009) and the related academic discussion of personalised integrated services (#CITATION_TAG and Cotterill, 2007;Peterson et al., 2007;Homburg and Dijkshoorn, 2011). A characteristic of personalised services is that they generally, when using Customer Relationship Management systems (King and Cotterill, 2007), make use of authorisation, profiling and customisation in such a way that, eventually, one-to-one relationships between public service providers and citizens are established. One-to-one relationships may provide citizens with, for example, pre-filled forms, suggestions for permits or benefits that maybe relevant given past requests, automatically generated reminders, and news updates based on customer preferences. The eventual aim is to provide services that are geared towards the needs of citizens, and less towards the existing supply-oriented organisational routines of service providers.	R
CC1101	"Laboratory-based dream research has been based, from its early beginnings (e.g., #CITATION_TAG and Kleitman, 1957), on an approach that combines physiological measurement (EEG and other markers) with subjective dream reports. This has revealed qualitative and quantitative differences in the nature of dream experiences reported after awakenings from REM sleep, NREM sleep (McNamara et al., 2010), and NREM Stage 1 sleep onset (Nielsen et al., 2005;Stenstrom et al., 2012): REM sleep has been found to possess the most vivid and immersive dreams, NREM sleep the most thought-like mentation, and Stage1 NREM sleep the briefest but nonetheless REM-like mentation (Dement and Kleitman, 1957). One major limitation of the laboratory-based study, however, is the ""first-night effect,"" known to change sleep architectureespecially that of REM sleep- (Agnew et al., 1966) and increase the incorporation of laboratory-related content into dreams (Schredl, 2008)."	0	"Laboratory-based dream research has been based, from its early beginnings (e.g., #CITATION_TAG and Kleitman, 1957), on an approach that combines physiological measurement (EEG and other markers) with subjective dream reports. This has revealed qualitative and quantitative differences in the nature of dream experiences reported after awakenings from REM sleep, NREM sleep (McNamara et al., 2010), and NREM Stage 1 sleep onset (Nielsen et al., 2005;Stenstrom et al., 2012): REM sleep has been found to possess the most vivid and immersive dreams, NREM sleep the most thought-like mentation, and Stage1 NREM sleep the briefest but nonetheless REM-like mentation (Dement and Kleitman, 1957). One major limitation of the laboratory-based study, however, is the ""first-night effect,"" known to change sleep architectureespecially that of REM sleep- (Agnew et al., 1966) and increase the incorporation of laboratory-related content into dreams (Schredl, 2008)."	L
CC561	Such homology theories are well known for associative (or Lie) algebras (see for example [23]). Also for A ___ , or L ___ -algebras with m 0 = 0 such homology theories have been known (we refer readers to [#CITATION_TAG] for the definitions using non-commutative geometry). The definition easily extends to the case with non-vanishing m 0 , but it turns out that the usual homological algebra does not immediately extend as the usual contracting homotopy of the bar complex does not work with m = 0. We show that by working with Novikov fields, modified contraction homotopy exists and that we still have the reduced Hochschild homology, and (b, B)-cyclic complex where Connes-Tsygan B-operator actually has an additional term compared to the standard case.	0	Such homology theories are well known for associative (or Lie) algebras (see for example [23]). Also for A ___ , or L ___ -algebras with m 0 = 0 such homology theories have been known (we refer readers to [#CITATION_TAG] for the definitions using non-commutative geometry). The definition easily extends to the case with non-vanishing m 0 , but it turns out that the usual homological algebra does not immediately extend as the usual contracting homotopy of the bar complex does not work with m = 0. We show that by working with Novikov fields, modified contraction homotopy exists and that we still have the reduced Hochschild homology, and (b, B)-cyclic complex where Connes-Tsygan B-operator actually has an additional term compared to the standard case.	l
CC564	A competing account has recently been proposed that regards inter-personal coordination in dialog as a form of synergy or dynamical coupling (#CITATION_TAG et al., 2014). This approach is rooted in dynamical approaches to coordination that are levelagnostic, seeking to understand emergent phenomena at one level (e.g., the dyad) as arising through processes of self-organization from the constrained interaction of autonomous components at a lower level (the speaker/listeners) (Kelso, 1995;Latash, 2008). This approach highlights the sensitivity of participants to real time recurrent interaction, as is evident even in the early interactions of infants and mothers (Murray and Trevarthen, 1986). It emphasizes the intertwining of the movements of participants, leading to dimensional reduction, so that two interacting persons become, temporarily, a simpler collective entity than the two persons considered as a mere conjunction of individuals. It acknowledges both synchronized and complementary actions as they contribute to this simplification, and it emphasizes the manner in which shared understanding of task constraints leads to stability of patterning in time. Although still somewhat speculative, this level-independent approach seems commensurate with the approach to be developed here that treats groups of people as synergetically organized domains in their own right, with respect to which subjectivities of a collective nature can be identified.	0	A competing account has recently been proposed that regards inter-personal coordination in dialog as a form of synergy or dynamical coupling (#CITATION_TAG et al., 2014). This approach is rooted in dynamical approaches to coordination that are levelagnostic, seeking to understand emergent phenomena at one level (e.g., the dyad) as arising through processes of self-organization from the constrained interaction of autonomous components at a lower level (the speaker/listeners) (Kelso, 1995;Latash, 2008). This approach highlights the sensitivity of participants to real time recurrent interaction, as is evident even in the early interactions of infants and mothers (Murray and Trevarthen, 1986). It emphasizes the intertwining of the movements of participants, leading to dimensional reduction, so that two interacting persons become, temporarily, a simpler collective entity than the two persons considered as a mere conjunction of individuals.	A
CC578	When an utterance is made in a specific context with speaker and listener both present, it is interpreted in the light of the shared understanding of all parties. This has found expression in theoretical notions of common ground (Clark and Brennan, 1991), or socially shared cognition (Schegloff, 1991). Most developments of the idea of common ground are couched within the information processing/message passing framework, and therefore make use of some version of aligned or shared representational content. However, it is not necessary to appeal to such unobservable constructs from a hidden Cartesian world (#CITATION_TAG and Myin, 2013). There is ample evidence that participants in a conversational exchange become mutually linked in many subtle but observable ways. Eye movements (Richardson et al., 2007), postural sway (Shockley et al., 2009), and even blinking (Cummins, 2012) have all been found to become subtly intertwined in conversation, leading to a dynamic entanglement of the participants. Speakers and listeners are further linked through the provision by the latter of signals of ongoing engagement through postural, gestural, and vocal indices or backchannels (Wagner et al., 2014).	0	When an utterance is made in a specific context with speaker and listener both present, it is interpreted in the light of the shared understanding of all parties. This has found expression in theoretical notions of common ground (Clark and Brennan, 1991), or socially shared cognition (Schegloff, 1991). Most developments of the idea of common ground are couched within the information processing/message passing framework, and therefore make use of some version of aligned or shared representational content. However, it is not necessary to appeal to such unobservable constructs from a hidden Cartesian world (#CITATION_TAG and Myin, 2013). There is ample evidence that participants in a conversational exchange become mutually linked in many subtle but observable ways. Eye movements (Richardson et al., 2007), postural sway (Shockley et al., 2009), and even blinking (Cummins, 2012) have all been found to become subtly intertwined in conversation, leading to a dynamic entanglement of the participants. Speakers and listeners are further linked through the provision by the latter of signals of ongoing engagement through postural, gestural, and vocal indices or backchannels (Wagner et al., 2014).	e
CC1820	"There are number of reasons to hypothesize that choice and choice-induced preference change are modulated by action. A recent study [14] reported that the relative value of visually represented items is enhanced by button presses to a coinciding auditory cue. This ""cue-approach training"" increased choice preference for otherwise equally valued items and led to a subsequently increased valuation. Interestingly, this effect was only seen for items with relatively high value. Additionally, a revaluation effect was driven by a value change induced by the preceding choice, indicating a specificity of the cue approach training to bias the binary choice between two items. The observation that the effect is only present for high value items points to a specific interaction of approach with choice and positive value. We believe that this is consistent with a Pavlovian congruence framework #CITATION_TAG, in which button presses are a model for approach behaviour. Indeed, a coupling of action and reward could affect choice-induced preference change as it has been linked to dopaminergic circuitry [16,17] and an effect in the striatum [10,18]. Therefore, we hypothesize a value enhancing effect by action in a context of positive value."	0	Interestingly, this effect was only seen for items with relatively high value. Additionally, a revaluation effect was driven by a value change induced by the preceding choice, indicating a specificity of the cue approach training to bias the binary choice between two items. The observation that the effect is only present for high value items points to a specific interaction of approach with choice and positive value. We believe that this is consistent with a Pavlovian congruence framework #CITATION_TAG, in which button presses are a model for approach behaviour. Indeed, a coupling of action and reward could affect choice-induced preference change as it has been linked to dopaminergic circuitry [16,17] and an effect in the striatum [10,18]. Therefore, we hypothesize a value enhancing effect by action in a context of positive value.	i
CC1951	Lanthanum aluminate (LaAlO 3 -LAO) is a prototype compound for soft-mode driven antiferrodistorsive phase transitions. At ambient conditions, LAO crystallizes in the R3c space group, as a result of the condensation of a soft mode at the R point of the Brillouin zone boundary. At ambient conditions, the rhombohedral cell has the lattice parameters a R = 5.357�� and �� R = 60.12 _�� , or in the hexagonal setting a H = 5.366�� and c H = 13.109�� [7]. In this structure, the oxygen octahedra are rotated along the [111] C direction of the parent cubic cell. The rhombohedral distortion can be described by this single tilt angle. Following early structural studies [#CITATION_TAG,9], the temperature-induced phase transition at 813 K has been studied in great details (e.g. [10] and references within). On the theoretical side, the parameters of a Landau potential were fitted to available experimental data [11]. The pressure-induced rhombohedral to cubic transition of LAO was revealed by a powder Raman spectroscopy and synchrotron diffraction study [12]. It was at that time the first exception to the so-far general rule stating that tilt angles in antiferrodistorsive perovskites should increase under pressure [13]. This example has motivated theoretical work to explain this behaviour and formulate new rules and models to predict qualitatively and quantitavely the evolution of distortions (tilt angles) under pressure [14,15,16,17]. The evolution of the tilt angles in LAO itself was investigated by single crystal diffraction under hydrostatic stress up to 8 GPa [7] and non-hydrostatic stress [18].	0	At ambient conditions, the rhombohedral cell has the lattice parameters a R = 5.357�� and �� R = 60.12 _�� , or in the hexagonal setting a H = 5.366�� and c H = 13.109�� [7]. In this structure, the oxygen octahedra are rotated along the [111] C direction of the parent cubic cell. The rhombohedral distortion can be described by this single tilt angle. Following early structural studies [#CITATION_TAG,9], the temperature-induced phase transition at 813 K has been studied in great details (e.g. [10] and references within). On the theoretical side, the parameters of a Landau potential were fitted to available experimental data [11]. The pressure-induced rhombohedral to cubic transition of LAO was revealed by a powder Raman spectroscopy and synchrotron diffraction study [12]. It was at that time the first exception to the so-far general rule stating that tilt angles in antiferrodistorsive perovskites should increase under pressure [13].	w
CC1560	"While many authors criticize TCA for failing to recognize that value creation rather than minimizing costs is the primary goal of business exchange (e.g. Anderson, 1995;Ghosh & John, 1999;#CITATION_TAG & Tallman, 1998;Zajac & Olsen, 1993), others see TCA as a starting point for analyzing value creation between exchange partners. Kleinaltenkamp and Ehret (2006) position specific investments as a source of switching costs but also as an important source of value creation. Indeed, productivity advantages of specific investments become apparent when the original TCA assumption ""output is held constant"" (Williamson, 1985, p.85) is given up (Kim, 1999;Kleinaltenkamp & Ehret, 2006). Since specific investments are particular to the focal relationship, the associated gain referred to as ""quasirent"" can only be realized between the involved parties (Backhaus & B�_schken, 1999). By making specific investments for a customer, a supplier can help this particular customer in developing more efficient operations or in better differentiating its market offerings (Ghosh & John, 1999). Consequently, the creation of superior value increases the customer""s willingness to pay for the supplier""s offerings which results, in turn, in a higher customer lifetime value for the supplier. The same applies in converse when a customer specifically invests in a supplier."	0	"While many authors criticize TCA for failing to recognize that value creation rather than minimizing costs is the primary goal of business exchange (e.g. Anderson, 1995;Ghosh & John, 1999;#CITATION_TAG & Tallman, 1998;Zajac & Olsen, 1993), others see TCA as a starting point for analyzing value creation between exchange partners. Kleinaltenkamp and Ehret (2006) position specific investments as a source of switching costs but also as an important source of value creation. Indeed, productivity advantages of specific investments become apparent when the original TCA assumption ""output is held constant"" (Williamson, 1985, p.85) is given up (Kim, 1999;Kleinaltenkamp & Ehret, 2006). Since specific investments are particular to the focal relationship, the associated gain referred to as ""quasirent"" can only be realized between the involved parties (Backhaus & B�_schken, 1999)."	W
CC300	"To summarize, what can be witnessed is that, in line with institutional theory (Di-Maggio and Powell, 1983;Ashworth et al., 2009;#CITATION_TAG et al., 2006), all the municipalities reported perceiving persuasive pressure to adopt personalization measures, both from outside the set of potential adopters (referring to norms to conform to citizens"" needs, or to be receptive towards national initiatives) as well as from within the set of potential adopters (referring to the norm to excel in relation to one""s peers). Adopters are associated with a higher perceived persuasive pressure than non-adopters. The abovementioned observations lead us to formulate the conjectures 1A and 1B."	0	"To summarize, what can be witnessed is that, in line with institutional theory (Di-Maggio and Powell, 1983;Ashworth et al., 2009;#CITATION_TAG et al., 2006), all the municipalities reported perceiving persuasive pressure to adopt personalization measures, both from outside the set of potential adopters (referring to norms to conform to citizens"" needs, or to be receptive towards national initiatives) as well as from within the set of potential adopters (referring to the norm to excel in relation to one""s peers). Adopters are associated with a higher perceived persuasive pressure than non-adopters. The abovementioned observations lead us to formulate the conjectures 1A and 1B."	T
CC1923	"(1) /w r g-te n/ [FEMININE DUAL NOUN Broken plurals are formed by stem change in the vocalic pattern of the noun. This process is dependent on two morphological phenomena: the vocalic patterns of broken plurals and the consonantal roots. Lexical items of Arabic origin consist of a consonantal root which is a string of consonants. Most roots in Arabic are made up of three consonants which are referred to as triliteral roots or consonantal roots (Zabbal, 2002;#CITATION_TAG, 2006). A root can be notated as C1 C2 C3. Roots are combined with vowels that embed themselves between root consonants to form a lexical item. The meaning of a given lexical item can be changed by changing the vowel pattern within the word form. For example, the word form /k t b / meaning ""he wrote"" is derived from the consonantal root /ktb/ which is the triliteral root for word forms derived from these three consonants. The same triliteral root /ktb/ is used with a change of vowels for the word form /k taeb / meaning ""writing"". Previous literature (Soudi et al., 2002;Zabbal, 2002;Prunet, 2006) has stated that broken plurals are formed in a three stage process. For the sake of describing this process, the singular noun /r l/ ""man"" will be used as an example through the three stages of broken pluralisation process. Stage one: the root from the underlying singular noun is selected. Stage two: the broken plural pattern i.e. vocalic pattern is selected. Stage three: the selected root is merged with the selected vocalic pattern to form the broken plural of the singular noun /r l/ , to become /r :l/ ."	0	(1) /w r g-te n/ [FEMININE DUAL NOUN Broken plurals are formed by stem change in the vocalic pattern of the noun. This process is dependent on two morphological phenomena: the vocalic patterns of broken plurals and the consonantal roots. Lexical items of Arabic origin consist of a consonantal root which is a string of consonants. Most roots in Arabic are made up of three consonants which are referred to as triliteral roots or consonantal roots (Zabbal, 2002;#CITATION_TAG, 2006). A root can be notated as C1 C2 C3. Roots are combined with vowels that embed themselves between root consonants to form a lexical item. The meaning of a given lexical item can be changed by changing the vowel pattern within the word form.	t
CC1066	Further analysis of the factor scores indicated that this two factor solution correlated with dominant area of appearance self-consciousness. There was a greater likelihood of significance and large effect sizes in SBSC for people who identified their main area of sensitivity in a region of their body that was sexually significant or concealable by clothing. There is clear evidence that increased body self-consciousness is related to decreased sexual satisfaction in the general population (#CITATION_TAG & Warren, 2014). Issues concerning appearance and sexual difference for people with a visibly different appearance are also recognised as neglected areas such as in burns rehabilitation (Ahmad et al., 2013). Furthermore, there is evidence that in appearance altering conditions such as breast cancer (Taylor et al., 2013), professionals may not routinely attend to issues of sexuality, despite the reported willingness of patients to discuss it. Including an assessment such as the SBSC factor of DAS24 will facilitate these discussions and bring to the foreground for healthcare providers the potential impact of appearance on sexuality. The lack of understanding of sexual functioning in relation to body image, and any accompanying lack of measurement tools have been cited as a major barrier to developing effective interventions (Corry, Pruzinsky & Rumsey, 2009;Taylor et al., 2011).	2	Further analysis of the factor scores indicated that this two factor solution correlated with dominant area of appearance self-consciousness. There was a greater likelihood of significance and large effect sizes in SBSC for people who identified their main area of sensitivity in a region of their body that was sexually significant or concealable by clothing. There is clear evidence that increased body self-consciousness is related to decreased sexual satisfaction in the general population (#CITATION_TAG & Warren, 2014). Issues concerning appearance and sexual difference for people with a visibly different appearance are also recognised as neglected areas such as in burns rehabilitation (Ahmad et al., 2013). Furthermore, there is evidence that in appearance altering conditions such as breast cancer (Taylor et al., 2013), professionals may not routinely attend to issues of sexuality, despite the reported willingness of patients to discuss it. Including an assessment such as the SBSC factor of DAS24 will facilitate these discussions and bring to the foreground for healthcare providers the potential impact of appearance on sexuality.	e
CC2039	Dune reactivation is associated with large-scale disturbance of the forest which is dominated by pitch pine (Pinus rigida) and a variety of oak species (e.g., Quercus alba, and Quercus coccinea), but white pine (Pinus strobes), red maple (Acer rubrum), hickory (Carya spp.), and American beech (Fagus grandifolia) are locally common (Motzkin et al., 1999;Cogbill et al., 2002). Cape Cod with its limited topographic relief and dominance of pine forests is particularly susceptible to tree blow-down with Fujita scale winds >2 (>181 km/h; cf. Bosse et al., 2001). Tropical storms of hurricane intensity are known to blow down large forest areas along tracks that are 50-100 km in width and extend for 100s of kilometers in New England (Foster et al., 2006, p. 49;Zeng et al., 2009). Specifically, nearly 100% of pine stands older than 20 years were damaged by a hurricane in 1938 that impacted central New England, whereas hardwood stands were less effected (Foster, 1988). The mound and pit mircotopography associated with wind uprooted trees can dominate landscapes. For example, in New Brunswick up to 50% of a forest area post-storm was affected with 600 mounds/acre (Foster and Boose, 1995, p. 323) and such processes have been documented in the past ca. 1000 years (Foster et al., 2006, p. 49). The pit and mound topography provides fresh grains for eolian transport and initiates ecological succession with pioneer species occupying the mound (cf. Peterson et al., 1990). The recovery of coastal dune systems post-hurricane is also dependent on the frequency of succeeding storms (e.g., Houser, 2013) and thus, dune movement can be sustained with decadal to centennial periods of heightened hurricane/storm reoccurrence (e.g., Mann et al., 2009;Toomey et al., 2013). Also, ecologic succession to a climax forest post-disturbance can take centuries (#CITATION_TAG, 2000), though trees <20 years old are less susceptible to wind uprooting (Foster, 1988). Thus, dune migration on Cape Cod may reflect a pronounced period of storm or hurricane-related disturbance of the pine-dominated forest.	0	1000 years (Foster et al., 2006, p. 49). The pit and mound topography provides fresh grains for eolian transport and initiates ecological succession with pioneer species occupying the mound (cf. Peterson et al., 1990). The recovery of coastal dune systems post-hurricane is also dependent on the frequency of succeeding storms (e.g., Houser, 2013) and thus, dune movement can be sustained with decadal to centennial periods of heightened hurricane/storm reoccurrence (e.g., Mann et al., 2009;Toomey et al., 2013). Also, ecologic succession to a climax forest post-disturbance can take centuries (#CITATION_TAG, 2000), though trees <20 years old are less susceptible to wind uprooting (Foster, 1988). Thus, dune migration on Cape Cod may reflect a pronounced period of storm or hurricane-related disturbance of the pine-dominated forest.	g
CC535	Despite evidence supporting feedback to patients of their own PROM outcomes data in psychotherapeutic settings (#CITATION_TAG, Lambert, Harmon, Smart, & Bailey, 2008) only in some psychotherapy services in the UK does this occur, and this has yet to be reported in secondary care. It would seem likely that combining data from PROMs and CROMs might helpfully inform the clinical conversation in many settings, and would clearly improve data quality of the latter.	0	Despite evidence supporting feedback to patients of their own PROM outcomes data in psychotherapeutic settings (#CITATION_TAG, Lambert, Harmon, Smart, & Bailey, 2008) only in some psychotherapy services in the UK does this occur, and this has yet to be reported in secondary care. It would seem likely that combining data from PROMs and CROMs might helpfully inform the clinical conversation in many settings, and would clearly improve data quality of the latter.	D
CC2924	"Such research suggests that, despite its potential, the translation and use of RCA in healthcare remains problematic. RCA, it might be argued, is a highly context-specific model for learning that largely reflects the experiences and safety improvements witnessed in industries such as aviation and petrochemicals. In these nonhealthcare settings, the ""human factors"" approach has been instrumental in bringing about a radical shift in operational safety, whilst the RCA toolbox has been integral to producing recommendations for change. A growing body of research suggests, however, that the translation and replication of these successes in healthcare appears increasingly difficult. The notion of ""translation"" suggests that ideas, methods, and policies are not mechanically transferred or implemented from one setting to another. Instead, they travel in the guise of textualised intermediaries thanks to potential users who perceive some benefits from their adoption. Such travel in time and space thus depends on transformation, editing, and appropriation. Because there are always several possible competing interpretations of any idea, the way in which this is translated in practice is necessarily determined by specific interests and logic. The result is partial acceptance and adaptation, but also addition, substantial modification and even radical reinterpretation (Latour, 1986, talks about ""betrayal""). The translation of RCA from industry to healthcare thus involves the influence of multiple and competing interests and logics that reinterpret and reframe the investigation process to align with established ways of working and enduring lines of power. Underling tension may exist between the espoused aspiration of learning and the organisational context within which this learning is to take place. Accordingly, our aim in this paper is to understand further how RCA is used in practice and to bring to the fore the ""gap"" between theory and practice as found in the translation of RCA into healthcare. It is worth noting that our study intentionally eschews arguing whether RCA is right or wrong, or commenting on specific features of the approach. For example, we are aware that RCA is performed as a specific way of talking about, thinking of, and doing safety e what Zuiderent-Jerak, Strating, Nieboer, and Bal (2009) call a specific ""safety ontology"". This is characterised by a clinical/scientific approach (Iedema, Jorm, Long et al., 2006) and an orientation towards a ""lack view"" of safety (Mesman, 2009), that is, an approach that sees safety improvements stemming from the correction of organisational problems instead of, for instance, developing existing sources of resilience. We are also aware that RCA has been interpreted as an emerging form of self-surveillance (#CITATION_TAG, 2007) and potentially extending the principle of concertive control among healthcare practitioners (Iedema, Jorm, Long et al., 2006). However, reflecting Vincent""s (2009) exhortation that social science research should try to contribute positively to the cause of patient safety (see also Iedema, 2009), our underlying focus is to explore whether within these limits the particular way in which RCA has been translated in practice in the UK risks stifling its potential to generate organisational learning and thus safer healthcare."	1	"It is worth noting that our study intentionally eschews arguing whether RCA is right or wrong, or commenting on specific features of the approach. For example, we are aware that RCA is performed as a specific way of talking about, thinking of, and doing safety e what Zuiderent-Jerak, Strating, Nieboer, and Bal (2009) call a specific ""safety ontology"". This is characterised by a clinical/scientific approach (Iedema, Jorm, Long et al., 2006) and an orientation towards a ""lack view"" of safety (Mesman, 2009), that is, an approach that sees safety improvements stemming from the correction of organisational problems instead of, for instance, developing existing sources of resilience. We are also aware that RCA has been interpreted as an emerging form of self-surveillance (#CITATION_TAG, 2007) and potentially extending the principle of concertive control among healthcare practitioners (Iedema, Jorm, Long et al., 2006). However, reflecting Vincent""s (2009) exhortation that social science research should try to contribute positively to the cause of patient safety (see also Iedema, 2009), our underlying focus is to explore whether within these limits the particular way in which RCA has been translated in practice in the UK risks stifling its potential to generate organisational learning and thus safer healthcare."	r
CC2766	"Recent research proposes that network pictures exist on two levels: narrow or broad (Henneberg et al., 2010). Henneberg et al. (2006) suggest that network pictures collected from managers can provide an insight into the individual actor""s frame of mind (i.e. narrow network pictures), thereby provide an understanding of what they believe to be relevant and important. Thus, they are defined as managers\"" ""subjective, idiosyncratic sensemaking with regard to the main constituting characteristics of the network in which their company is operating"" (Henneberg et al., 2006, p. 409). Mouzas, Henneberg, and Naudթ (2008) argue that these individual network pictures represent not merely managers"" or companies"" views, but rather the interactions between managers, i.e. it is the clash of different network pictures that guides managerial actions. Interactions therefore cause a shared and inter-subjective understanding of the environment (Daft & Weick, 1984;#CITATION_TAG & Roberts, 1993)."	2	"Henneberg et al. (2006) suggest that network pictures collected from managers can provide an insight into the individual actor""s frame of mind (i.e. narrow network pictures), thereby provide an understanding of what they believe to be relevant and important. Thus, they are defined as managers\"" ""subjective, idiosyncratic sensemaking with regard to the main constituting characteristics of the network in which their company is operating"" (Henneberg et al., 2006, p. 409). Mouzas, Henneberg, and Naudթ (2008) argue that these individual network pictures represent not merely managers"" or companies"" views, but rather the interactions between managers, i.e. it is the clash of different network pictures that guides managerial actions. Interactions therefore cause a shared and inter-subjective understanding of the environment (Daft & Weick, 1984;#CITATION_TAG & Roberts, 1993)."	r
CC2227	The STEREO mission is contributing much to the analysis of loop morphology and geometry, thanks to its unique capability to observe the Sun simultaneously from different positions. The three-dimensional shape of magnetic loops in an active region was first stereoscopically reconstructed from two different vantage points based on simultaneously recorded STEREO/SECCHI images (Feng et al., 2007). Five relatively long loops were measured and found to be non-planar Living Reviews in Solar Physics http://www.livingreviews.org/lrsp-2014-4 Fabio Reale and more curved than field lines extrapolated from SoHO/MDI measurements, probably due to the inadequacy of the linear force-free field model used for the extrapolation. A misalignment of 20 -40 deg between theoretical model and observed loops has been quantified from STEREO results (Sandman et al., 2009;DeRosa et al., 2009). Systematic triangulations and 3D reconstructions using the Extreme UltraViolet Imager (EUVI) telescopes on both STEREO spacecrafts were used to derive loop characteristics, such as the loop plane inclination angles (#CITATION_TAG et al., 2008(Aschwanden et al., , 2009(Aschwanden et al., , 2012. Deviations from circularity within 30%, less significant from coplanarity and twisting below the threshold for kink instability, were found.	5	Five relatively long loops were measured and found to be non-planar Living Reviews in Solar Physics http://www. livingreviews.org/lrsp-2014-4 Fabio Reale and more curved than field lines extrapolated from SoHO/MDI measurements, probably due to the inadequacy of the linear force-free field model used for the extrapolation. A misalignment of 20 -40 deg between theoretical model and observed loops has been quantified from STEREO results (Sandman et al., 2009;DeRosa et al., 2009). Systematic triangulations and 3D reconstructions using the Extreme UltraViolet Imager (EUVI) telescopes on both STEREO spacecrafts were used to derive loop characteristics, such as the loop plane inclination angles (#CITATION_TAG et al., 2008(Aschwanden et al., , 2009(Aschwanden et al., , 2012. Deviations from circularity within 30%, less significant from coplanarity and twisting below the threshold for kink instability, were found.	m
CC274	"To summarize, what can be witnessed is that, in line with institutional theory (Di-Maggio and Powell, 1983;#CITATION_TAG et al., 2009;Lai et al., 2006), all the municipalities reported perceiving persuasive pressure to adopt personalization measures, both from outside the set of potential adopters (referring to norms to conform to citizens"" needs, or to be receptive towards national initiatives) as well as from within the set of potential adopters (referring to the norm to excel in relation to one""s peers). Adopters are associated with a higher perceived persuasive pressure than non-adopters. The abovementioned observations lead us to formulate the conjectures 1A and 1B."	0	"To summarize, what can be witnessed is that, in line with institutional theory (Di-Maggio and Powell, 1983;#CITATION_TAG et al., 2009;Lai et al., 2006), all the municipalities reported perceiving persuasive pressure to adopt personalization measures, both from outside the set of potential adopters (referring to norms to conform to citizens"" needs, or to be receptive towards national initiatives) as well as from within the set of potential adopters (referring to the norm to excel in relation to one""s peers). Adopters are associated with a higher perceived persuasive pressure than non-adopters. The abovementioned observations lead us to formulate the conjectures 1A and 1B."	T
CC395	"The literature states that carpool commuting trips are generally longer than the journeys of single occupant vehicle (SOV) drivers (Ferguson, 1997a). However, the relation between distance and carpooling is multifaceted. First, a driver often needs to make a detour to pick-up or drop-off the passenger. This extra travel is also known as circuity (Shoup, 1997, p. 205), andRietveld et al. (1999) estimate a travel time increase of 17% compared with solo driving (based on a limited sample). This pick-up/drop-off delay and extra travel and waiting time make carpooling less suitable for short distances. Second, the savings made by sharing travel costs increase with distance which makes carpooling more attractive for longer trips. Third, ""pool geography"" is related to distance. Finding a carpool partner with the same origin and destination zone may be difficult, especially in low-density areas (#CITATION_TAG and Lin, 1999) and at larger distances from the destination. As a result, Buliung et al. (2010) note that there is a threshold distance above which carpooling is less likely to occur, besides the positive relationship between distance and carpool propensity. They also stress the importance of the pool-size effect, which is present both at the origin and destination side of the trip. The spatial clustering of commuters at the home-end is a crucial factor in the formation of carpools, but also firm size matters since a larger pool of employees within the same work environment increases the number of potential carpool partners. Similar work schedules and higher levels of trust between colleagues (Correia and Viegas, 2011) further increase the potential of workplaces as matching places."	0	"This pick-up/drop-off delay and extra travel and waiting time make carpooling less suitable for short distances. Second, the savings made by sharing travel costs increase with distance which makes carpooling more attractive for longer trips. Third, ""pool geography"" is related to distance. Finding a carpool partner with the same origin and destination zone may be difficult, especially in low-density areas (#CITATION_TAG and Lin, 1999) and at larger distances from the destination. As a result, Buliung et al. (2010) note that there is a threshold distance above which carpooling is less likely to occur, besides the positive relationship between distance and carpool propensity. They also stress the importance of the pool-size effect, which is present both at the origin and destination side of the trip. The spatial clustering of commuters at the home-end is a crucial factor in the formation of carpools, but also firm size matters since a larger pool of employees within the same work environment increases the number of potential carpool partners."	 
CC193	"We searched MEDLINE via OvidSP, PsycINFO via OvidSP, CINAHL via EBSCO, EMBASE and Sociological Abstracts in September 2013 for articles reporting on views about or experiences of quitting without assistance. Current best practice for identifying qualitative research recommends comprehensive searches of multiple sources, balancing sensitivity against specificity to maximise number of records retrieved while reducing retrieval of records that are not relevant. [22] We used empirically derived qualitative research filters where available (MED-LINE, [23] CINAHL [24] and PsycINFO [25]) (Table 1). We complemented this search strategy by conducting ""berry picking #CITATION_TAG including grey literature searching, reference checking and author searching to uncover articles that are difficult to locate by modifying search terms and shifting searching strategies (Fig 1)."	5	"We searched MEDLINE via OvidSP, PsycINFO via OvidSP, CINAHL via EBSCO, EMBASE and Sociological Abstracts in September 2013 for articles reporting on views about or experiences of quitting without assistance. Current best practice for identifying qualitative research recommends comprehensive searches of multiple sources, balancing sensitivity against specificity to maximise number of records retrieved while reducing retrieval of records that are not relevant. [22] We used empirically derived qualitative research filters where available (MED-LINE, [23] CINAHL [24] and PsycINFO [25]) (Table 1). We complemented this search strategy by conducting ""berry picking #CITATION_TAG including grey literature searching, reference checking and author searching to uncover articles that are difficult to locate by modifying search terms and shifting searching strategies (Fig 1)."	c
CC99	In general, the Markovian approximation does not hold when long time scales of brain activity are considered. The friction force becomes retarded or frequency-dependent, and the statistics of the driving noise is neither Gaussian (Freyer et al., 2009) nor __-correlated. The dynamics incorporates extended memory and temporal non-locality. Ordinary exponential relaxation is replaced by complex, viz. Mittag-Leffler (Bianco et al., 2007a) scaling, with stretched exponential relaxation ___ exp __�(t/__) _� at microscopic scales (t < __), and asymptotical convergence to an inverse power law Novikov et al., 1997;Linkenkaer-Hansen et al., 2001;Gong et al., 2002;Freeman et al., 2003;Stam and de Bruin, 2004;Buiatti et al., 2007;van de Ville et al., 2010;#CITATION_TAG et al., 2012). For _� ___ 1, the correlation time __ C and the corresponding correlation length __ diverge. Note that correlation time __ C and characteristic time __ m , which coincide for exponential functional forms of the autocorrelation function, do not coincide for power-law ones. The CLT is violated and scale separation is lost, so that microscopic stochasticity becomes detectable at macroscopic scales (Grigolini et al., 1999). Activity undergoes anomalous diffusion with the MSD travelled by the particle no longer a linear function of time:	0	The friction force becomes retarded or frequency-dependent, and the statistics of the driving noise is neither Gaussian (Freyer et al., 2009) nor __-correlated. The dynamics incorporates extended memory and temporal non-locality. Ordinary exponential relaxation is replaced by complex, viz. Mittag-Leffler (Bianco et al., 2007a) scaling, with stretched exponential relaxation ___ exp __�(t/__) _� at microscopic scales (t < __), and asymptotical convergence to an inverse power law Novikov et al., 1997;Linkenkaer-Hansen et al., 2001;Gong et al., 2002;Freeman et al., 2003;Stam and de Bruin, 2004;Buiatti et al., 2007;van de Ville et al., 2010;#CITATION_TAG et al., 2012). For _� ___ 1, the correlation time __ C and the corresponding correlation length __ diverge. Note that correlation time __ C and characteristic time __ m , which coincide for exponential functional forms of the autocorrelation function, do not coincide for power-law ones. The CLT is violated and scale separation is lost, so that microscopic stochasticity becomes detectable at macroscopic scales (Grigolini et al., 1999).	a
CC1202	Starting in the late 1960s, the problem was reinvestigated in an effort initiated by DeWitt which led to PhD theses by��ade�_, Smarr and Eppley [142][143][#CITATION_TAG. These works implemented the ADM equations in axisymmetry using a specific type of coordinates often referred to as��ade�_ coordinates and thus evolved head-on collisions starting with Misner data, testing several gauge conditions including maximal slicing, vanishing shift and minimal distortion shift. Their equal-mass head-on collisions predict a GW energy of about 0.1 % of the total mass, albeit with uncertainties of a factor a few; for details see [36,145,146]. This value turned out to be correct within a factor of about 2.	0	Starting in the late 1960s, the problem was reinvestigated in an effort initiated by DeWitt which led to PhD theses by��ade�_, Smarr and Eppley [142][143][#CITATION_TAG. These works implemented the ADM equations in axisymmetry using a specific type of coordinates often referred to as��ade�_ coordinates and thus evolved head-on collisions starting with Misner data, testing several gauge conditions including maximal slicing, vanishing shift and minimal distortion shift. Their equal-mass head-on collisions predict a GW energy of about 0.1 % of the total mass, albeit with uncertainties of a factor a few; for details see [36,145,146]. This value turned out to be correct within a factor of about 2.	S
CC2631	Most previous comparative research documented educational differences in mortality. Only a few studies have analysed occupational class differences in mortality across Europe. These mostly focused on the 1970s, 1980s, and 1990s, and not always took economically inactive persons into account, due to lacking information on their occupational class #CITATION_TAG[9][10]. Inactive persons , generally have a higher mortality when compared with active persons from the same occupational class, and more often have a manual occupation [11]. Therefore, disregarding them in mortality analyses by occupational class can lead to an underestimation of mortality differences [11].	5	Most previous comparative research documented educational differences in mortality. Only a few studies have analysed occupational class differences in mortality across Europe. These mostly focused on the 1970s, 1980s, and 1990s, and not always took economically inactive persons into account, due to lacking information on their occupational class #CITATION_TAG[9][10]. Inactive persons , generally have a higher mortality when compared with active persons from the same occupational class, and more often have a manual occupation [11]. Therefore, disregarding them in mortality analyses by occupational class can lead to an underestimation of mortality differences [11].	e
CC2246	SoHO spectrometric data have contributed to investigate the loop thermal structure for a long time. From a differential emission measure (DEM) analysis with a forward-folding technique on SoHO/CDS data, some loops were found to be isothermal and others to have a broad DEM (#CITATION_TAG et al., 2007(Schmelz et al., , 2008. Three distinct isothermal components, reminiscent of coronal hole, quiet-Sun, and active region plasmas, were found from the analysis of an active region spectrum observed by the SoHO/SUMER (Landi and Feldman, 2008).	5	SoHO spectrometric data have contributed to investigate the loop thermal structure for a long time. From a differential emission measure (DEM) analysis with a forward-folding technique on SoHO/CDS data, some loops were found to be isothermal and others to have a broad DEM (#CITATION_TAG et al., 2007(Schmelz et al., , 2008. Three distinct isothermal components, reminiscent of coronal hole, quiet-Sun, and active region plasmas, were found from the analysis of an active region spectrum observed by the SoHO/SUMER (Landi and Feldman, 2008).	r
CC1229	Data mining is a relatively young field that has evolved primarily to aid the extraction of information from the vast amounts of data accumulated in databases and data repositories in many domains (Larose, 2005). The wide range of analytical techniques used in data mining emanate from a variety of disciplines including database systems, statistics, machine learning, visualization, logic, spatial analysis, signal processing, image analysis, information retrieval, and natural language processing, thereby making data mining itself a diverse, interdisciplinary field of study (#CITATION_TAG & Kamber, 2006). Data mining uses inductive reasoning to find strong evidence of a conclusion. While suited to big data analysis, it does not provide the statistical certainty offered by traditional statistical modelling (Nisbet et al., 2009). Algorithms typically used on educational data include the following: a) clustering techniques to identify homogenous subgroups in a dataset; b) association analysis to identify values that frequently co-occur; c) classification techniques to build models that predict membership of predefined classes in a dataset; and d) visual analytics to facilitate human analysis via interactive visual representations of the data (Baelpler & Murdoch, 2010;Romero & Ventura, 2007). A review of mining approaches used in educational data mining by Baker and Yacef (2010) identified a recent predominance of classification techniques, which are reviewed in the following section.	5	Data mining is a relatively young field that has evolved primarily to aid the extraction of information from the vast amounts of data accumulated in databases and data repositories in many domains (Larose, 2005). The wide range of analytical techniques used in data mining emanate from a variety of disciplines including database systems, statistics, machine learning, visualization, logic, spatial analysis, signal processing, image analysis, information retrieval, and natural language processing, thereby making data mining itself a diverse, interdisciplinary field of study (#CITATION_TAG & Kamber, 2006). Data mining uses inductive reasoning to find strong evidence of a conclusion. While suited to big data analysis, it does not provide the statistical certainty offered by traditional statistical modelling (Nisbet et al., 2009). Algorithms typically used on educational data include the following: a) clustering techniques to identify homogenous subgroups in a dataset; b) association analysis to identify values that frequently co-occur; c) classification techniques to build models that predict membership of predefined classes in a dataset; and d) visual analytics to facilitate human analysis via interactive visual representations of the data (Baelpler & Murdoch, 2010;Romero & Ventura, 2007).	h
CC541	"For many outside any health service it would seem axiomatic that they would be interested in the results of their ministrations and the fact that this is clearly not the case even in orthopaedics where ""The End Result Idea"" originated (Kaska & Weinstein, 1998), is a mystery beyond the reach of this paper. In the UK, however, the word ""outcomes"" has recently been heard at every level of government (Macdonald, 2014) and, as ever imperfectly translated into action or actual resources, it is helping drive the RCOM process forwards, as is excitement about ""value"" (Porter et al., 2006). As described by these authors, value is defined as ""outcomes that are important to the patient"" divided by cost. The simplicity and validity of the numerator is perhaps misleading (#CITATION_TAG, 1997), especially in mental health."	1	"For many outside any health service it would seem axiomatic that they would be interested in the results of their ministrations and the fact that this is clearly not the case even in orthopaedics where ""The End Result Idea"" originated (Kaska & Weinstein, 1998), is a mystery beyond the reach of this paper. In the UK, however, the word ""outcomes"" has recently been heard at every level of government (Macdonald, 2014) and, as ever imperfectly translated into action or actual resources, it is helping drive the RCOM process forwards, as is excitement about ""value"" (Porter et al., 2006). As described by these authors, value is defined as ""outcomes that are important to the patient"" divided by cost. The simplicity and validity of the numerator is perhaps misleading (#CITATION_TAG, 1997), especially in mental health."	 
CC2389	"The relationship between euthanasia and spiritual or existential caregiving to the patient has been examined in a study involving the last three months of life of patients of a representative panel of Belgian general practitioners (Van den Block et al. 2009). When euthanasia was performed, there was clear evidence of exceptionally high levels of spiritual or existential care. Religious or existential care is therefore not excluded at all from the care of people who request euthanasia but, rather, connected to it. Since in Belgium, which predominantly has a Catholic tradition, this type of care is more often religious than secular, we can infer that religious beliefs often do not, as a matter of course, restrain many patients from presenting requests for euthanasia. A new development is what we term ""solemn or ceremonial"" euthanasia, carried out at a predetermined time and place, among family loved ones and sometimes also in the presence of a pastor who has administered the last rites. For example, a recent documentary followed a cancer patient over the last six months of her life in palliative care and ends with her euthanasia in the supportive company of her family and friends that takes place in her home and is carried out by her general practitioner (GP) and palliative care nurse (Lanssens 2011). Another documentary showed a 90-year-old woman with refractory cancer symptoms preparing to undergo euthanasia and her interactions prior to doing so with Dr. Marc Desmet, a respected Belgian palliative care physician who is also a Jesuit priest (Gilsenan 2012). Applying Anamnestic Comparative Self Assessment (ACSA), a method to let respondents construct a personal scale of subjective well-being (#CITATION_TAG 1999b;Theuns, Hofmans;, as an instrument of spiritual care, Desmet asks his patient what was the happiest time in her life. Assisting my parents to die at home,"" she replies. Patient and doctor communicate warmly. When Dr. Desmet is asked by the interviewer Alan Gilsenan whether his religious beliefs are not an obstacle to granting euthanasia, the physician points out that according to Christian tradition one""s conscience and compassion trump doctrine. He will entrust the patient to a colleague rather than performing the euthanasia himself, but he will accompany his patients ""as far as he can go. Dr. Desmet had previously debated author JB and written a book opposing euthanasia (Desmet 2000). He now heads the ethics group of the Flemish Association for Palliative Care (Vanden Berghe et al. 2013)."	4	"A new development is what we term ""solemn or ceremonial"" euthanasia, carried out at a predetermined time and place, among family loved ones and sometimes also in the presence of a pastor who has administered the last rites. For example, a recent documentary followed a cancer patient over the last six months of her life in palliative care and ends with her euthanasia in the supportive company of her family and friends that takes place in her home and is carried out by her general practitioner (GP) and palliative care nurse (Lanssens 2011). Another documentary showed a 90-year-old woman with refractory cancer symptoms preparing to undergo euthanasia and her interactions prior to doing so with Dr. Marc Desmet, a respected Belgian palliative care physician who is also a Jesuit priest (Gilsenan 2012). Applying Anamnestic Comparative Self Assessment (ACSA), a method to let respondents construct a personal scale of subjective well-being (#CITATION_TAG 1999b;Theuns, Hofmans;, as an instrument of spiritual care, Desmet asks his patient what was the happiest time in her life. Assisting my parents to die at home,"" she replies. Patient and doctor communicate warmly. When Dr. Desmet is asked by the interviewer Alan Gilsenan whether his religious beliefs are not an obstacle to granting euthanasia, the physician points out that according to Christian tradition one""s conscience and compassion trump doctrine."	g
CC42	"Several studies including The Metric Tide [4], The Stern Report #CITATION_TAG and the HEFCE pilot study [15] all state that metrics should be used as an additional component in research evaluation, with peer review remaining as the central pillar. Yet, peer review has been shown by [16], [17] and [18] amongst others to exhibit many forms of bias including institutional bias, gender / age related bias and bias against interdisciplinary research. In an examination of one of the most critical forms of bias, that of publication bias, Emerson [19] noted that reviewers were much more likely to recommend papers demonstrating positive results over those that demonstrated null or negative results. All of the above biases exist even when peer review is carried out to the highest international standards. There were close to 1,000 peer review experts recruited by the REF, however the sheer volume of outputs requiring review calls into question the exactitude of the whole process. As an example the REF panel for UoA 9, Physics, consisted of 20 members. The total number of outputs submitted for this UoA was 6,446. Each paper is required to be read by two referees. This increases the overall total requirement to read 12,892 paper instances. Therefore each panel member was required to review, to international standards, an average of 644 papers in a little over ten months. If every panel member, worked every day for ten months, each member would need to read and review 2.14 papers per day to complete the work on time. This is, of course, in addition to the panelist""s usual full-time work load. Moreover, Physics is not an unusual example and many other UoAs tell a similar story in terms of the average number of papers each panel member was expected to review; Business and Management Studies (1,017 papers), General Engineering (868 papers), Clinical Medicine (765 papers). The burden placed on the expert reviewers during the REF process was onerous in the extreme. Coles [20] calculated a very similar figure of 2 papers per day, based on an estimate before the data we now have was available. It is blindingly obvious,"" he concluded, ""that whatever the panels do will not be a thorough peer review of each paper, equivalent to refereeing it for publication in a journal"". Sayer [21] is equally disparaging in regards to the volume of papers each reviewer was required to read and also expresses significant doubts about the level of expertise within the review panels themselves."	0	Several studies including The Metric Tide [4], The Stern Report #CITATION_TAG and the HEFCE pilot study [15] all state that metrics should be used as an additional component in research evaluation, with peer review remaining as the central pillar. Yet, peer review has been shown by [16], [17] and [18] amongst others to exhibit many forms of bias including institutional bias, gender / age related bias and bias against interdisciplinary research. In an examination of one of the most critical forms of bias, that of publication bias, Emerson [19] noted that reviewers were much more likely to recommend papers demonstrating positive results over those that demonstrated null or negative results. All of the above biases exist even when peer review is carried out to the highest international standards.	S
CC956	"Finally, development and evaluation of methods of reconstruction of order restrictions are conducted without consideration for the evolutionary model of tumor progression (but see [7,8] and Discussion). This problem is highlighted by Sprouffske et al. [37]: referring to oncogenetic tree models they say (p. 1136) ""This is not an evolutionary model because the oncogenetic tree does not represent ancestral relationships within a neoplasm but rather a summary of the observed co-occurrences of mutations across independent neoplasms"". This lack of consideration for the evolutionary model is also unfortunate since it does not provide a clear mechanistic interpretation of (nor a simple mechanistically-based procedure for generating) deviations from the restrictions encoded in the graph. Of particular interest is monotonicity (a mutation in a driver gene can only be observed if the preceding parent mutations in the graph have occurred), because deviations from it can easily arise when a mutation behaves as a driver or as a passenger depending on the genetic context -i.e., depending on which other genes are mutated [30,#CITATION_TAG]."	0	"This problem is highlighted by Sprouffske et al. [37]: referring to oncogenetic tree models they say (p. 1136) ""This is not an evolutionary model because the oncogenetic tree does not represent ancestral relationships within a neoplasm but rather a summary of the observed co-occurrences of mutations across independent neoplasms"". This lack of consideration for the evolutionary model is also unfortunate since it does not provide a clear mechanistic interpretation of (nor a simple mechanistically-based procedure for generating) deviations from the restrictions encoded in the graph. Of particular interest is monotonicity (a mutation in a driver gene can only be observed if the preceding parent mutations in the graph have occurred), because deviations from it can easily arise when a mutation behaves as a driver or as a passenger depending on the genetic context -i.e., depending on which other genes are mutated [30,#CITATION_TAG]."	a
CC2329	The ability of human listeners to adapt to distortions of speech is of interest both in relation to the effectiveness of auditory prostheses and in more basic investigations of speech perception. This ability also raises the question of how such adaptation can be facilitated through training. Much work has centered on noise or tone-vocoding, which has become popular as a technique for manipulating the level of spectral detail, and because of its similarity to the processing in cochlear implants. Listeners can adapt rapidly to low spectral resolution vocoded speech (e.g., Davis et al., 2005). However, a combination of vocoding with spectral shifting can be more challenging. When vocoded speech is spectrally shifted upward to simulate relatively shallow CI electrode insertions, shifts in excess of 3 mm of basilar membrane distance have large acute effects on speech perception (Dorman et al., 1997;#CITATION_TAG et al., 1998). These effects can be markedly reduced with training, but this requires several hours, substantially longer than for vocoding alone (Faulkner et al., 2006;Fu and Galvin, 2003;Nogaki et al., 2007;Rosen et al., 1999). Cochlear implant listeners show comparable adaptation to changes of frequency mapping (Dorman and Ketten, 2003;Fu et al., 2002), and several studies indicate that explicit speech-based training can facilitate this Fu and Galvin, 2008).	0	Much work has centered on noise or tone-vocoding, which has become popular as a technique for manipulating the level of spectral detail, and because of its similarity to the processing in cochlear implants. Listeners can adapt rapidly to low spectral resolution vocoded speech (e.g., Davis et al., 2005). However, a combination of vocoding with spectral shifting can be more challenging. When vocoded speech is spectrally shifted upward to simulate relatively shallow CI electrode insertions, shifts in excess of 3 mm of basilar membrane distance have large acute effects on speech perception (Dorman et al., 1997;#CITATION_TAG et al., 1998). These effects can be markedly reduced with training, but this requires several hours, substantially longer than for vocoding alone (Faulkner et al., 2006;Fu and Galvin, 2003;Nogaki et al., 2007;Rosen et al., 1999). Cochlear implant listeners show comparable adaptation to changes of frequency mapping (Dorman and Ketten, 2003;Fu et al., 2002), and several studies indicate that explicit speech-based training can facilitate this Fu and Galvin, 2008).	v
CC2621	"Third, another (overlapping) strand of economic geographical thinking has developed a conception of the market-as-network informed by actor-network theory (Murdoch, 1998;Callon, 1998) and which has deployed a rhizomatic concept of the network (c.f. Grabher, 2006: 166;. The rhizome metaphor is based on poststructuralist thinking (c.f. Deleuze and Guattari, 1982) that reconfigures the concept of a network to ""a multiplex, heterogenous and robust web of relations and it is a primary influence in the development of the concept of the network used by actor-network theorists (#CITATION_TAG and Hassard, 1999;Latour, 2005). ANT reconfigures the market into a socio-material concept constituted through potentially infinite networks of association (in both time and space) because it destabilises Euclidean spatial epistemology (e.g. local, regional, global scales) on at least three fronts: it introduces ""a genuine relational perception of space as topological stratifications"" (Murdoch, 1998) where ""time-space consists of multiple pleats of relations stitched together"" (Latham, 2002: 131); it breaks down established demarcations between human/non-humans (c.f. Latour, 1993); and it brings under scrutiny the qualities and nature of both the constitution and dynamism of relational associations in every part of a network (Latour, 2005). Thus, as Grabher points out, an understanding of the spatiality of markets framed by the social network and governance approach is called into question by ANT. Economic geographers have thus begun to develop the key contribution by Michel Callon in applying the insight of ANT to the economic realm. Callon uses ANT""s epistemological approach to map the multiple socio-technical and material spaces that the practices that constitute market action inhabit (Callon, 1998;Callon et al., 2007). I will further discuss ANT""s importance for conceptualising market spatiality shortly, but with regard to the notion of ""market-as-network"" it develops is important for enabling geographical thinking to develop a multi-dimensional conception of market space that moves beyond the notion of markets being composed of actors distributed in a purely physical-territorial space."	5	"Third, another (overlapping) strand of economic geographical thinking has developed a conception of the market-as-network informed by actor-network theory (Murdoch, 1998;Callon, 1998) and which has deployed a rhizomatic concept of the network (c.f. Grabher, 2006: 166;. The rhizome metaphor is based on poststructuralist thinking (c.f. Deleuze and Guattari, 1982) that reconfigures the concept of a network to ""a multiplex, heterogenous and robust web of relations and it is a primary influence in the development of the concept of the network used by actor-network theorists (#CITATION_TAG and Hassard, 1999;Latour, 2005). ANT reconfigures the market into a socio-material concept constituted through potentially infinite networks of association (in both time and space) because it destabilises Euclidean spatial epistemology (e.g. local, regional, global scales) on at least three fronts: it introduces ""a genuine relational perception of space as topological stratifications"" (Murdoch, 1998) where ""time-space consists of multiple pleats of relations stitched together"" (Latham, 2002: 131); it breaks down established demarcations between human/non-humans (c.f. Latour, 1993); and it brings under scrutiny the qualities and nature of both the constitution and dynamism of relational associations in every part of a network (Latour, 2005). Thus, as Grabher points out, an understanding of the spatiality of markets framed by the social network and governance approach is called into question by ANT. Economic geographers have thus begun to develop the key contribution by Michel Callon in applying the insight of ANT to the economic realm."	h
CC1136	"Nevertheless, the default-mode network apparently corresponds to the true default mental state in the absence of demands for operant activity by tasks and goals. An essential question is the mechanism for switching between these two attentional orientations. Spreng et al. (2010) proposed a three-network model of how this happens: the default-mode network, the dorsal attention network, and a frontoparietal control network. The first two have ""an intrinsic competitive relationship"" whereas the third serves ""as a cortical mediator linking the two networks in support of goal-directed cognitive processes"" (Spreng et al., 2010, p. 303; conceptually extended by #CITATION_TAG et al., 2012). That is, Spreng et al.""s (2010) fMRI results showed the frontoparietal control network to be activated during both autobiographical (inner-focused) and visuospatial planning (in an adaptation of the Tower of London game). That suggests that the frontoparietal control network plays a key role in the switch back and forth."	0	"Nevertheless, the default-mode network apparently corresponds to the true default mental state in the absence of demands for operant activity by tasks and goals. An essential question is the mechanism for switching between these two attentional orientations. Spreng et al. (2010) proposed a three-network model of how this happens: the default-mode network, the dorsal attention network, and a frontoparietal control network. The first two have ""an intrinsic competitive relationship"" whereas the third serves ""as a cortical mediator linking the two networks in support of goal-directed cognitive processes"" (Spreng et al., 2010, p. 303; conceptually extended by #CITATION_TAG et al., 2012). That is, Spreng et al.""s (2010) fMRI results showed the frontoparietal control network to be activated during both autobiographical (inner-focused) and visuospatial planning (in an adaptation of the Tower of London game). That suggests that the frontoparietal control network plays a key role in the switch back and forth."	 
CC1598	Over the past 20 years, optimal foraging theory (OFT) has been the basis of numerous studies. Researchers usually assume that people are seeking to maximize their rate of calorific intake when they are engaged in food-getting activities, on the further assumption that, other things being equal, natural selection will favor those individuals that are most efficient. The diet breadth model postulates that an individual will make the choice whether to exploit a particular encountered resource by determining whether the postencounter returns obtained after pursuing (if necessary) and processing it into a form in which it can be eaten will be greater than those to be obtained by ignoring that resource and looking for something better. Thus, resources can be ranked in terms of their postencounter returns. Highly ranked resources will always be taken on encounter, but lower ranked ones will be ignored. This principle is important and, in some respects, counterintuitive. Whether a resource is exploited does not depend on its own abundance but on that of the resources ranked higher than it. Resource ranks may be assessed on the basis of experimental or ethnographic work in the present (e.g., #CITATION_TAG 2002, Kaplan & Hill 1992. In terms of archaeological evidence, significant taphonomic and sampling issues potentially arise, but assuming that these can be overcome, faunal assemblages, for example, can be evaluated in terms of some measure of their likely productivity or resource rank. Because animal body size is correlated with handling costs and is readily assessable using archaeological faunal data, the proportion of large-bodied vs. small-bodied animal bones has very frequently been used as a diet-breadth measure (e.g., Broughton 1994; see also Ugan 2005; for within-species size variation see, e.g., Mannino & Thomas 2002). Stiner and colleagues (2000) have used the proportions of slow-moving vs. fast-moving (and therefore hard to catch) small game as a diet breadth measure in their studies of faunal exploitation in the east Mediterranean later Palaeolithic. Despite its simplicity, the diet breadth model has been remarkably successful in accounting for variation in faunal assemblages, especially in the context of diachronic sequences showing resource depression (e.g., Broughton 1997, Butler 2000.	0	Highly ranked resources will always be taken on encounter, but lower ranked ones will be ignored. This principle is important and, in some respects, counterintuitive. Whether a resource is exploited does not depend on its own abundance but on that of the resources ranked higher than it. Resource ranks may be assessed on the basis of experimental or ethnographic work in the present (e.g., #CITATION_TAG 2002, Kaplan & Hill 1992. In terms of archaeological evidence, significant taphonomic and sampling issues potentially arise, but assuming that these can be overcome, faunal assemblages, for example, can be evaluated in terms of some measure of their likely productivity or resource rank. Because animal body size is correlated with handling costs and is readily assessable using archaeological faunal data, the proportion of large-bodied vs. small-bodied animal bones has very frequently been used as a diet-breadth measure (e.g., Broughton 1994; see also Ugan 2005; for within-species size variation see, e.g., Mannino & Thomas 2002). Stiner and colleagues (2000) have used the proportions of slow-moving vs. fast-moving (and therefore hard to catch) small game as a diet breadth measure in their studies of faunal exploitation in the east Mediterranean later Palaeolithic.	e
CC206	Unless a paper is available in a structure format, such as an XML, there is a requirement for converting the original PDF file into full text prior to analysis. There are numerous tools available for the conversion of PDF to text files. However, automatic text extraction from PDF is known to be problematic #CITATION_TAG. Some tools for inferring the document structure, such as ParsCit [12], require initial conversion to plain text. Others, such as GROBID [13], operate directly on the PDF file.	0	Unless a paper is available in a structure format, such as an XML, there is a requirement for converting the original PDF file into full text prior to analysis. There are numerous tools available for the conversion of PDF to text files. However, automatic text extraction from PDF is known to be problematic #CITATION_TAG. Some tools for inferring the document structure, such as ParsCit [12], require initial conversion to plain text. Others, such as GROBID [13], operate directly on the PDF file.	w
CC2260	"An analogous approach is to analyze the intensity distributions. The distribution of impulsive events vs their number in the solar and stellar corona is typically described with a power law. The slope of the power law is a critical parameter to establish weather such events are able to heat the solar corona (#CITATION_TAG, 1991). In particular, a power law index of 2 is the critical value above or below which flare-like events may be able or unable, respectively, to power the whole corona (e.g., Aschwanden, 1999;Bareford et al., 2010;Tajfirouze and Safari, 2012). Unfortunately, due to the faintness of the events, the distribution of weak events is particularly difficult to derive and might even be separate from that of proper flares and microflares. A hydrodynamic model was used to simulate the UV emission of a loop system heated by nanoflares on small, spatially unresolved scales (Parenti and Young, 2008). The simulations confirmed previous results that several spectral lines have an intensity distribution that follows a power-law, in a similar way to the heating function (Hudson, 1991). However, only the high temperature lines best preserve the heating function""s power law index (especially Fe xix)."	5	An analogous approach is to analyze the intensity distributions. The distribution of impulsive events vs their number in the solar and stellar corona is typically described with a power law. The slope of the power law is a critical parameter to establish weather such events are able to heat the solar corona (#CITATION_TAG, 1991). In particular, a power law index of 2 is the critical value above or below which flare-like events may be able or unable, respectively, to power the whole corona (e.g., Aschwanden, 1999;Bareford et al., 2010;Tajfirouze and Safari, 2012). Unfortunately, due to the faintness of the events, the distribution of weak events is particularly difficult to derive and might even be separate from that of proper flares and microflares. A hydrodynamic model was used to simulate the UV emission of a loop system heated by nanoflares on small, spatially unresolved scales (Parenti and Young, 2008).	e
CC4	The ongoing collaboration between The Open University and Springer Nature has produced several semantic solutions for supporting the SN editorial team. These include the Smart Topic API [2,5], an online service for automatically tagging publications with a set of relevant topics from CSO. This API supports a number of applications, including Smart Book Recommender, Smart Topic Miner [5], the Technology-Topic Framework #CITATION_TAG, a system that forecasts the propagation of technologies across research communities, and the Pragmatic Ontology Evolution Framework [7], an approach to ontology evolution that is able to select new concepts on the basis of their contribution to specific computational tasks.	0	The ongoing collaboration between The Open University and Springer Nature has produced several semantic solutions for supporting the SN editorial team. These include the Smart Topic API [2,5], an online service for automatically tagging publications with a set of relevant topics from CSO. This API supports a number of applications, including Smart Book Recommender, Smart Topic Miner [5], the Technology-Topic Framework #CITATION_TAG, a system that forecasts the propagation of technologies across research communities, and the Pragmatic Ontology Evolution Framework [7], an approach to ontology evolution that is able to select new concepts on the basis of their contribution to specific computational tasks.	i
CC621	Most of the absorption of solar radiation takes place at the surface, while the planet is cooled mostly by the emission of radiation from greenhouse gases in the atmosphere aloft. In the global mean, about 50% of the incident solar radiation is absorbed at the surface, resulting in radiative heating. Using the numbers from above, this results in maximum theoretical work of about 83 000 TW. However, extracting work from this gradient competes with the radiative exchange, which also depletes this gradient, so a maximum power exists that can be extracted from this driver [48][49][50]#CITATION_TAG[37]. Following the same line of reasoning as in the last section, the simple model of Appendix B results in about 1/2 of the heat flux being able to be extracted to perform work at a maximum efficiency of 11.4%, yielding about 5000 TW.	1	Most of the absorption of solar radiation takes place at the surface, while the planet is cooled mostly by the emission of radiation from greenhouse gases in the atmosphere aloft. In the global mean, about 50% of the incident solar radiation is absorbed at the surface, resulting in radiative heating. Using the numbers from above, this results in maximum theoretical work of about 83 000 TW. However, extracting work from this gradient competes with the radiative exchange, which also depletes this gradient, so a maximum power exists that can be extracted from this driver [48][49][50]#CITATION_TAG[37]. Following the same line of reasoning as in the last section, the simple model of Appendix B results in about 1/2 of the heat flux being able to be extracted to perform work at a maximum efficiency of 11.4%, yielding about 5000 TW.	e
CC536	Priebe et al (#CITATION_TAG, Golden, McCabe, & Reininghaus, 2012) has reported the results of subjective quality of life items in DIALOG (see Table 1), a structured communication tool in mental health services. Some UK services have plans to pilot these for routine use.	0	Priebe et al (#CITATION_TAG, Golden, McCabe, & Reininghaus, 2012) has reported the results of subjective quality of life items in DIALOG (see Table 1), a structured communication tool in mental health services. Some UK services have plans to pilot these for routine use.	P
CC2361	1 Introduction -considering different perspectives on a complex problem Scientific evidence of climate warming and of projected resulting impacts can provide the basis for a responsible and efficient adaptation strategy if implemented in a timely and careful fashion, but can also be misused to legitimize particular interests (Arnall et al., 2014;Dietz, 2011;Neuburger, 2008). While the physical aspects of climate change are, though complex, of relatively straightforward nature, societal processes in reaction to them are contingent upon and characterized by the different interests, positions and vulnerabilities of affected groups (#CITATION_TAG et al., 2008;Sietz, 2014;Zimmerer, 1993).	0	1 Introduction -considering different perspectives on a complex problem Scientific evidence of climate warming and of projected resulting impacts can provide the basis for a responsible and efficient adaptation strategy if implemented in a timely and careful fashion, but can also be misused to legitimize particular interests (Arnall et al., 2014;Dietz, 2011;Neuburger, 2008). While the physical aspects of climate change are, though complex, of relatively straightforward nature, societal processes in reaction to them are contingent upon and characterized by the different interests, positions and vulnerabilities of affected groups (#CITATION_TAG et al., 2008;Sietz, 2014;Zimmerer, 1993).	h
CC1491	The previous discussion suggests that a value around two is a reasonable estimate of the gaps in marginal value products between non-agriculture and agriculture in US states. This raises the question what might explain such sizable gaps in the US. While the analysis so far does not rule out the possibility of mis-allocation between non-agriculture and agriculture, standard explanations for mis-allocation are geared toward developing countries with large shares of the labor force in agriculture. For example, Adamopoulous and Restuccia (2014) and Donovan (2014) point to the scale or risk of farming; Restuccia et al. (2008) and #CITATION_TAG and Teixeira (2011) to barriers of moving workers or intermediate goods between agriculture and non-agriculture; and Lagakos and Waugh (2013) to selection of the workers in the two sectors.	0	The previous discussion suggests that a value around two is a reasonable estimate of the gaps in marginal value products between non-agriculture and agriculture in US states. This raises the question what might explain such sizable gaps in the US. While the analysis so far does not rule out the possibility of mis-allocation between non-agriculture and agriculture, standard explanations for mis-allocation are geared toward developing countries with large shares of the labor force in agriculture. For example, Adamopoulous and Restuccia (2014) and Donovan (2014) point to the scale or risk of farming; Restuccia et al. (2008) and #CITATION_TAG and Teixeira (2011) to barriers of moving workers or intermediate goods between agriculture and non-agriculture; and Lagakos and Waugh (2013) to selection of the workers in the two sectors.	 
CC293	"Our hope is that, by focusing on ""agency"" alongside ""structure"" (Orlikowksi and Barley, 2001), more light will be shed on the process of technological and organisational change (#CITATION_TAG, 1985) such that, eventually, the diffusion of egovernment will be better understood. This paper seeks to examine the diffusion of a specific, fairly mature form of e-government, namely personalised egovernment, in a specific setting (municipalities in the Netherlands). The analysis reported in this paper extends the existing literature on e-government to include both ""agency"" and ""process"" aspects of e-government innovation."	0	"Our hope is that, by focusing on ""agency"" alongside ""structure"" (Orlikowksi and Barley, 2001), more light will be shed on the process of technological and organisational change (#CITATION_TAG, 1985) such that, eventually, the diffusion of egovernment will be better understood. This paper seeks to examine the diffusion of a specific, fairly mature form of e-government, namely personalised egovernment, in a specific setting (municipalities in the Netherlands). The analysis reported in this paper extends the existing literature on e-government to include both ""agency"" and ""process"" aspects of e-government innovation."	O
CC928	"Through the quantification of interspecific variation in plant functional traits, comparative plant ecology has gained momentum in explaining species distributions, tradeoffs in biochemistry and physiology, as well as whole-ecosystem processes (Westoby and Wright 2006). Lavorel et al. (2007) considered vascular plant functional traits as predictors of key vegetation responses to environmental variation and vegetation effects on ecosystem functions at diverse spatial and temporal scales. In contrast to the effort put into developing extensive international methodological protocols and databases for quantitative analysis of vascular plant traits (#CITATION_TAG et al. 2003), non-vascular plants such as cryptogams have remained mostly out of the scientists"" trait research scope to date, even though the application of bryophyte and lichen traits as drivers of large-scale biogeochemistry holds much promise. Indeed, bryophytes and lichens are of particular importance in cold biomes such as the Arctic tundra (Longton 1997), where they are key contributors to green biomass and control soil hydrology, temperatures and chemistry (Cornelissen et al. 2007). One aspect of the latter is their ability to form symbiotic relationships with cyanobacteria (Dalton and Chatfield 1985;During and Van Tooren 1990;Henriksson et al. 1987). Most bryophytes have a high water retention capacity and thus provide a stable and favourable habitat for cyanobacterial growth and N fixation activity (Dickson 2000). In lichens, cyanobacteria are a symbiont which provide their fungal partner with nitrogen from fixed atmospheric N in exchange for physical protection and-in the case of tripartite lichens-from carbohydrates in the fungi (Nash 1996). Since nitrogen is a principal limiting environmental factor for plant growth and soil organic matter turnover in polar regions (Longton 1988) and biological nitrogen fixation there occurs mostly in cyanobacteria (Solheim et al. 1996), the interaction between cryptogams and cyanobacteria forms an important field for ecological investigations."	0	"Through the quantification of interspecific variation in plant functional traits, comparative plant ecology has gained momentum in explaining species distributions, tradeoffs in biochemistry and physiology, as well as whole-ecosystem processes (Westoby and Wright 2006). Lavorel et al. (2007) considered vascular plant functional traits as predictors of key vegetation responses to environmental variation and vegetation effects on ecosystem functions at diverse spatial and temporal scales. In contrast to the effort put into developing extensive international methodological protocols and databases for quantitative analysis of vascular plant traits (#CITATION_TAG et al. 2003), non-vascular plants such as cryptogams have remained mostly out of the scientists"" trait research scope to date, even though the application of bryophyte and lichen traits as drivers of large-scale biogeochemistry holds much promise. Indeed, bryophytes and lichens are of particular importance in cold biomes such as the Arctic tundra (Longton 1997), where they are key contributors to green biomass and control soil hydrology, temperatures and chemistry (Cornelissen et al. 2007). One aspect of the latter is their ability to form symbiotic relationships with cyanobacteria (Dalton and Chatfield 1985;During and Van Tooren 1990;Henriksson et al. 1987). Most bryophytes have a high water retention capacity and thus provide a stable and favourable habitat for cyanobacterial growth and N fixation activity (Dickson 2000)."	 
CC2301	The data presented herein may have implications for understanding the mechanisms of the virus-induced exacerbations of chronic respiratory diseases, such as severe asthma [29]. Asthma has long been associated with a skewing of lung immunity away from a Th1 (i.e. IFN�_ producing) response to a high Th2 (i.e. IL-4/13 producing) response [30] and more recent work has demonstrated no significant induction of IFN�_ in BAL cells from asthma patients experimentally challenged with rhinovirus [31]. Additionally, previous work has demonstrated a defect in both epithelial [32,#CITATION_TAG] and bronchoalveolar lavage cell [29] production of type I and type III IFNs in asthma in response to viral infection. Thus, in addition to the increase of viral replication resulting from decreased IFN production, the worsening of symptoms and increased inflammatory burden associated with asthma exacerbations may also be enhanced by decreased IFN-induced PDL1 expression failing to adequately control the inflammatory response to virus. Further work will be required to fully assess this possibility.	0	The data presented herein may have implications for understanding the mechanisms of the virus-induced exacerbations of chronic respiratory diseases, such as severe asthma [29]. Asthma has long been associated with a skewing of lung immunity away from a Th1 (i.e. IFN�_ producing) response to a high Th2 (i.e. IL-4/13 producing) response [30] and more recent work has demonstrated no significant induction of IFN�_ in BAL cells from asthma patients experimentally challenged with rhinovirus [31]. Additionally, previous work has demonstrated a defect in both epithelial [32,#CITATION_TAG] and bronchoalveolar lavage cell [29] production of type I and type III IFNs in asthma in response to viral infection. Thus, in addition to the increase of viral replication resulting from decreased IFN production, the worsening of symptoms and increased inflammatory burden associated with asthma exacerbations may also be enhanced by decreased IFN-induced PDL1 expression failing to adequately control the inflammatory response to virus. Further work will be required to fully assess this possibility.	d
CC1661	Many estimates of the SCCO 2 21 have been proposed in the literature. Some early estimates include Fankhauser (1994) who reports marginal impacts of between 2 and 12 $/tCO 2 with a mean value of 5 $/tCO 2 (figures in US$1990). The Second Assessment Report from the IPCC (1996) estimates range from 1 to 34 $/tCO 2 (US$1990). Tol (1999) estimates the marginal impact to be between 2 and 6 $/tCO 2 (US$1990). #CITATION_TAG (2005) gathered over 100 estimates from 28 published studies and combined them to form a probability density function with a median of $/tCO 2 , a mean of 25 $/tCO 2 , and a 95 th percentile of $/tCO 2 . In an updated version of this meta-analysis, Tol (2008) considered 211 estimates of the SCC and found higher estimates than in the previous studies. Adjusting alternative kernel density estimators to data points, the author found that when the Gaussian distribution and the sample coefficient of variation is used (which is the case closest to the 2005 study), the distribution of the estimates has a median of 4 $/tCO , a mean of 28 $/tCO 2 and a 95 th (99 th ) percentile of 162 $/tCO 2 (552 $/tCO 2 ). The Stern Review Stern 2007, which uses the PAGE2002 model, estimates a SCCO of 85 $/tCO 2 (US$ 2000).	1	Some early estimates include Fankhauser (1994) who reports marginal impacts of between 2 and 12 $/tCO 2 with a mean value of 5 $/tCO 2 (figures in US$1990). The Second Assessment Report from the IPCC (1996) estimates range from 1 to 34 $/tCO 2 (US$1990). Tol (1999) estimates the marginal impact to be between 2 and 6 $/tCO 2 (US$1990). #CITATION_TAG (2005) gathered over 100 estimates from 28 published studies and combined them to form a probability density function with a median of $/tCO 2 , a mean of 25 $/tCO 2 , and a 95 th percentile of $/tCO 2 . In an updated version of this meta-analysis, Tol (2008) considered 211 estimates of the SCC and found higher estimates than in the previous studies. Adjusting alternative kernel density estimators to data points, the author found that when the Gaussian distribution and the sample coefficient of variation is used (which is the case closest to the 2005 study), the distribution of the estimates has a median of 4 $/tCO , a mean of 28 $/tCO 2 and a 95 th (99 th ) percentile of 162 $/tCO 2 (552 $/tCO 2 ). The Stern Review Stern 2007, which uses the PAGE2002 model, estimates a SCCO of 85 $/tCO 2 (US$ 2000).	A
CC433	"An approach by #CITATION_TAG (2003) for computing changes in beef demand is used to compute yearly import demand and export supply shifts from all major exporting and importing regions of farmed salmon worldwide. While this procedure has been used to compute demand growth in the EU and France , and to compute the size and impact of a oneoff supply shift in Chile in the early 2000s (Kinnucan and Myrland 2006), this article extends the approach to all market participants in the global salmon market. The specification of demand and supply shifts was adjusted from the previous literature so that they could be applied in an EDM (see e.g. Piggott, 1992). This permits the determination of the relative price and quantity impact from each region""s demand or supply shift. This approach requires price and quantity data, as well as appropriate elasticity parameters. Although being restrictive in its interpretative ability, this parsimonious approach allows a comparatively thorough analysis based on limited data."	2	"An approach by #CITATION_TAG (2003) for computing changes in beef demand is used to compute yearly import demand and export supply shifts from all major exporting and importing regions of farmed salmon worldwide. While this procedure has been used to compute demand growth in the EU and France , and to compute the size and impact of a oneoff supply shift in Chile in the early 2000s (Kinnucan and Myrland 2006), this article extends the approach to all market participants in the global salmon market. The specification of demand and supply shifts was adjusted from the previous literature so that they could be applied in an EDM (see e.g. Piggott, 1992). This permits the determination of the relative price and quantity impact from each region""s demand or supply shift."	A
CC749	Finally, in view of the facts that genetic manipulation of NLGNs results in ASD-like phenotypes with altered E/I balance in mouse models (Chubykin et al., 2007;Tabuchi et al., 2007;Etherton et al., 2011) and NLGN mRNA translation is enhanced concomitant with increased E/I ratio in Eif4ebp2 knockout mice, Gkogkas et al. tested the effect of NLGN knockdown on synaptic plasticity and behaviour in these mice (Gkogkas et al., 2013). NLGN1 is predominantly postsynaptic at excitatory synapses and promotes excitatory synaptic transmission (#CITATION_TAG et al., 2006;Kwon et al., 2012). The authors found that NLGN1 knockdown reverses changes at excitatory synapses and partially rescues the social interaction deficits in Eif4ebp2 knockout mice (Gkogkas et al., 2013). These findings thus established a strong link between eIF4E-dependent translational control of NLGNs, E/I balance and the development of ASD-like animal behaviors (Figure 1).	0	Finally, in view of the facts that genetic manipulation of NLGNs results in ASD-like phenotypes with altered E/I balance in mouse models (Chubykin et al., 2007;Tabuchi et al., 2007;Etherton et al., 2011) and NLGN mRNA translation is enhanced concomitant with increased E/I ratio in Eif4ebp2 knockout mice, Gkogkas et al. tested the effect of NLGN knockdown on synaptic plasticity and behaviour in these mice (Gkogkas et al., 2013). NLGN1 is predominantly postsynaptic at excitatory synapses and promotes excitatory synaptic transmission (#CITATION_TAG et al., 2006;Kwon et al., 2012). The authors found that NLGN1 knockdown reverses changes at excitatory synapses and partially rescues the social interaction deficits in Eif4ebp2 knockout mice (Gkogkas et al., 2013). These findings thus established a strong link between eIF4E-dependent translational control of NLGNs, E/I balance and the development of ASD-like animal behaviors (Figure 1).	L
CC608	"One fundamental theory in physics that allows us to address these questions is thermodynamics. Central to thermodynamics are the first and second laws. While the first law of thermodynamics quantifies how much work can be extracted from gradients in heating and cooling under the constraint of energy conservation, the second law tells us about the irreversibility of processes and thereby provides us with an ""arrow of time"" [11]. Most common applications of thermodynamics are usually found in engineering, for instance the typical applications to the Carnot cycle of heat engines and refrigerators. Thermodynamics has also been applied to physical processes of the Earth system, e.g. the generation and dissipation of atmospheric motion [12] and associated optimality [13][14][15], the intensity of hurricanes [16], hydrological processes at the land surface [17,18], the atmospheric branch of hydrologic cycle [19,20], ocean dynamics [21,#CITATION_TAG], and, obviously, geochemical transformations. These processes generally operate away from a state of thermodynamic equilibrium and there is some evidence that these operate in steady states at which they maximize their dissipative activity, or, almost equivalently, maximize power generation or entropy production (the proposed Maximum Entropy Production (MEP) principle, [23][24][25][26][27])."	5	"Central to thermodynamics are the first and second laws. While the first law of thermodynamics quantifies how much work can be extracted from gradients in heating and cooling under the constraint of energy conservation, the second law tells us about the irreversibility of processes and thereby provides us with an ""arrow of time"" [11]. Most common applications of thermodynamics are usually found in engineering, for instance the typical applications to the Carnot cycle of heat engines and refrigerators. Thermodynamics has also been applied to physical processes of the Earth system, e.g. the generation and dissipation of atmospheric motion [12] and associated optimality [13][14][15], the intensity of hurricanes [16], hydrological processes at the land surface [17,18], the atmospheric branch of hydrologic cycle [19,20], ocean dynamics [21,#CITATION_TAG], and, obviously, geochemical transformations. These processes generally operate away from a state of thermodynamic equilibrium and there is some evidence that these operate in steady states at which they maximize their dissipative activity, or, almost equivalently, maximize power generation or entropy production (the proposed Maximum Entropy Production (MEP) principle, [23][24][25][26][27])."	m
CC614	"As noted by Boltzmann [28] and Schroedinger [29], thermodynamics must apply to the living organisms as well. Life feeds on low entropy ""food"" and rejects high entropy ""waste"" products and heat, thereby fueling its metabolism and sustaining its structure. Living organisms, just as any other dissipative process, maintain their state away from thermodynamic equilibrium by an overall net export of entropy to the surroundings. This commonality of living organisms and purely physical dissipative processes led Lovelock and Margulis [30] to use the metaphor of describing Earth as a superorganism. To characterize the organization of the steady state that describes the sum of all living organisms, in an ecosystem at the small scale, or in the biosphere at the global scale, the same thermodynamic maximization principles have been proposed, first by Lotka [31,#CITATION_TAG], and further explored by others [33][34][35][36][37][38][39]."	0	"Life feeds on low entropy ""food"" and rejects high entropy ""waste"" products and heat, thereby fueling its metabolism and sustaining its structure. Living organisms, just as any other dissipative process, maintain their state away from thermodynamic equilibrium by an overall net export of entropy to the surroundings. This commonality of living organisms and purely physical dissipative processes led Lovelock and Margulis [30] to use the metaphor of describing Earth as a superorganism. To characterize the organization of the steady state that describes the sum of all living organisms, in an ecosystem at the small scale, or in the biosphere at the global scale, the same thermodynamic maximization principles have been proposed, first by Lotka [31,#CITATION_TAG], and further explored by others [33][34][35][36][37][38][39]."	h
CC789	The association between wear time and PA and SED were investigated using linear mixed model regression analyses including a random intercept for subjects. Differences between week-and weekend days were tested by including a dummy variable (week-vs. weekend day) in the above-mentioned model (i.e., adjustment were made for wear time). Residuals were normally distributed in all models. Findings were reported as regression coefficients and 95% confidence intervals (CI). Further, we used variance partitioning obtained from a two-way mixed effect model reporting the ratio of week-weekend variance to the total variance (week-weekend variance/(between subject variance + week-weekend variance + residual variance)), including wear time as a fixed effect #CITATION_TAG. A wear time criterion of !10 hours/day was used for these analyses.	5	weekend day) in the above-mentioned model (i.e., adjustment were made for wear time). Residuals were normally distributed in all models. Findings were reported as regression coefficients and 95% confidence intervals (CI). Further, we used variance partitioning obtained from a two-way mixed effect model reporting the ratio of week-weekend variance to the total variance (week-weekend variance/(between subject variance + week-weekend variance + residual variance)), including wear time as a fixed effect #CITATION_TAG. A wear time criterion of ! 10 hours/day was used for these analyses.	e
CC1989	"The term ""effort which received an early definition by Fenichel [12] as ""a set of search variables [including] e.g. number of commands and descriptors [and] connect time is quite often considered in the more general literature on information seeking behavior, with this or a variety of other, more or less similar definitions. Zippf\""s ""law of least effort"" is often invoked to explain users\"" choice of information channel [13] , which refers to a number of studies who take this perspective. When effort is considered in the more restricted environment of information search behavior, however, it is often relatively vaguely defined. Typically, it is treated as in [14], where, in an investigation of the influence of user experience on search outcomes, effort is considered as one of several ""search language use patterns"" and defined to consist of ""mean number of cycles per topic, mean command frequency per topic, and mean number of documents visited per cycle"" without any motivation for this choice of parameters. A number of authors invoke ""cognitive effort"" as distinct from observable, logged actions in their characterization of search [15]. Cognitive effort is a concept well known from fields such as psychology and decision theory, but as a parameter of search effort it is often treated with a similar lack of specific definition as the concept of effort in general. Where it is defined the measurement definitions range widely, from ""pupil dilation"" in an eye-tracking study of search and evaluation behavior [16] to ""number of iterations, i.e. queries in a search"" [17]. The term transition, or parallel expressions such as shifts, state changes etc. is widely used in both the general literature on information seeking and more specifically in studies of information search behavior. It is generally defined in terms of a move from one state to another (or a sequence of such moves). Stages or patterns of stages appear in more and more fine-grained form in models of information seeking behavior from Ellis"" and others"" early models [#CITATION_TAG,19], and are becoming more and more fine-grained, as in Xie [20], where the interest is in shifting patterns between search stages. Such stages may be identified for instance in information seeking mediation, as in [21] where stages are identified as sets of cognitive and operational elements and transitions between stages are identified through vocabulary changes in dialogue. Transitions have been of particular interest to studies of search system interactions, where it has been thought that being able to detect transitions or distinct shifts in interaction would enable the automatic detection of patterns that might engender some kind of machine assistance or inform interface design. Variants of Markov modeling have often been suggested for such modeling, in [22] weaknesses of this approach is discussed, and an alternative modeling approach with Petri nets are suggested. In this paper and many others the transitions themselves are vaguely defined, and this is a persistent problem in the literature."	2	"Where it is defined the measurement definitions range widely, from ""pupil dilation"" in an eye-tracking study of search and evaluation behavior [16] to ""number of iterations, i.e. queries in a search"" [17]. The term transition, or parallel expressions such as shifts, state changes etc. is widely used in both the general literature on information seeking and more specifically in studies of information search behavior. It is generally defined in terms of a move from one state to another (or a sequence of such moves). Stages or patterns of stages appear in more and more fine-grained form in models of information seeking behavior from Ellis"" and others"" early models [#CITATION_TAG,19], and are becoming more and more fine-grained, as in Xie [20], where the interest is in shifting patterns between search stages. Such stages may be identified for instance in information seeking mediation, as in [21] where stages are identified as sets of cognitive and operational elements and transitions between stages are identified through vocabulary changes in dialogue. Transitions have been of particular interest to studies of search system interactions, where it has been thought that being able to detect transitions or distinct shifts in interaction would enable the automatic detection of patterns that might engender some kind of machine assistance or inform interface design. Variants of Markov modeling have often been suggested for such modeling, in [22] weaknesses of this approach is discussed, and an alternative modeling approach with Petri nets are suggested."	 
CC784	"In adults, !3-5 days of monitoring are normally considered appropriate, which is in accordance with recommendations given [2]. However, estimates of how many days of monitoring that should be included to obtain a reliable result vary considerably between studies [3-7, 2, 8], and might also vary between outcome variables of interest [#CITATION_TAG,8]. According to Matthews et al [6], inclusion of more days may be needed to arrive at reliable estimates (intraclass correlation coefficient (ICC) !0.80) for ""physical inactivity"" (<500 cpm from the Actigraph 7164) (!7 days), compared to PA (! 3-4 days). A comparable finding has been shown in older adults, where 2-3 days was needed for PA, whereas 5 days of monitoring was needed for SED (<50 cpm from the Actigraph 7164). The possible impaired reliability for SED compared to other variables may be of critical importance, given the increased interest in SED in the primary and secondary prevention of a range of chronic diseases as well as premature death [9][10][11]. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included !3-5 days of measurement [12][13][14][15][16][17], with some exceptions (!1 day [18]; !6-7 days [19,20]). Moreover, as SED are likely to be related to wear time, correction for wear time might improve reliability. Consistent with this hypothesis, percent SED has previously been shown to be superior to minutes of SED as a predictor of metabolic risk [18]."	4	"In adults, ! 3-5 days of monitoring are normally considered appropriate, which is in accordance with recommendations given [2]. However, estimates of how many days of monitoring that should be included to obtain a reliable result vary considerably between studies [3-7, 2, 8], and might also vary between outcome variables of interest [#CITATION_TAG,8]. According to Matthews et al [6], inclusion of more days may be needed to arrive at reliable estimates (intraclass correlation coefficient (ICC) ! 0.80) for ""physical inactivity"" (<500 cpm from the Actigraph 7164) (! 7 days), compared to PA (!"	w
CC1793	"A critical feature of CL explanation is that it abstracts away from details of processing and implementation. To illustrate, consider the motivation given by Xu and Tenenbaum (2007) in developing their Bayesian account of word learning. The authors\"" aim is ""to understand in functional terms how implicit knowledge and inferential machinery guide people in generalising from examples-rather than to describe precisely the psychological processes involved"" (p. 251). In other words, their aim is to present an account of their domain of interest (learning the meanings of words) that captures behavioral regularities while abstracting away from specific algorithmic and representational commitments. Similar appeals to the utility of CL explanation are common in the Bayesian literature (see, e.g., #CITATION_TAG, 2006). The emphasis is generally on how the CL account may serve as a highly abstract statement of theory that deliberately avoids lower level commitments. This appeal to abstraction is also apparent in Marr""s original account of CL explanation. Specifically, his attack on the insufficiency of early AI programs as explanations included the complaint that ""particular data structures, such as lists of attribute value pairs called property lists in the LISP programming language, were held to amount to theories of the representation of knowledge"" (Marr, 1982, p. 28). By abstracting away from specific representational devices (such as LISP data structures), CL accounts side-step representational debates (including, e.g., that underlying the connectionist/symbolic divide)."	0	"The authors\"" aim is ""to understand in functional terms how implicit knowledge and inferential machinery guide people in generalising from examples-rather than to describe precisely the psychological processes involved"" (p. 251). In other words, their aim is to present an account of their domain of interest (learning the meanings of words) that captures behavioral regularities while abstracting away from specific algorithmic and representational commitments. Similar appeals to the utility of CL explanation are common in the Bayesian literature (see, e.g., #CITATION_TAG, 2006). The emphasis is generally on how the CL account may serve as a highly abstract statement of theory that deliberately avoids lower level commitments. This appeal to abstraction is also apparent in Marr""s original account of CL explanation. Specifically, his attack on the insufficiency of early AI programs as explanations included the complaint that ""particular data structures, such as lists of attribute value pairs called property lists in the LISP programming language, were held to amount to theories of the representation of knowledge"" (Marr, 1982, p. 28)."	a
CC2030	Stratigraphic, sedimentologic and geochronologic data indicate that there may be six distinct periods of eolian deposition in the Cape Cod dune field (Figure 9). The oldest recognized depositional event is at the base of the Highway 6 section, where quartz grains yielded an OSL age of 3.7 �� 0.3 ka (Figure 6). This is a single site, with one OSL age, and thus it is unknown how pervasive is this early eolian depositional event. Lacustrine sediment cores from the northeastern U.S. show a decline in Hemlock (Tsuga sp.), a decrease in lake level, or an increase in clastic sedimentation ca. 5.5-3.8 5.5-3.8 ka and is associated with a broad scale drying (e.g., Newby et al., 2000;Shuman et al., 2001;Shuman and Donnelly, 2006;Oswald et al., 2007;#CITATION_TAG et al., 2013). Recent analysis of paleolimnologic data indicates that this drying is associated with cooling of coastal air by 0.5-2.5 _�� C and a precipitation decrease of ___100 mm, linked to cooler surface temperatures in the North Atlantic Ocean (Marsicek et al., 2013). Wetter conditions prevailed post 3.8 ka (cf. Shuman and Donnelly, 2006;Oswald et al., 2007;Marsicek et al., 2013) (Figure 9) with an inferred period of heightened hurricane activity ca. 3.6-3.4 ka (Toomey et al., 2013). Cores from the Makepeace Cedar Swamp in far eastern Massachusetts indicate the onset of the wettest conditions post-glacial at ca. 3.2 ka (Newby et al., 2000). The penultimate eolian depositional event is represented by only the Trailhead site with two OSL ages of ca. 2.4 ka (Figure 6). The spatial extent of this eolian depositional event is unknown though these ages may be coincident with a pronounced period of increased hurricane activity in the North Atlantic Ocean (Toomey et al., 2013), but with dry and cool conditions on Cape Cod (Shuman and Donnelly, 2006;Marsicek et al., 2013) (Figure 9). It is salient to note that there is an absence in the presented stratigraphic record of eolian sand deposited ca. 3 ka ago (Figure 9), when inferred hurricane activity was heightened (Toomey et al., 2013).	1	This is a single site, with one OSL age, and thus it is unknown how pervasive is this early eolian depositional event. Lacustrine sediment cores from the northeastern U.S. show a decline in Hemlock (Tsuga sp. ), a decrease in lake level, or an increase in clastic sedimentation ca. 5.5-3.8 5.5-3.8 ka and is associated with a broad scale drying (e.g., Newby et al., 2000;Shuman et al., 2001;Shuman and Donnelly, 2006;Oswald et al., 2007;#CITATION_TAG et al., 2013). Recent analysis of paleolimnologic data indicates that this drying is associated with cooling of coastal air by 0.5-2.5 _�� C and a precipitation decrease of ___100 mm, linked to cooler surface temperatures in the North Atlantic Ocean (Marsicek et al., 2013). Wetter conditions prevailed post 3.8 ka (cf. Shuman and Donnelly, 2006;Oswald et al., 2007;Marsicek et al., 2013) (Figure 9) with an inferred period of heightened hurricane activity ca. 3.6-3.4 ka (Toomey et al., 2013).	.
CC2472	The notion of periodicity in words and its many variants have been well-studied in numerous fields like combinatorics on words, pattern matching, data compression, automata theory, formal language theory, and molecular biology (see [9]). However the classic notion of periodicity is too restrictive to provide a description of a word such as abaababaaba, which is covered by copies of aba, yet not exactly periodic. To fill this gap, the idea of quasiperiodicity was introduced #CITATION_TAG. In a periodic word, the occurrences of the period do not overlap. In contrast, the occurrences of a quasiperiod in a quasiperiodic word may overlap. Quasiperiodicity thus enables the detection of repetitive structures that would be ignored by the classic characterization of periods.	0	The notion of periodicity in words and its many variants have been well-studied in numerous fields like combinatorics on words, pattern matching, data compression, automata theory, formal language theory, and molecular biology (see [9]). However the classic notion of periodicity is too restrictive to provide a description of a word such as abaababaaba, which is covered by copies of aba, yet not exactly periodic. To fill this gap, the idea of quasiperiodicity was introduced #CITATION_TAG. In a periodic word, the occurrences of the period do not overlap. In contrast, the occurrences of a quasiperiod in a quasiperiodic word may overlap. Quasiperiodicity thus enables the detection of repetitive structures that would be ignored by the classic characterization of periods.	 
CC797	"In adults, !3-5 days of monitoring are normally considered appropriate, which is in accordance with recommendations given [2]. However, estimates of how many days of monitoring that should be included to obtain a reliable result vary considerably between studies [3-7, 2, 8], and might also vary between outcome variables of interest [6,8]. According to Matthews et al [6], inclusion of more days may be needed to arrive at reliable estimates (intraclass correlation coefficient (ICC) !0.80) for ""physical inactivity"" (<500 cpm from the Actigraph 7164) (!7 days), compared to PA (! 3-4 days). A comparable finding has been shown in older adults, where 2-3 days was needed for PA, whereas 5 days of monitoring was needed for SED (<50 cpm from the Actigraph 7164). The possible impaired reliability for SED compared to other variables may be of critical importance, given the increased interest in SED in the primary and secondary prevention of a range of chronic diseases as well as premature death [9][10][11]. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included !3-5 days of measurement [12][13][14][15][16]#CITATION_TAG, with some exceptions (!1 day [18]; !6-7 days [19,20]). Moreover, as SED are likely to be related to wear time, correction for wear time might improve reliability. Consistent with this hypothesis, percent SED has previously been shown to be superior to minutes of SED as a predictor of metabolic risk [18]."	0	A comparable finding has been shown in older adults, where 2-3 days was needed for PA, whereas 5 days of monitoring was needed for SED (<50 cpm from the Actigraph 7164). The possible impaired reliability for SED compared to other variables may be of critical importance, given the increased interest in SED in the primary and secondary prevention of a range of chronic diseases as well as premature death [9][10][11]. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included ! 3-5 days of measurement [12][13][14][15][16]#CITATION_TAG, with some exceptions (! 1 day [18]; ! 6-7 days [19,20]). Moreover, as SED are likely to be related to wear time, correction for wear time might improve reliability.	f
CC2186	"In view of the increasing range of sophisticated technology being developed to support people with communication impairments, it is important to examine published work regarding the interventions using these devices. Evaluation of study findings is needed in order to provide evidence-based information for funders, potential users, and service managers, and to underpin evidence-based practice amongst speech-language pathologists [8] . This review therefore was undertaken as a ""state of the art"" review #CITATION_TAG to present an assessment of the current state of knowledge in the field. The work encompassed both quantitative intervention studies and qualitative papers reporting views of service users and providers. Findings regarding the qualitative studies are reported elsewhere [10] . In this paper we consider primary studies reporting evaluations of interventions."	5	"In view of the increasing range of sophisticated technology being developed to support people with communication impairments, it is important to examine published work regarding the interventions using these devices. Evaluation of study findings is needed in order to provide evidence-based information for funders, potential users, and service managers, and to underpin evidence-based practice amongst speech-language pathologists [8] This review therefore was undertaken as a ""state of the art"" review #CITATION_TAG to present an assessment of the current state of knowledge in the field. The work encompassed both quantitative intervention studies and qualitative papers reporting views of service users and providers. Findings regarding the qualitative studies are reported elsewhere [10] . In this paper we consider primary studies reporting evaluations of interventions."	i
CC1673	"For the two-population simulations, ancestral haplotypes were chosen from the International HapMap3 project (#CITATION_TAG et al. 2010) (http://hapmap.ncbi.nlm.nih.gov/), with one ancestor chosen from Utah individuals of European descent (CEU) and the other from Yoruba individuals from Nigeria (YRI), simulating the largely West African ancestry of African American individuals (Salas et al. 2005). The distribution of recombination breakpoints was generated by using eight generations since admixture (G �_ 8), and genomic segments were chosen from ancestors according to freq(YRI) �_ Beta (12,3) to model the average African ancestry proportion among African Americans, approximately 80% (Bryc et al. 2010). Variants with r 2 �_ 0.80 were removed, and 12 simulated haplotypes were analyzed with PCAdmix and HAPMIX by using �� �_ 8, the true parameter value. PCAdmix was run with a window size of 20 SNPs. We also tested our method""s robustness to the choice of parameter values by allowing �� to vary from 1 to 128, the number of SNPs per window to vary from 1 to 160, and the LD filtering to vary from an r 2 threshold of 0.80 to no LD filtering. Finally, we examined our method""s robustness to the choice of ancestral representatives by analyzing the simulated haplotypes with the true populations (YRI and CEU), sets of three populations [YRI, CEU, and Han Chinese and Japanese (CHB-JPT) or Italian (TSI)], and misspecified ancestral populations by using Luhya (LWK) or Maasai (MKK) to represent YRI ancestry. These populations are likely to be poor ancestral proxies because they are East African populations, whereas the simulated haplotypes were sampled from individuals from a West African population, the Yoruba. The F ST between Luhya and Yoruba is 0.0080 and between Maasai and Yoruba is 0.027 (Altshuler et al. 2010)."	5	For the two-population simulations, ancestral haplotypes were chosen from the International HapMap3 project (#CITATION_TAG et al. 2010) (http://hapmap. ncbi. nlm. nih.gov/), with one ancestor chosen from Utah individuals of European descent (CEU) and the other from Yoruba individuals from Nigeria (YRI), simulating the largely West African ancestry of African American individuals (Salas et al. 2005).	F
CC714	The bigest strength of our analysis is the utilisation of efficacy data derived from a systematic literature review and NMA. This methodology enabled us to consider information from both direct and indirect comparisons between interventions, and allowed simultaneous inference on all treatment options examined in trial pair-wise comparisons while preserving randomisation [20,#CITATION_TAG]. This approach for evidence synthesis is essential for populating model-based economic studies assessing more than two competing interventions. The NMA principally utilised continuous data to estimate the relative treatment effects of interventions, and then transformed the estimated SMDs into probabilities of recovery. Such a transformation is valid as long as the assumed relationship between the treatment effect based on continuous data and the treatment effect estimated using recovery data holds. This assumption could not be checked for all interventions, but available data indicated a strong relationship and therefore this transformation is unlikely to have introduced substantial bias into the analysis [14]. These assumptions along with the limitations of the NMA model and the limitations of the RCTs considered in the NMA [14] may have impacted on the quality of the respective input parameters used to populate the economic model.	5	The bigest strength of our analysis is the utilisation of efficacy data derived from a systematic literature review and NMA. This methodology enabled us to consider information from both direct and indirect comparisons between interventions, and allowed simultaneous inference on all treatment options examined in trial pair-wise comparisons while preserving randomisation [20,#CITATION_TAG]. This approach for evidence synthesis is essential for populating model-based economic studies assessing more than two competing interventions. The NMA principally utilised continuous data to estimate the relative treatment effects of interventions, and then transformed the estimated SMDs into probabilities of recovery. Such a transformation is valid as long as the assumed relationship between the treatment effect based on continuous data and the treatment effect estimated using recovery data holds.	h
CC2213	A first step to modeling fine-structured loops is to use multistrand static models. Such models show some substantial inconsistencies with observations, e.g., in general they predict too large loop cross sections (Reale and Peres, 2000). Such strands are conceptually different from the thin strands predicted in the nanoflare scenario (#CITATION_TAG, 1988), which imply a highly dynamic evolution due to pulsed-heating. The nanoflare scenario is approached in multi-thread loop models, convolving the independent hydrodynamic evolution of the plasma confined in each pulse-heated strand (see Section 4.3). These are able to match some more features of the evolution of warm loops observed with TRACE (Warren et al., 2002(Warren et al., , 2003Winebarger et al., 2003b,a). According to detailed hydrodynamic loop modeling, an ensemble of independently heated strands can be significantly brighter than a static uniformly heated loop and would have a flat filter ratio temperature when observed with TRACE (Warren et al., 2002). As an extension, time-dependent hydrodynamic modeling of an evolving active region loop observed with TRACE showed that a loop made as a set of small-scale, impulsively heated strands can generally reproduce the spatial and temporal properties of the observed loops, such as a delay between the appearance of the loop in different filters (Warren et al., 2003). An evolution of this approach was to model an entire active region for comparison with a SoHO/EIT observation (Warren and Winebarger, 2006); the modeling includes extrapolating the magnetic field and populating the field lines with solutions to the hydrostatic loop equations assuming steady, uniform heating. The result was the link between the heating rate and the magnetic field and size of the structures, but there were also significant discrepancies with the observed EIT emission.	4	A first step to modeling fine-structured loops is to use multistrand static models. Such models show some substantial inconsistencies with observations, e.g., in general they predict too large loop cross sections (Reale and Peres, 2000). Such strands are conceptually different from the thin strands predicted in the nanoflare scenario (#CITATION_TAG, 1988), which imply a highly dynamic evolution due to pulsed-heating. The nanoflare scenario is approached in multi-thread loop models, convolving the independent hydrodynamic evolution of the plasma confined in each pulse-heated strand (see Section 4.3). These are able to match some more features of the evolution of warm loops observed with TRACE (Warren et al., 2002(Warren et al., , 2003Winebarger et al., 2003b,a). According to detailed hydrodynamic loop modeling, an ensemble of independently heated strands can be significantly brighter than a static uniformly heated loop and would have a flat filter ratio temperature when observed with TRACE (Warren et al., 2002).	c
CC1966	Clinical trials in TSC populations using everolimus (RAD001, Novartis), as mTOR inhibitor, have shown promising results regarding epilepsy, skin manifestations, subependymal giant cell astrocytomas (SEGAs), and renal and lung manifestations (#CITATION_TAG et al., 2008) and also improved subependymal giant cell astrocytomas (SEGAs), specific brain tumors associated with TSC (Krueger et al, 2010;J�_zwiak et al 2012;Curran et al 2012;Franz et al, 2013). In fact, everolimus has already been approved to treat this kind of TSC lesions (Kohrman, 2012).	0	Clinical trials in TSC populations using everolimus (RAD001, Novartis), as mTOR inhibitor, have shown promising results regarding epilepsy, skin manifestations, subependymal giant cell astrocytomas (SEGAs), and renal and lung manifestations (#CITATION_TAG et al., 2008) and also improved subependymal giant cell astrocytomas (SEGAs), specific brain tumors associated with TSC (Krueger et al, 2010;J�_zwiak et al 2012;Curran et al 2012;Franz et al, 2013). In fact, everolimus has already been approved to treat this kind of TSC lesions (Kohrman, 2012).	C
CC528	NGC 1333 is one of the youngest and most well-studied star forming regions, in part because it is located at only ___235 pc (Hirota et al. 2008(Hirota et al. , 2011. Its stars are thought to have an average age of 1-2 Myr (e.g., #CITATION_TAG et al. 2008), but it also contains several Class 0 objects, objects in the earliest stages of star formation (see, e.g., Sadavoy et al. 2014or Sandell & Knee 2001.	0	NGC 1333 is one of the youngest and most well-studied star forming regions, in part because it is located at only ___235 pc (Hirota et al. 2008(Hirota et al. , 2011. Its stars are thought to have an average age of 1-2 Myr (e.g., #CITATION_TAG et al. 2008), but it also contains several Class 0 objects, objects in the earliest stages of star formation (see, e.g., Sadavoy et al. 2014or Sandell & Knee 2001.	t
CC1637	"Ratchet effects have received surprisingly little empirical study beyond Roy""s (1952) famous description (#CITATION_TAG et al. 2012). Some theoretical analysis exists (Lazear 1986;Gibbons 1987). Given the many parameters of an incentive system that may need to be changed, ratchet effects are likely a common concern in implementing incentive systems. The famous Lincoln Electric case (Milgrom & Roberts 1995) provides insights into how a firm might reduce ratchet effects. The company has an explicit policy of not changing piece rates unless a change occurs in methods of production. Piece rates are set by a special department with great expertise at setting accurate rates (they set approximately 10,000 piece rates per year, according to a company executive I know). Employees are allowed to challenge piece rates, and their complaints are taken seriously. Expected compensation is pegged to wage data from the Bureau of Labor Statistics, eliminating discretion over one determinant of piece rates. Top managers have strong pay for performance, which reduces their temptation to lower compensation for employees in ways that reduce firm performance."	5	"Ratchet effects have received surprisingly little empirical study beyond Roy""s (1952) famous description (#CITATION_TAG et al. 2012). Some theoretical analysis exists (Lazear 1986;Gibbons 1987). Given the many parameters of an incentive system that may need to be changed, ratchet effects are likely a common concern in implementing incentive systems. The famous Lincoln Electric case (Milgrom & Roberts 1995) provides insights into how a firm might reduce ratchet effects."	R
CC51	"The NACP was formally recognized by the United States in 2002 under the mantle of the nation""s overall climate change management strategy (Wofsy and Harriss, 2002). The first implementation plan for the NACP put forward a research agenda that was centered on quantifying and understanding carbon sinks and sources in North America and surrounding oceans, and the integration of such information into socially, economically, and politically relevant decision-support systems (Sarmiento and Wofsy, 1999;Wofsy and Harriss, 2002). The State of the Carbon Cycle Report established that North America is a net source of CO2 to the atmosphere, due to fossil-fuel emissions and that there are globally important carbon sinks whose future are highly uncertain (#CITATION_TAG et al., 2007). Understanding how humans both experience and influence the carbon cycle and climate change is critical to the interests of decision makers (Bernabo, 1995;Feldman and Ingram, 2009), such as those who confer support upon the member agencies of the NACP through funding and other resources."	0	"The NACP was formally recognized by the United States in 2002 under the mantle of the nation""s overall climate change management strategy (Wofsy and Harriss, 2002). The first implementation plan for the NACP put forward a research agenda that was centered on quantifying and understanding carbon sinks and sources in North America and surrounding oceans, and the integration of such information into socially, economically, and politically relevant decision-support systems (Sarmiento and Wofsy, 1999;Wofsy and Harriss, 2002). The State of the Carbon Cycle Report established that North America is a net source of CO2 to the atmosphere, due to fossil-fuel emissions and that there are globally important carbon sinks whose future are highly uncertain (#CITATION_TAG et al., 2007). Understanding how humans both experience and influence the carbon cycle and climate change is critical to the interests of decision makers (Bernabo, 1995;Feldman and Ingram, 2009), such as those who confer support upon the member agencies of the NACP through funding and other resources."	e
CC2816	The key elements associated with successful schemes include careful job placement, prior job training, advocacy, follow-up monitoring and long-term support to ensure job retention (Keel et al., 1997;Mawhood and Howlin, 1999; #CITATION_TAG and Rusch, 1989;Wehman and Kregel, 1985).	0	The key elements associated with successful schemes include careful job placement, prior job training, advocacy, follow-up monitoring and long-term support to ensure job retention (Keel et al., 1997;Mawhood and Howlin, 1999; #CITATION_TAG and Rusch, 1989;Wehman and Kregel, 1985).	T
CC292	Implementing personalised electronic government services is far from trivial and the literature reports many obstacles. These include socio-political problems of information sharing across traditional organisational boundaries (Mulgan, 2005;Homburg, 2008), the existence of legacy systems (#CITATION_TAG et al., 2007) and concerns related to privacy issues (Lips et al., 2004).	0	Implementing personalised electronic government services is far from trivial and the literature reports many obstacles. These include socio-political problems of information sharing across traditional organisational boundaries (Mulgan, 2005;Homburg, 2008), the existence of legacy systems (#CITATION_TAG et al., 2007) and concerns related to privacy issues (Lips et al., 2004).	h
CC1431	"Theories and models that endorse or assume the kind of dichotomy just described are often referred to as dual-process or dual-system theories (Chaiken and Trope 1999, Evans 2003, 2014a, 2014b, Evans and Stanovich 2013, Frankish 2010, Hammond 1996, Samuels 2009, Sloman 1996, #CITATION_TAG and DeCoster 2000, Thompson 2009, 2010, Wilson Lindsey and Schooler 2000. The duality referred to by ""dualprocess"" has many names, each with it""s own story: associative vs. rulebased (Sloman 1996), heuristic vs. analytic (Evans 1984(Evans , 1989, tacit thought vs. explicit thought (Evans and Over 1996), implicit cognition vs. explicit learning (Reber 1989), interactional vs. analytic (Levinson 1995), experiential vs. rational (Epstein 1994), quick and inflexive modules vs. intellection (Pollock 1991), intuitive cognition vs. analytical cognition (Hammond 1996), recognition primed choice vs. rational choice strategy (Klein 1998), implicit inference vs. explicit inference (Johnson-Laird 1983), automatic vs. controlled processing (Shiffrin and Schneider 1977), automatic activation vs. conscious processing system (Posner andSnyder 1975, 2004), rationality vs. rationality (Evans & Over 1996, intuitive vs. reflective , model-based vs. model-free (Daw et al 2005) and system 1 vs. system 2 (see Stanovich and West 2000 for the first mention of these terms as well as a useful, albeit dated, list of dualprocess terminology; see also Frankish 2010 for a list of features commonly associated with System 1 and System 2). While there are nuanced differences between certain dual-process theories, dual-process theories are those which claim that one can distinguish between at least two cognitive strategies in reasoning, learning, deciding, etc.-I emphasize that there might be more than two processes to avoid problems that result from positing ""binary oppositions"" (Newell 1973). One strategy is characterized by quick, effortless, and possibly associative seemings-referred to in this paper as intuitive-and the other of which is characterized by longer, more effortful, deliberative, perhaps calculative or even rule-based judgmentsreferred to in this paper as reflective. Although it is not entirely clear how these two strategies operate (e.g., serially vs. in parallel), how these strategies interact (e.g., competitively vs. non-competitively, mutually inhibitorily vs. complementarily, etc.), how these strategies are realized neurobiologically, or what these strategies actually are (e.g., systems, processes, styles, habits, personality traits, etc.), there are a variety of reasoning tasks ! 12 that demonstrate the dissociability of the two strategies (see Sloman 1996 for a helpful summary)."	4	"Theories and models that endorse or assume the kind of dichotomy just described are often referred to as dual-process or dual-system theories (Chaiken and Trope 1999, Evans 2003, 2014a, 2014b, Evans and Stanovich 2013, Frankish 2010, Hammond 1996, Samuels 2009, Sloman 1996, #CITATION_TAG and DeCoster 2000, Thompson 2009, 2010, Wilson Lindsey and Schooler 2000. The duality referred to by ""dualprocess"" has many names, each with it""s own story: associative vs. rulebased (Sloman 1996), heuristic vs. analytic (Evans 1984(Evans , 1989, tacit thought vs. explicit thought (Evans and Over 1996), implicit cognition vs. explicit learning (Reber 1989), interactional vs. analytic (Levinson 1995), experiential vs. rational (Epstein 1994), quick and inflexive modules vs. intellection (Pollock 1991), intuitive cognition vs. analytical cognition (Hammond 1996), recognition primed choice vs. rational choice strategy (Klein 1998), implicit inference vs. explicit inference (Johnson-Laird 1983), automatic vs. controlled processing (Shiffrin and Schneider 1977), automatic activation vs. conscious processing system (Posner andSnyder 1975, 2004), rationality vs. rationality (Evans & Over 1996, intuitive vs. reflective , model-based vs. model-free (Daw et al 2005) and system 1 vs. system 2 (see Stanovich and West 2000 for the first mention of these terms as well as a useful, albeit dated, list of dualprocess terminology; see also Frankish 2010 for a list of features commonly associated with System 1 and System 2). While there are nuanced differences between certain dual-process theories, dual-process theories are those which claim that one can distinguish between at least two cognitive strategies in reasoning, learning, deciding, etc.-I emphasize that there might be more than two processes to avoid problems that result from positing ""binary oppositions"" (Newell 1973). One strategy is characterized by quick, effortless, and possibly associative seemings-referred to in this paper as intuitive-and the other of which is characterized by longer, more effortful, deliberative, perhaps calculative or even rule-based judgmentsreferred to in this paper as reflective."	T
CC221	Multiple sclerosis (MS), a chronic, debilitating disease of the central nervous system, is the leading cause of nontraumatic disability in young adults [1]. It is estimated that more than two million people live with this disease worldwide [1], although the incidence and prevalence vary geographically [2]#CITATION_TAG[4]. Furthermore, reports of recent increases in the incidence and prevalence, and in the ratio of women to men with MS, have been inconsistent across regions [5][6][7].	0	Multiple sclerosis (MS), a chronic, debilitating disease of the central nervous system, is the leading cause of nontraumatic disability in young adults [1]. It is estimated that more than two million people live with this disease worldwide [1], although the incidence and prevalence vary geographically [2]#CITATION_TAG[4]. Furthermore, reports of recent increases in the incidence and prevalence, and in the ratio of women to men with MS, have been inconsistent across regions [5][6][7].	t
CC1824	Consistency of choice and valuation (choice spread independent of whether the choice was made before or after revaluation) was higher when stimuli were of positive valence. This choice consistency is not merely revaluation caused by the choice but also includes conforming the choice to fluctuations in valuation. The fact that this effect was stronger for positive stimuli shows a choice bias that is sensitive to stimulus valence. Additionally, the magnitude of the choice-induced preference change was not modulated by action or valence per se, but by their interaction. This effect was driven by changes across all conditions, including the control condition. Schonberg at al. [14] showed that subsequent choice is biased after as little as eight repetitions of Go responses that coincided with item presentation. In this study we show that when action is integrated into the choice both choice and valuation are affected. Note that action and inaction choice trials happen once per item and merely differ by a single keypress obviating any learning effect per se. A possible explanation for the effects that valence and action have on choice and valuation involves a Pavlovian coupling of response tendencies (e.g. action/inaction) with the valence of the outcome (e.g. choosing among overall positive or overall negative outcomes) [15,22]. This is usually seen to arise out of a consequence that in natural environments action and reward tend to be closely aligned, i.e. approach for reward. Furthermore, a recent study shows that devaluation of stimuli can be induced by stopping actions [23]. The present study shows how this intrinsic coupling of action and valence affects forced choices among similarly valued rewards and the dynamics of revaluation. In this regard the findings add to an emerging picture that action expression in itself is relevant for valuation and behavior [16,17,[24][25][26]. A similar account could also explain the increased consistency of choice and valuation for items of positive valence. The fact that dynamic of revaluation is sensitive to stimulus valence favours a Pavlovian congruence account over more non-specific biases, e.g. attentional focus due to action. However, the factors of action and valence also modulate the spread of value due to choice in the Session 2 Choice condition (see Fig. 2a). Effects in the Session 1 Choice condition could be attributed to a change in revaluation caused by active or inactive choice. Effects in the Session 2 Choice condition could be attributed to a drift in ratings between session that interacts with active and inaction choice tendencies (for example the relatively more positive value change in Session 2 Choice Go Unchosen may be due to the relatively high values of both options biasing the choice towards action). Therefore, the precise effects of action and valence on the dynamics of revaluation, i.e. the interaction of choice biasing valuation and vice versa, remain to be established. A disposition towards viewing the outcomes of active choices more favourably post choice may provide a basis for cognitive biases, e.g. the aforementioned asymmetry in regret over committed or omitted action [27]. A role for action rather than passive inaction may also underpin the omission bias in moral judgements. Here omitting an action that causes harm is seen as more favourable than committing an action with the same outcome. One possibility is this is related to an increased sense of causality and personal responsibility associated with overt actions [28]. Therefore, the effect of action on dynamics of choice-induced preference change is also consistent with accounts of preference change focusing on self-perception (see [13,#CITATION_TAG] for discussions).	2	A role for action rather than passive inaction may also underpin the omission bias in moral judgements. Here omitting an action that causes harm is seen as more favourable than committing an action with the same outcome. One possibility is this is related to an increased sense of causality and personal responsibility associated with overt actions [28]. Therefore, the effect of action on dynamics of choice-induced preference change is also consistent with accounts of preference change focusing on self-perception (see [13,#CITATION_TAG] for discussions).	f
CC2460	where D(j) and ��(j) are non-oscillating functions decaying as j __�2 and j __�9/2 respectively, with (_�� _�� _�� ) representing higher order terms. The dominant asymptotic D(j), understood to be due to the contribution of degenerate geometries, masks the desired Regge action amplitude [8,11,#CITATION_TAG].	0	where D(j) and ��(j) are non-oscillating functions decaying as j __�2 and j __�9/2 respectively, with (_�� _�� _�� ) representing higher order terms. The dominant asymptotic D(j), understood to be due to the contribution of degenerate geometries, masks the desired Regge action amplitude [8,11,#CITATION_TAG].	h
CC1552	"While many authors criticize TCA for failing to recognize that value creation rather than minimizing costs is the primary goal of business exchange (e.g. Anderson, 1995;#CITATION_TAG & John, 1999;Madhok & Tallman, 1998;Zajac & Olsen, 1993), others see TCA as a starting point for analyzing value creation between exchange partners. Kleinaltenkamp and Ehret (2006) position specific investments as a source of switching costs but also as an important source of value creation. Indeed, productivity advantages of specific investments become apparent when the original TCA assumption ""output is held constant"" (Williamson, 1985, p.85) is given up (Kim, 1999;Kleinaltenkamp & Ehret, 2006). Since specific investments are particular to the focal relationship, the associated gain referred to as ""quasirent"" can only be realized between the involved parties (Backhaus & B�_schken, 1999). By making specific investments for a customer, a supplier can help this particular customer in developing more efficient operations or in better differentiating its market offerings (Ghosh & John, 1999). Consequently, the creation of superior value increases the customer""s willingness to pay for the supplier""s offerings which results, in turn, in a higher customer lifetime value for the supplier. The same applies in converse when a customer specifically invests in a supplier."	0	"While many authors criticize TCA for failing to recognize that value creation rather than minimizing costs is the primary goal of business exchange (e.g. Anderson, 1995;#CITATION_TAG & John, 1999;Madhok & Tallman, 1998;Zajac & Olsen, 1993), others see TCA as a starting point for analyzing value creation between exchange partners. Kleinaltenkamp and Ehret (2006) position specific investments as a source of switching costs but also as an important source of value creation. Indeed, productivity advantages of specific investments become apparent when the original TCA assumption ""output is held constant"" (Williamson, 1985, p.85) is given up (Kim, 1999;Kleinaltenkamp & Ehret, 2006). Since specific investments are particular to the focal relationship, the associated gain referred to as ""quasirent"" can only be realized between the involved parties (Backhaus & B�_schken, 1999)."	W
CC2161	Concerning the background independence of quantum gravity, the dimensions of spacetime should be regarded as an effective dynamical quantity rather than a given constant. This viewpoint not only seems natural from the physical requirement of quantum gravity, but has also been supported by some recent results from simplicial quantum gravity #CITATION_TAG and field theoretical treatment . On the other hand, however, the original formulation of tensor models as well as the group field theory depend on the considering dimensions in their formalism through the rank of tensors and the choices of groups. Therefore it would be desired to find another interpretation of tensor models which singles out a tensor model that is applicable to general dimensions.	0	Concerning the background independence of quantum gravity, the dimensions of spacetime should be regarded as an effective dynamical quantity rather than a given constant. This viewpoint not only seems natural from the physical requirement of quantum gravity, but has also been supported by some recent results from simplicial quantum gravity #CITATION_TAG and field theoretical treatment . On the other hand, however, the original formulation of tensor models as well as the group field theory depend on the considering dimensions in their formalism through the rank of tensors and the choices of groups. Therefore it would be desired to find another interpretation of tensor models which singles out a tensor model that is applicable to general dimensions.	h
CC2546	"One notable side effect of focusing on the justifications for a simulation model is to engage researchers from differing disciplines in the process of capturing the model to be implemented. Following sound software engineering principles, we can express the model in structures and language that is not discipline dependent, and thenp r o v i d ea na r g u m e n tt h a tt h e implementation conforms, in a traceable, repeatable manner, to this model as part of the overall fitness-for-purpose argument #CITATION_TAG. Constructing the argument using a visual notation results in a document that can be easily interpreted by researchers across disciplines, and which can be published alongside the description of and results from the simulation. The overarching objective of this approach is to increase confidence in the use of simulation-derived predictions, potentially increasing the impact of a simulation study. While it is possible that providing a detailed rationale behind simulation development might risk limiting a researcher""s willingness to challenge the results of work supported by argumentation, we feel this technique has the potential to address an important void in biological simulation development: the provision of a method by which a researcher can fully appreciate the thought processes behind model composition. In biological contexts, the rationale is never a fully explored, uncontentious argument, because the context is not that well understood. Instead, the argument exposes the rationale and understanding to critique, initiating a healthy debate about the quality of the experimentation and the results."	2	"One notable side effect of focusing on the justifications for a simulation model is to engage researchers from differing disciplines in the process of capturing the model to be implemented. Following sound software engineering principles, we can express the model in structures and language that is not discipline dependent, and thenp r o v i d ea na r g u m e n tt h a tt h e implementation conforms, in a traceable, repeatable manner, to this model as part of the overall fitness-for-purpose argument #CITATION_TAG. Constructing the argument using a visual notation results in a document that can be easily interpreted by researchers across disciplines, and which can be published alongside the description of and results from the simulation. The overarching objective of this approach is to increase confidence in the use of simulation-derived predictions, potentially increasing the impact of a simulation study. While it is possible that providing a detailed rationale behind simulation development might risk limiting a researcher""s willingness to challenge the results of work supported by argumentation, we feel this technique has the potential to address an important void in biological simulation development: the provision of a method by which a researcher can fully appreciate the thought processes behind model composition."	o
CC2767	"The study described here is a cross-sectional study of actor perceptions in a network. Although the study analyses different time dimensions (past/present/future), the data were collected at a given point in time. However, it is possible to use the method for a longitudinal study. In the present study actors were asked to describe their network at three points in time. Considering limitations of the study, we encounter the general concerns that have been raised about doing case study research. Dubois and Gadde (2002) for instance discuss a number of limitations: first, case studies are seen to provide little basis for scientific generalization (Weick, 1979;#CITATION_TAG, 1994). Second, case studies are often rich descriptions of events without clear analytical framing; they at best only partially support quasi-deductive theories, and they suggest some notion of statistical generalization where multiple case studies are used (Easton, 1995). Dubois and Gadde (2002) introduce the abductive approach to case study research, also referred to as ""systematic combining where ""theoretical framework, empirical fieldwork and case analysis evolve simultaneously"" (p. 554). This has also been our experience in developing the analytical framework and the dottogram method. A second limitation of our study concerns the framework. It presents an account of how actors perceive changes affecting different aspect of the network. As such, this is not a representation of ""reality but represents perceived reality, i.e. a socially constructed view of the world. It bases its foundations on the concept of network pictures and sensemaking, where reality is an idiosyncratic construct. When studying changes, we actually study the outcomes of change, i.e. the product, while not explaining the process of change. Changes can only be understood in retrospect. It therefore makes sense to describe changes in terms of the actor""s perception of them, because it is these perceived changes which serve as basis for their actions (Ford et al., 2003). A third limitation concerns the dimensions of the analytical framework, in that it is concerned with several network picture characteristics, but it does not take into account other dimensions such as company performance, and as such we have developed a descriptive rather than a normative approach."	5	"However, it is possible to use the method for a longitudinal study. In the present study actors were asked to describe their network at three points in time. Considering limitations of the study, we encounter the general concerns that have been raised about doing case study research. Dubois and Gadde (2002) for instance discuss a number of limitations: first, case studies are seen to provide little basis for scientific generalization (Weick, 1979;#CITATION_TAG, 1994). Second, case studies are often rich descriptions of events without clear analytical framing; they at best only partially support quasi-deductive theories, and they suggest some notion of statistical generalization where multiple case studies are used (Easton, 1995). Dubois and Gadde (2002) introduce the abductive approach to case study research, also referred to as ""systematic combining where ""theoretical framework, empirical fieldwork and case analysis evolve simultaneously"" (p. 554)."	s
CC2964	A critical component of mind wandering is memory, which provides the basic elements from which our mind wanderings are constructed. Memory itself can be divided into declarative memory, which can be made explicit or conscious, and non-declarative memory, which comprises the non-conscious products of learning, such as habits or learned skills like driving or playing the piano. Declarative memory, in turn, can be divided into episodic memory, which is personal memory for past episodes, and semantic memory, which is basic knowledge about the world (Squire, 2004). According to #CITATION_TAG (1972), episodic memory is unique to humans.	2	A critical component of mind wandering is memory, which provides the basic elements from which our mind wanderings are constructed. Memory itself can be divided into declarative memory, which can be made explicit or conscious, and non-declarative memory, which comprises the non-conscious products of learning, such as habits or learned skills like driving or playing the piano. Declarative memory, in turn, can be divided into episodic memory, which is personal memory for past episodes, and semantic memory, which is basic knowledge about the world (Squire, 2004). According to #CITATION_TAG (1972), episodic memory is unique to humans.	o
CC1154	"This raises the question: to what extent are people positively motivated to spend time mind-wandering (i.e., under the sway of the default-mode network)? Clearly, people are positively motivated to pursue goals (by definition!), which must limit the amount of time they spend mind-wandering. There is extensive reason to believe that a sense that one""s life is meaningful depends on having attractive, attainable goals and that this is necessary for sustained mental health (Klinger, 1977(Klinger, , 2012. The absence of such goals leads to boredom, depressed moods, excessive entertainment, and substance use. People strive for a balance between active goal pursuit and inner life such as ordinary mind-wandering and imaginative daydreaming, which are themselves goal-related. There are large individual differences in the amount of time that people desire to spend in imaginative daydreaming (#CITATION_TAG and Bonanno, 1990;Singer, 1966Singer, , 1975Bigelsen and Schupak, 2011), some valuing it as a resource for self-amusement and stimulation and others eager to reduce or eliminate it but unable to attain a daydream-free state."	0	"There is extensive reason to believe that a sense that one""s life is meaningful depends on having attractive, attainable goals and that this is necessary for sustained mental health (Klinger, 1977(Klinger, , 2012. The absence of such goals leads to boredom, depressed moods, excessive entertainment, and substance use. People strive for a balance between active goal pursuit and inner life such as ordinary mind-wandering and imaginative daydreaming, which are themselves goal-related. There are large individual differences in the amount of time that people desire to spend in imaginative daydreaming (#CITATION_TAG and Bonanno, 1990;Singer, 1966Singer, , 1975Bigelsen and Schupak, 2011), some valuing it as a resource for self-amusement and stimulation and others eager to reduce or eliminate it but unable to attain a daydream-free state."	a
CC814	The present estimates of number of days needed to achieve a reliability of ICC = 0.80 for overall PA and MVPA, fall in between of previous estimates. While several studies have found that 2-6 days are required [#CITATION_TAG,7,8,6,30], other studies have shown that 12 [4] and 16-23 days are needed [3]. Jerome et al [3] used a wear time criteria of !6 h, which might have led to the need for many days of measurement, in line with the trend across the wear time criteria applied in the present study. Levin et al [4] applied a protocol with measurements during an entire year, which may have introduced greater variation (e.g., seasonal effects) than across subsequent days and weeks. Nevertheless, as ICC is based on variance partitioning and depends on the relative difference in variances (inter-vs. intra-individual sources of variance), the coefficient is context-specific. Thus, it change with the range of observations (similar to the correlation coefficient), despite residual variance and absolute measures of reliability being unaffected [26][27][28]. The current sample was a convenience sample exhibiting a higher activity level (but with approximately similar heterogeneity as evaluated by SDs for the different outcome variables) as compared to population estimates for the corresponding age group [34][35][36]. Thus, it could be argued that our estimates of ICC might be fairly generalizable to the general adult population. Yet, previous studies have concluded that it does not seem to be any relationship between activity level and reliability [6,4]. Based on the heteroscedastic patterns of variability found for overall PA level and MVPA (Fig 1), the current study indicates otherwise. Thus, the sample characteristics (exhibiting a high PA level) might be a likely explanation for the lower reliability found for PA compared to other studies. This means that researchers should determine reliability in their specific samples to make appropriate decisions on valid wear criteria in a given study. Yet, it could be hypothesized from the current study that SED are less influenced by sample characteristics than PA, as a homoscedastic pattern was evident for SED as opposed to the heteroscedastic pattern found for PA.	1	The present estimates of number of days needed to achieve a reliability of ICC = 0.80 for overall PA and MVPA, fall in between of previous estimates. While several studies have found that 2-6 days are required [#CITATION_TAG,7,8,6,30], other studies have shown that 12 [4] and 16-23 days are needed [3]. Jerome et al [3] used a wear time criteria of ! 6 h, which might have led to the need for many days of measurement, in line with the trend across the wear time criteria applied in the present study. Levin et al [4] applied a protocol with measurements during an entire year, which may have introduced greater variation (e.g., seasonal effects) than across subsequent days and weeks.	h
CC1947	Lanthanum aluminate (LaAlO 3 -LAO) is a prototype compound for soft-mode driven antiferrodistorsive phase transitions. At ambient conditions, LAO crystallizes in the R3c space group, as a result of the condensation of a soft mode at the R point of the Brillouin zone boundary. At ambient conditions, the rhombohedral cell has the lattice parameters a R = 5.357�� and �� R = 60.12 _�� , or in the hexagonal setting a H = 5.366�� and c H = 13.109�� [7]. In this structure, the oxygen octahedra are rotated along the [111] C direction of the parent cubic cell. The rhombohedral distortion can be described by this single tilt angle. Following early structural studies [8,9], the temperature-induced phase transition at 813 K has been studied in great details (e.g. [10] and references within). On the theoretical side, the parameters of a Landau potential were fitted to available experimental data [11]. The pressure-induced rhombohedral to cubic transition of LAO was revealed by a powder Raman spectroscopy and synchrotron diffraction study [12]. It was at that time the first exception to the so-far general rule stating that tilt angles in antiferrodistorsive perovskites should increase under pressure [13]. This example has motivated theoretical work to explain this behaviour and formulate new rules and models to predict qualitatively and quantitavely the evolution of distortions (tilt angles) under pressure [14,15,16,#CITATION_TAG]. The evolution of the tilt angles in LAO itself was investigated by single crystal diffraction under hydrostatic stress up to 8 GPa [7] and non-hydrostatic stress [18].	4	On the theoretical side, the parameters of a Landau potential were fitted to available experimental data [11]. The pressure-induced rhombohedral to cubic transition of LAO was revealed by a powder Raman spectroscopy and synchrotron diffraction study [12]. It was at that time the first exception to the so-far general rule stating that tilt angles in antiferrodistorsive perovskites should increase under pressure [13]. This example has motivated theoretical work to explain this behaviour and formulate new rules and models to predict qualitatively and quantitavely the evolution of distortions (tilt angles) under pressure [14,15,16,#CITATION_TAG]. The evolution of the tilt angles in LAO itself was investigated by single crystal diffraction under hydrostatic stress up to 8 GPa [7] and non-hydrostatic stress [18].	p
CC1590	"The literature on industrial marketing seems to agree that companies maintain relationships either because they have to or because they want to. The commitment concept helps understand this duality in bonding rationales. Commitment occupies a central role in the study of successful relationships between firms (Gilliland & Bello, 2002;Gundlach et al., 1995;#CITATION_TAG et al., 2006;Morgan & Hunt, 1994). While early research models commitment as a onedimensional construct (e.g. Andaleeb, 1996;Anderson & Weitz, 1992;Ganesan, 1994), later studies often highlight its duality. Bendapudi and Berry (1997) approach this dichotomy as ""dedication-based"" versus ""constraint-based"" relationship maintenance. In other studies, ""affective commitment"" is the extent to which partners like to stay in existing relationships while ""calculative commitment"" is the degree to which they need to stay (de Ruyter et al., 2001;Geyskens et al., 1996). Brown et al. (1995) used ""normative"" and ""instrumental"" commitment for very similar notions of bonding between firms. S�_llner (1999) bases his arguments on transaction cost analysis (TCA) and becomes somewhat more specific about the bonding dimensions in buyer-seller relationships: on the one hand, instrumental and attitudinal inputs by one partner are specific economic or social-psychological investments 1 that would be lost outside the focal relationship and hence act as switching costs. In order not to incur those switching costs the partner stays in the relationship. On the other hand, relationship outputs (performance, justice) represent another bonding dimension which makes business partners want to stay in a relationship. More recently, this voluntary dimension is investigated as relationship value (Corsaro & Snehota, 2010;Ulaga, 2003;."	0	"The literature on industrial marketing seems to agree that companies maintain relationships either because they have to or because they want to. The commitment concept helps understand this duality in bonding rationales. Commitment occupies a central role in the study of successful relationships between firms (Gilliland & Bello, 2002;Gundlach et al., 1995;#CITATION_TAG et al., 2006;Morgan & Hunt, 1994). While early research models commitment as a onedimensional construct (e.g. Andaleeb, 1996;Anderson & Weitz, 1992;Ganesan, 1994), later studies often highlight its duality. Bendapudi and Berry (1997) approach this dichotomy as ""dedication-based"" versus ""constraint-based"" relationship maintenance. In other studies, ""affective commitment"" is the extent to which partners like to stay in existing relationships while ""calculative commitment"" is the degree to which they need to stay (de Ruyter et al., 2001;Geyskens et al., 1996)."	m
CC2699	#CITATION_TAG and Salhi (2007) deal with the Euclidean CMSWP by proposing a perturbation-based heuristic	0	#CITATION_TAG and Salhi (2007) deal with the Euclidean CMSWP by proposing a perturbation-based heuristic	#
CC2673	The first view, which largely derives from models based on the median voter theorem (Downs, 1957), emphasizes that in democracies the distribution of political power is typically more equal than the distribution of income and wealth. As a consequence, voting models predict that democracies tend to redistribute from the rich to the poor, and this effect will be stronger with higher income inequality as the middle-class has more incentives to form coalitions with the poor (see Alesina and Rodrik, 1994;Persson and Tabellini, 1994). Based on this logic, #CITATION_TAG and Robinson (2000;2006) predict redistribution from the elite to the citizens after an extension of voting rights.	0	The first view, which largely derives from models based on the median voter theorem (Downs, 1957), emphasizes that in democracies the distribution of political power is typically more equal than the distribution of income and wealth. As a consequence, voting models predict that democracies tend to redistribute from the rich to the poor, and this effect will be stronger with higher income inequality as the middle-class has more incentives to form coalitions with the poor (see Alesina and Rodrik, 1994;Persson and Tabellini, 1994). Based on this logic, #CITATION_TAG and Robinson (2000;2006) predict redistribution from the elite to the citizens after an extension of voting rights.	s
CC2365	"4. Dry spell during wet season: sum(P (d : d + 6)) < 10 mm: Criterion 4 marks dry periods (1 week with precipitation < 10 mm). The limit of 10 mm of weekly precipitation to define dry spells is mostly arbitrary as there is no universal amount of weekly precipitation that is required to keep different soil types with different slopes and aspects wet for optimal plant growth, and different crops have different water demands. However, we consider this amount to be a rough indication of when soils get drier and plants might suffer from water scarcity, especially when several dry spells follow each other. We developed the criterion to meet the agricultural view of the peasants"" report analyses. Thresholds are, as a consequence, different from those one would obtain when following climatological/statistical criteria such as in #CITATION_TAG et al. (2001), Nieto-Ferreira andRickenbach (2011) or Sulca et al. (2016). It is also worth mentioning explicitly that each wet spell stands for 1 week of relatively dry conditions (whereas the date used in Fig. 3 is defined as the 3rd day of the respective week) and dry spells are allowed to overlap. Each consecutive dry spell enlarges the affected period by 1 day. The overall duration of a ""dry spell period"" (a series of dry spells) can easily be estimated from Fig. 3 with respect to the time axis."	5	"The limit of 10 mm of weekly precipitation to define dry spells is mostly arbitrary as there is no universal amount of weekly precipitation that is required to keep different soil types with different slopes and aspects wet for optimal plant growth, and different crops have different water demands. However, we consider this amount to be a rough indication of when soils get drier and plants might suffer from water scarcity, especially when several dry spells follow each other. We developed the criterion to meet the agricultural view of the peasants"" report analyses. Thresholds are, as a consequence, different from those one would obtain when following climatological/statistical criteria such as in #CITATION_TAG et al. (2001), Nieto-Ferreira andRickenbach (2011) or Sulca et al. (2016). It is also worth mentioning explicitly that each wet spell stands for 1 week of relatively dry conditions (whereas the date used in Fig. 3 is defined as the 3rd day of the respective week) and dry spells are allowed to overlap. Each consecutive dry spell enlarges the affected period by 1 day. The overall duration of a ""dry spell period"" (a series of dry spells) can easily be estimated from Fig. 3 with respect to the time axis."	h
CC5	1) It represents journals, books, and conferences according to the metadata of their chapters/articles and uses the Smart Topic API #CITATION_TAG  to characterize each of them with a semantically enhanced topic vector.	5	1) It represents journals, books, and conferences according to the metadata of their chapters/articles and uses the Smart Topic API #CITATION_TAG  to characterize each of them with a semantically enhanced topic vector.	1
CC726	Clinical efficacy. Efficacy data were derived from a systematic literature review and NMA of randomised controlled trials (RCTs) of interventions for adults with social anxiety disorder [14]; the NMA, based on a random effects model [22], was conducted within a Bayesian framework using Markov Chain Monte Carlo simulation techniques implemented in WinBUGS 1.4 [23,#CITATION_TAG]. The NMA included 101 trials reporting continuous data on symptom scales of social anxiety, of which 24 also reported dichotomous recovery data (with recovery being defined as no longer meeting criteria for diagnosis). The studies reported several continuous measures of social anxiety, none of which were common to all trials, so treatment effects for each trial were calculated as a standardised mean difference (SMD). Based on published psychometric properties and data from clinically referred participants who completed several measures, continuous measures were assumed to be equally responsive and had a mean correlation of 0.65 (for details see [14]). The outcome measures used in the clinical analysis are provided in S1 File.	5	Clinical efficacy. Efficacy data were derived from a systematic literature review and NMA of randomised controlled trials (RCTs) of interventions for adults with social anxiety disorder [14]; the NMA, based on a random effects model [22], was conducted within a Bayesian framework using Markov Chain Monte Carlo simulation techniques implemented in WinBUGS 1.4 [23,#CITATION_TAG]. The NMA included 101 trials reporting continuous data on symptom scales of social anxiety, of which 24 also reported dichotomous recovery data (with recovery being defined as no longer meeting criteria for diagnosis). The studies reported several continuous measures of social anxiety, none of which were common to all trials, so treatment effects for each trial were calculated as a standardised mean difference (SMD). Based on published psychometric properties and data from clinically referred participants who completed several measures, continuous measures were assumed to be equally responsive and had a mean correlation of 0.65 (for details see [14]).	f
CC2456	Having been defined, the new models must be tested to see whether their semiclassical behavior is an improvement over the BC model. The standard tests involve examining the asymptotics of their vertex amplitudes and checking the semiclassical behavior of observables. So far, two test problems involving observables have been proposed: semiclassical wave packet propagation [29], and evaluation of the graviton 2-point function [#CITATION_TAG,17,34]. Both problems require the computation of large sums, where the spin foam vertex amplitude is contracted with a suitably defined boundary state. These computations, while important for extracting the physical content of the new spin foam models, have so far not been tractable. This paper describes efficient numerical algorithms, based on the existing Christensen-Egan (CE) algorithm for the BC model, to evaluate the new spin foam vertex amplitudes, both with fixed boundary spins and contracted with a boundary state. The accessible boundary states are restricted to the large (though with important limitations) class of so-called factored boundary states. Calculation of the new vertex amplitude asymptotics shows that they are dominated by the degenerate spin foam sector, as is the BC model. Also, the numerical simulation of semiclassical wave packet propagation shows that, for factored boundary states, the shape of the propagated wave packet does not agree with the desired semiclassical result. The state of the good semiclassical behavior conjecture [29] for states that do not neglect spin-spin correlations (unlike factored states) remains open.	0	Having been defined, the new models must be tested to see whether their semiclassical behavior is an improvement over the BC model. The standard tests involve examining the asymptotics of their vertex amplitudes and checking the semiclassical behavior of observables. So far, two test problems involving observables have been proposed: semiclassical wave packet propagation [29], and evaluation of the graviton 2-point function [#CITATION_TAG,17,34]. Both problems require the computation of large sums, where the spin foam vertex amplitude is contracted with a suitably defined boundary state. These computations, while important for extracting the physical content of the new spin foam models, have so far not been tractable. This paper describes efficient numerical algorithms, based on the existing Christensen-Egan (CE) algorithm for the BC model, to evaluate the new spin foam vertex amplitudes, both with fixed boundary spins and contracted with a boundary state.	 
CC1948	"In the past, much progress in the understanding of properties of ferroic ABO 3 perovskites, such as ferroelasticity or ferroelectricity, and their related phase transitions has been achieved through temperature-, or chemical composition-dependent investigations. The use of pressure has been much rarer and was mostly limited to pressures below 10 GPa due to experimental difficulties which have now been overcome for a number of years by the use of diamond-anvil cells. One of the attracting properties of the external parameter high-pressure is its character of a ""cleaner"" variable, since it acts only on interatomic distances #CITATION_TAG. Furthermore, external pressure leads to otherwise unachievable reductions of volume and chemical bond lengths. Despite the now accessible investigation of phase transitions into the very high pressure regime, experimental investigations of prototype ferroic perovskites above 50 GPa remain still scarce, with some notable exceptions among ferroelectrics (e.g. KNbO [2], PbTiO 3 [3]), relaxor ferroelectrics (PbZn 1/3 Nb 2/3 O 3 [4]), ferroelastics (SrTiO 3 [5]). Explorations of pressure-temperature or pressure-substitution phase diagrams remain even rarer [6]."	0	"In the past, much progress in the understanding of properties of ferroic ABO 3 perovskites, such as ferroelasticity or ferroelectricity, and their related phase transitions has been achieved through temperature-, or chemical composition-dependent investigations. The use of pressure has been much rarer and was mostly limited to pressures below 10 GPa due to experimental difficulties which have now been overcome for a number of years by the use of diamond-anvil cells. One of the attracting properties of the external parameter high-pressure is its character of a ""cleaner"" variable, since it acts only on interatomic distances #CITATION_TAG. Furthermore, external pressure leads to otherwise unachievable reductions of volume and chemical bond lengths. Despite the now accessible investigation of phase transitions into the very high pressure regime, experimental investigations of prototype ferroic perovskites above 50 GPa remain still scarce, with some notable exceptions among ferroelectrics (e.g. KNbO [2], PbTiO 3 [3]), relaxor ferroelectrics (PbZn 1/3 Nb 2/3 O 3 [4]), ferroelastics (SrTiO 3 [5]). Explorations of pressure-temperature or pressure-substitution phase diagrams remain even rarer [6]."	e
CC2681	The relationship between democracy and growth has received much attention in the recent literature. Cross-country studies on the impact of democratic institutions on growth yield ambiguous and inconclusive results (#CITATION_TAG, 1997;Glaeser et al. 2004). Even studies exploiting the within country variation in the data still show that transitions towards democracy are not necessarily associated with large improvements in economic outcomes (Rodrik and Wacziarg, 2005;Giavazzi and Tabellini, 2005;Persson andTabellini, 2006, 2008). Furthermore, the direction of causation is hard to establish (see Acemoglu et al., 2008;Gundlach and Paldam, 2009). Crucial questions in this debate of course are about the mechanism of how political institutions affect economic growth. In this respect, government policies should play a key role. Political institutions affect (economic) policy making by shaping the rules of the game and determine the context in which key policy decisions are made, such as redistribution of income and the provision of public goods (Persson and Tabellini, 2003).	0	The relationship between democracy and growth has received much attention in the recent literature. Cross-country studies on the impact of democratic institutions on growth yield ambiguous and inconclusive results (#CITATION_TAG, 1997;Glaeser et al. 2004). Even studies exploiting the within country variation in the data still show that transitions towards democracy are not necessarily associated with large improvements in economic outcomes (Rodrik and Wacziarg, 2005;Giavazzi and Tabellini, 2005;Persson andTabellini, 2006, 2008). Furthermore, the direction of causation is hard to establish (see Acemoglu et al., 2008;Gundlach and Paldam, 2009). Crucial questions in this debate of course are about the mechanism of how political institutions affect economic growth.	r
CC7	We will now discuss some of these ontology-based approaches. Sieg et al. [16] present an ontology-based recommender to improve personalised Web searching in which the user profiles are instances of a reference domain ontology and are incrementally updated based on the user interaction with the system. Middleton et al. [18] describe a hybrid recommender system that exploit ontologies for increasing the accuracy of the profiling process and hence the usefulness of the recommendations. Thiagarajan et al. [19] use a different strategy by representing user profiles as bags-ofwords and weighing each term according to the user interests derived from a domain ontology. Razmerita et al. [20] describe OntobUM, an ontology-based recommender that integrates three ontologies: i) the user ontology, which structures the characteristics of users and their relationships, ii) the domain ontology, which defines the domain concepts and their relationships, and iii) the log ontology, which defines the semantics of the user interactions with the system. Birukou et al [21] present an agent-based system that learns the preferences of experienced researchers and provides specific suggestions to support search for scientific publications. Colombo-Mendoza et al #CITATION_TAG propose RecomMetz, a context-aware mobile recommender system based on Semantic Web technologies. This system introduced some unique features, such as the composite structure of the items and the integration of temporal and crowd factors into a contextaware model. Finally, Cantador et al. [23] propose a hybrid recommendation model in which user preferences are described in terms of semantic concepts defined in domain ontologies. Similar to all these systems SBR builds a semantic representation of the items and exploits the ontology for inferring additional concepts. However, rather than creating a representation of a single user, it characterizes the overall interests of the research community associated with the proceedings of a conference series.	1	Thiagarajan et al. [19] use a different strategy by representing user profiles as bags-ofwords and weighing each term according to the user interests derived from a domain ontology. Razmerita et al. [20] describe OntobUM, an ontology-based recommender that integrates three ontologies: i) the user ontology, which structures the characteristics of users and their relationships, ii) the domain ontology, which defines the domain concepts and their relationships, and iii) the log ontology, which defines the semantics of the user interactions with the system. Birukou et al [21] present an agent-based system that learns the preferences of experienced researchers and provides specific suggestions to support search for scientific publications. Colombo-Mendoza et al #CITATION_TAG propose RecomMetz, a context-aware mobile recommender system based on Semantic Web technologies. This system introduced some unique features, such as the composite structure of the items and the integration of temporal and crowd factors into a contextaware model. Finally, Cantador et al. [23] propose a hybrid recommendation model in which user preferences are described in terms of semantic concepts defined in domain ontologies. Similar to all these systems SBR builds a semantic representation of the items and exploits the ontology for inferring additional concepts.	o
CC1105	An early investigation (#CITATION_TAG et al., 1966) that obtained thought reports after brief signal-detection trials established a number of other conditions. First, the rate at which participants had to make judgments and the difficulty of the task (detect a tone of a particular frequency versus detect a change in frequency from the previous tone) both significantly affected reports of taskirrelevant thoughts. That is, the more demanding the task, the less minds wandered.	0	An early investigation (#CITATION_TAG et al., 1966) that obtained thought reports after brief signal-detection trials established a number of other conditions. First, the rate at which participants had to make judgments and the difficulty of the task (detect a tone of a particular frequency versus detect a change in frequency from the previous tone) both significantly affected reports of taskirrelevant thoughts. That is, the more demanding the task, the less minds wandered.	A
CC1293	"Statistical models have dominated data analysis in the social sciences, including educational psychology (Dekker et al., 2009;Freedman, 1987;Herzog, 2006). For example, the studies cited in section 2 primarily used correlation (78% of the studies) and regression (54% of the studies), with some papers citing path analysis results (14%) and structural equation models (11%). Statistical modelling has a sound theoretical basis, allowing verifiable conclusions to be drawn from model coefficients; therefore, statistical models have made, and will continue to make, a valuable contribution to the understanding of learners and the learning process. However, such models are based on assumptions, including assumptions of normality, independency, linear additively, and constant variance (Nisbet et al., 2009). It is evident from current knowledge of the factors influencing academic performance, that such factors are interdependent (Prinsloo et al., 2012). While each factor measures unique attributes, overlaps occur in the constructs being measured. In addition, there is evidence to suggest variance is not constant for all attributes. For example, De Feyter et al. (2012) found that low levels of self-efficacy had a positive, direct effect on academic performance for neurotic students only, and for stable students, average or higher levels of self-efficacy had a direct effect on academic performance. In addition, #CITATION_TAG and Kendall (2006) found evidence that high levels of self-efficacy can lead to overconfidence regarding exam preparedness, which in turn can have a negative impact on academic performance. Similarly, Poropat (2009) cites evidence of non-linear relationships between factors of personality and academic performance, including conscientiousness and openness. Duff et al. (2004) observed that because academic performance is itself a complex measure, calculated as an aggregate of a variety of assessment types, this weakens the result of correlation analysis with other learning dimensions. While recognizing the continuing importance of statistical models, Freedman (1987) and Breiman (2001) argued that alternative-modelling approaches should be considered when dimensionality is high, and relationships are complex such as in the social sciences. Cox, in a response to Breiman""s paper, notes the importance of the probabilistic base of standard statistical modelling, but agrees with Breiman that in some circumstances, an empirical approach is better (Breiman, 2001, p. 18). It is therefore pertinent to ask if data mining""s empirical modelling approach can add value to psychometric data analysis, in particular their relevance to models of academic achievement."	4	While each factor measures unique attributes, overlaps occur in the constructs being measured. In addition, there is evidence to suggest variance is not constant for all attributes. For example, De Feyter et al. (2012) found that low levels of self-efficacy had a positive, direct effect on academic performance for neurotic students only, and for stable students, average or higher levels of self-efficacy had a direct effect on academic performance. In addition, #CITATION_TAG and Kendall (2006) found evidence that high levels of self-efficacy can lead to overconfidence regarding exam preparedness, which in turn can have a negative impact on academic performance. Similarly, Poropat (2009) cites evidence of non-linear relationships between factors of personality and academic performance, including conscientiousness and openness. Duff et al. (2004) observed that because academic performance is itself a complex measure, calculated as an aggregate of a variety of assessment types, this weakens the result of correlation analysis with other learning dimensions. While recognizing the continuing importance of statistical models, Freedman (1987) and Breiman (2001) argued that alternative-modelling approaches should be considered when dimensionality is high, and relationships are complex such as in the social sciences.	i
CC2710	In the form in which we use them here they go back (at least) to #CITATION_TAG et al	5	In the form in which we use them here they go back (at least) to #CITATION_TAG et al	I
CC1630	Because virtually all jobs use subjectivity, it must have some benefits. A smaller literature considers these (e.g., #CITATION_TAG & Oyer 2003;Gibbs et al. 2004). Subjective evaluations are an alternative means by which a firm can address limitations of numeric measures. Subjective evaluation allows the supervisor to incorporate her own observations about employee performance that are not reflected in a numeric measure, and allows her to do so dynamically and to incorporate ex-post settling up.	4	Because virtually all jobs use subjectivity, it must have some benefits. A smaller literature considers these (e.g., #CITATION_TAG & Oyer 2003;Gibbs et al. 2004). Subjective evaluations are an alternative means by which a firm can address limitations of numeric measures. Subjective evaluation allows the supervisor to incorporate her own observations about employee performance that are not reflected in a numeric measure, and allows her to do so dynamically and to incorporate ex-post settling up.	 
CC2849	"The incorporation of a topography discretization technique with Godunov-type SWE numerical solvers has been an issue of extreme relevance to their practical development. In respect of this, various discretization approaches have appeared, over the last two decades, which are featured with the ability to maintain a correct discrete balance between topography gradient and the spatial flux. Such a numerical model has been referred to be ""well-balanced"" or to satisfy the ""C-property"" (Berm�_dez and VՍzquez-Cend�_n 1994, Greenberg and LeRoux 1996, LeVeque 1998. Among the popular topography discretization approaches, the sophisticated Upwind approach is the eldest. It was derived by Berm�_dez and VՍzquez-Cend�_n (1994) for a first-order accurate formulation and was then adopted to integrate the topography within higher-order FV-based Godunov-type models (e.g. Garc�_a-Navarro and VՍzquez-Cend�_n 1999, Vulovic and Sopta 2002, Crnjaric-Zic et al. 2004). However, the Upwind approach is renowned for its complexity in implementation and its limitation to a FV approximation (Zhou et al. 2001, Kesserwani et al. 2010. The Surface Gradient Method (SGM) relies on numerical reconstruction to the free-surface elevation and discretises the topography source term using a cell-centred pointwise approach. It was first reported in Nujic (1995) for a first-order Godunov-type model, and formally formulated in Zhou et al. (2001) for a second-order MUSCL scheme. Alternatively, Rogers et al. (2003) approached the SGM mathematically providing a new well-balanced set for the SWE (referred hereafter to Etta-SWE) that incorporates the free-surface elevation as a main flow variable, and with which the topography source term discretizes in pointwise manner. The Etta-SWE approach was further improved and verified for a MUSCL second-order scheme in the work of Liang and Borthwick (2009). Another popular approach for constructing well-balanced shallow water numerical model is the one established by #CITATION_TAG et al. (2004), which is known as the hydrostatic reconstruction approach (referred hereafter to Hydr-Rec). The Hydr-Rec method reconstructs the free-surface elevation -likewise to the SGM -but acts differently to balance the flux and topography gradients so that to further maintain the positivity of the water depth, and as such provides the further ability to cope with wetting and drying. The Hydr-Rec approach has been successfully applied-as a topography discretization technique -to various high-order Godunov-type models such as WENO-FV and DG methods (Xing andShu 2006, Noelle et al. 2007), and further enhanced -as a wetting and drying condition -with both FV and DG second-order Godunov-type models (Liang 2010, Kesserwani andLiang 2012). As to the DG method, Xing and Shu (2006) demonstrated theoretically that it is by far the simplest approach to obtain the well-balanced shallow water numerical model. In light of this, Kesserwani et al. (2010) devised a second-order Runge-Kutta DG method (RKDG2) that is effortlessly well-balanced in which the local discrete topography is taken as piecewise-linear but globally continuous. This topography setting is particular to the RKDG2 framework which is experienced to be more costly than FV-based Godunov-type model. Unquestionably, this review is far from being complete and has only focused on the topography discretization approaches that will serve the purpose of this investigation. Nonetheless, for a more detailed review including the latest trends on the topic of source terms discretization within Godunovtype models, the reader is referred, for instance, to the work of George (2008), LeFloch and Tanh (2011), and Garc�_a-Navarro (2010, 2012)."	4	It was first reported in Nujic (1995) for a first-order Godunov-type model, and formally formulated in Zhou et al. (2001) for a second-order MUSCL scheme. Alternatively, Rogers et al. (2003) approached the SGM mathematically providing a new well-balanced set for the SWE (referred hereafter to Etta-SWE) that incorporates the free-surface elevation as a main flow variable, and with which the topography source term discretizes in pointwise manner. The Etta-SWE approach was further improved and verified for a MUSCL second-order scheme in the work of Liang and Borthwick (2009). Another popular approach for constructing well-balanced shallow water numerical model is the one established by #CITATION_TAG et al. (2004), which is known as the hydrostatic reconstruction approach (referred hereafter to Hydr-Rec). The Hydr-Rec method reconstructs the free-surface elevation -likewise to the SGM -but acts differently to balance the flux and topography gradients so that to further maintain the positivity of the water depth, and as such provides the further ability to cope with wetting and drying. The Hydr-Rec approach has been successfully applied-as a topography discretization technique -to various high-order Godunov-type models such as WENO-FV and DG methods (Xing andShu 2006, Noelle et al. 2007), and further enhanced -as a wetting and drying condition -with both FV and DG second-order Godunov-type models (Liang 2010, Kesserwani andLiang 2012). As to the DG method, Xing and Shu (2006) demonstrated theoretically that it is by far the simplest approach to obtain the well-balanced shallow water numerical model.	p
CC526	There is a wide body of inter-disciplinary literature which links corruption to various social and economic ills. For example, Escaleras et al. (2007) show that it raises fatalities from earthquakes, and  #CITATION_TAG et al. (2006) from traffic accidents. M��on and Sekkat (2005), Pellegrini and Gerlagh (2004), Mo (2001), and Mauro (1995) find that it leads to lower investment. Countries with high levels of public sector corruption are found to receive less foreign aid, by Alesina and Weder (2002), and less foreign direct investment, by Habib and Zurawicki (2002). Tanzi and Davoodi (1997), and Mauro (1997) argue that corruption shifts public expenditures from growth-promoting to lowproductivity projects. Murphy et al. (1991Murphy et al. ( , 1993 argue that it drives potential entrepreneurs to rent-seeking activities, or even to becoming corrupt officials themselves. In short, it can be said that corruption leads to lower economic growth through diminished and misallocated resources. 1 Thus it is undesirable not only on ethical grounds.	4	There is a wide body of inter-disciplinary literature which links corruption to various social and economic ills. For example, Escaleras et al. (2007) show that it raises fatalities from earthquakes, and  #CITATION_TAG et al. (2006) from traffic accidents. M��on and Sekkat (2005), Pellegrini and Gerlagh (2004), Mo (2001), and Mauro (1995) find that it leads to lower investment. Countries with high levels of public sector corruption are found to receive less foreign aid, by Alesina and Weder (2002), and less foreign direct investment, by Habib and Zurawicki (2002). Tanzi and Davoodi (1997), and Mauro (1997) argue that corruption shifts public expenditures from growth-promoting to lowproductivity projects.	o
CC1927	The single mechanism account maintains that one mechanism governs production of both regular and irregular forms (Bird et al., 2003;Braber et al., 2005;Bybee, 1995;Joanisse and Seidenberg, 1999;Juola and Plunkett, 2000;Lambon Ralph et al., 2005;Marchman, 1993;Patterson et al., 2001;Rumelhart and McClelland, 1986;Stockall, & Marantz, 2006;#CITATION_TAG, 2000). While there are a number of different views within this family of models, our focus will be on Joanisse and Seidenberg (1999) who proposed a connectionist account based on computerized simulations of regular and irregular processing. This view is based on the Parallel Distributed Processing (PDP) model (Rumelhart and McClelland, 1986). The connectionist perspective maintains that both forms are computed by a single mechanism, which is not rule-based as this is not necessary for formulation of regular forms. Instead, the processing of regular and irregular forms is governed by two types of lexical information i.e. semantic and phonological. According to Joanisse and Seidenberg (1999), an impairment in production of regular forms results from a phonological deficit but a deficit in irregular forms results from a semantic deficit. They assume that semantic knowledge has an impact on irregular word forms. They stipulate that the semantic system retrieves specific idiosyncratic forms of verbs. For example generating the past E D tation of this T phonologically similar word form -P ology has an impact on regular and novel word forms. It applies analogies to word forms which undergo similar phonological processes (such as adding the English past tense verb suffix -ed). For example, a novel word form such as no semantic reference) but its past tense can be generated using a phonological analogy. A deficit in phonology affects such processes.	0	The single mechanism account maintains that one mechanism governs production of both regular and irregular forms (Bird et al., 2003;Braber et al., 2005;Bybee, 1995;Joanisse and Seidenberg, 1999;Juola and Plunkett, 2000;Lambon Ralph et al., 2005;Marchman, 1993;Patterson et al., 2001;Rumelhart and McClelland, 1986;Stockall, & Marantz, 2006;#CITATION_TAG, 2000). While there are a number of different views within this family of models, our focus will be on Joanisse and Seidenberg (1999) who proposed a connectionist account based on computerized simulations of regular and irregular processing. This view is based on the Parallel Distributed Processing (PDP) model (Rumelhart and McClelland, 1986). The connectionist perspective maintains that both forms are computed by a single mechanism, which is not rule-based as this is not necessary for formulation of regular forms.	T
CC912	A great deal of variation in nitrogen fixation of cyanobacterial bryophytes has been attributed to abiotic factors; #CITATION_TAG et al. (2002) reported that in the High Arctic there is a strong dependency of fixation rates on site hydrology, temperature and light intensity. In regions with low precipitation during the growing season, for instance, N fixation is limited either to the period of snowmelt or to sites which stay wet during summer, e.g. peat bogs, fens, or dense moss mats (Zielke et al. 2005). Experimental longterm increase in UV-B radiation, simulating atmospheric ozone depletion, reduces N fixation activity in Arctic mosses, but does not affect it in sub-Arctic ones (Solheim et al. , 2006. Fixation rates also depend on the successional age of boreal forests via differences in environmental moisture, nutrition and possible other factors (DeLuca et al. 2008;Lagerstr�_m et al. 2009;Zackrisson et al. 2004). In contrast to this emphasis on abiotic control over Nfixation, only a few studies have investigated the role of host specificity in the interaction between cyanobacteria and bryophytes (see During and Van Tooren 1990). For mosses the types of association probably range from simply fortuitous, through more or less specific epiphytism, to intracellular colonisation of dead moss cells or hollow hyaline cells (Solheim and Zielke 2002). Even though extensive information is available on cyanobacteria-plant symbioses (including therein liverworts and hornworts) (Rai et al. 2000), there is hardly any information on how mosscyanobacteria associations are formed, what genes are involved if any, and the extent of nitrogen exchange between host and symbiont. At an ecosystem level, a single study seldom combines more than a couple of host species and those are often closely related phylogenetically. This poses problems when estimations of regional biological atmospheric nitrogen fixation are attempted. Even the widely applied technique for measuring N 2 fixation rates by means of acetylene reduction capacity provides idiosyncratic data, unless calibrated for each analysis by means of N 2 labelled gas (DeLuca et al. 2002).	0	A great deal of variation in nitrogen fixation of cyanobacterial bryophytes has been attributed to abiotic factors; #CITATION_TAG et al. (2002) reported that in the High Arctic there is a strong dependency of fixation rates on site hydrology, temperature and light intensity. In regions with low precipitation during the growing season, for instance, N fixation is limited either to the period of snowmelt or to sites which stay wet during summer, e.g. peat bogs, fens, or dense moss mats (Zielke et al. 2005). Experimental longterm increase in UV-B radiation, simulating atmospheric ozone depletion, reduces N fixation activity in Arctic mosses, but does not affect it in sub-Arctic ones (Solheim et al. , 2006. Fixation rates also depend on the successional age of boreal forests via differences in environmental moisture, nutrition and possible other factors (DeLuca et al. 2008;Lagerstr�_m et al. 2009;Zackrisson et al. 2004).	A
CC2097	"Recently alternative schemes to ""simulate"" artificial gauge fields for neutral atoms have been explored using two-dimensional (2D) optical lattices [8,9,10,11,12,13,14]. As they do not involve any mechanical rotation of the system, they should be less sensitive to the imperfection of the trapping potential and thus easier to implement. The guideline for these proposals is the celebrated Harper model [#CITATION_TAG,16], defined by the two-dimensional (2D) single-particle Hamiltonian, n��1,m�_ n,m +�_ _�_ n,m��1�_ n,m + h.c."	0	"Recently alternative schemes to ""simulate"" artificial gauge fields for neutral atoms have been explored using two-dimensional (2D) optical lattices [8,9,10,11,12,13,14]. As they do not involve any mechanical rotation of the system, they should be less sensitive to the imperfection of the trapping potential and thus easier to implement. The guideline for these proposals is the celebrated Harper model [#CITATION_TAG,16], defined by the two-dimensional (2D) single-particle Hamiltonian, n��1,m�_ n,m +�_ _�_ n,m��1�_ n,m + h.c."	e
CC1855	In an attempt to bring greater scientific objectivity to the conceptualisation and assessment of clergy work-related psychological health, a number of studies have employed the Maslach Burnout Inventory (MBI) established by Maslach and Jackson (1996). These studies included work reported by Warner and Carter (1984), Str�_mpfer and Bands (1996), Rodgerson and Piedmont (1998), Stanton-Rich and Iso-Ahola (1998), Virginia (1998), Evers and Tomic (2003), Golden, Piedmont, Ciarrocchi, and Rodgerson (2004), Raj and Dean (2005), #CITATION_TAG (2007a, 2007b and Doolittle (2007).	0	In an attempt to bring greater scientific objectivity to the conceptualisation and assessment of clergy work-related psychological health, a number of studies have employed the Maslach Burnout Inventory (MBI) established by Maslach and Jackson (1996). These studies included work reported by Warner and Carter (1984), Str�_mpfer and Bands (1996), Rodgerson and Piedmont (1998), Stanton-Rich and Iso-Ahola (1998), Virginia (1998), Evers and Tomic (2003), Golden, Piedmont, Ciarrocchi, and Rodgerson (2004), Raj and Dean (2005), #CITATION_TAG (2007a, 2007b and Doolittle (2007).	h
CC774	11 It is worth noting though that neither increasing the number of firms (Tanger�_s, 2009) nor allowing for heterogeneity of customers (Dessein, 2003 and #CITATION_TAG, 2004) suffices to overturn this result.	0	11 It is worth noting though that neither increasing the number of firms (Tanger�_s, 2009) nor allowing for heterogeneity of customers (Dessein, 2003 and #CITATION_TAG, 2004) suffices to overturn this result.	1
CC590	Synergistic approaches to human communication have been argued for by others. Thibault (2011) adopts a position not unlike the present one in which a fundamental distinction is drawn between what he calls talk and text. The role of voice described both here and in his work emphasizes the bodily entrainment that arises at a very fine scale among interactants, while the properties that linguists conventionally consider, and that admit of a computational description, constitute a distinct, and secondorder set of phenomena. Although not focussed on languaging, #CITATION_TAG et al. (2011) argue that interpersonal movement coordination is the result of establishing interpersonal synergies of the sort described here, and they distinguish between componentdominant dynamics, as portrayed within a cognitivist framework, with interaction-dominant dynamics in which the autonomy of the level of interaction is more thoroughly acknowledged. Finally, the perceptual crossing paradigm introduced by Auvray et al. ( 2009) provides a minimalist experimental set up in which two people interact in real time in a minimal virtual space. While not communicative in any conventional sense, the nature of the emergent behavior observed serves to illustrate the principal point being made that the interaction itself constitutes a level of relative autonomy that is not reducible to the conjunction of properties of its components (Froese et al., 2014). These latter two examples illustrate that social interaction and languaging are not separate phenomena. Languaging is a constitutive part of the manner in which interpersonal entrainment or coupling arises in the moment by moment real time reciprocal interaction among people.	0	Synergistic approaches to human communication have been argued for by others. Thibault (2011) adopts a position not unlike the present one in which a fundamental distinction is drawn between what he calls talk and text. The role of voice described both here and in his work emphasizes the bodily entrainment that arises at a very fine scale among interactants, while the properties that linguists conventionally consider, and that admit of a computational description, constitute a distinct, and secondorder set of phenomena. Although not focussed on languaging, #CITATION_TAG et al. (2011) argue that interpersonal movement coordination is the result of establishing interpersonal synergies of the sort described here, and they distinguish between componentdominant dynamics, as portrayed within a cognitivist framework, with interaction-dominant dynamics in which the autonomy of the level of interaction is more thoroughly acknowledged. Finally, the perceptual crossing paradigm introduced by Auvray et al. ( 2009) provides a minimalist experimental set up in which two people interact in real time in a minimal virtual space. While not communicative in any conventional sense, the nature of the emergent behavior observed serves to illustrate the principal point being made that the interaction itself constitutes a level of relative autonomy that is not reducible to the conjunction of properties of its components (Froese et al., 2014). These latter two examples illustrate that social interaction and languaging are not separate phenomena.	h
CC903	"Bioethics has long been a multidisciplinary enterprise. 1 Increasingly, social science methods, approaches and perspectives are deemed to have relevance for or are even integrated into the bioethical enterprise. This is reflected in and further stimulated by recent moves towards ""empirical bioethics"" [#CITATION_TAG,27,40]. Science and technology studies (STS) is one such tradition that is articulating with bioethics, though sometimes fractiously [9,24]. In this paper I discuss what benefits bioethics might afford from (more) engagement with STS, whilst also highlighting some of the challenges to such rapprochements."	0	"Bioethics has long been a multidisciplinary enterprise. 1 Increasingly, social science methods, approaches and perspectives are deemed to have relevance for or are even integrated into the bioethical enterprise. This is reflected in and further stimulated by recent moves towards ""empirical bioethics"" [#CITATION_TAG,27,40]. Science and technology studies (STS) is one such tradition that is articulating with bioethics, though sometimes fractiously [9,24]. In this paper I discuss what benefits bioethics might afford from (more) engagement with STS, whilst also highlighting some of the challenges to such rapprochements."	i
CC2600	"Huczynski bases a chapter on the work of Kipnis et al (1984), who empirically developed a taxonomy of seven inuence strategies that managers use to get their way in their organisations (Kipnis et al, 1984, p.59): reason, friendliness, coalition, bargaining, assertiveness, higher authority and sanctions. Not all of these are directly applicable in a design context, but most have parallels with other strategies encountered in design for behaviour changee.g.`reason"" is seen in Petty and Cacioppo""s central route persuasion (Lockton, 2012b);`friendliness"" sums up a number of Carnegie""s recommendations;`higher authority"" is seen in Cialdini""s`authority"" (Lockton, 2012b). The idea of`coalition"" the mobilization of other people in the organisation (Kipnis et al, 1984, p.60)oers something interesting from a design perspective: can a product or system help`mobilise"" other people, perhaps someone""s peers or friends, to inuence him or her to change behaviour? 6 Neuro-linguistic programming Neuro-linguistic programming (NLP) is a dicult area to review; #CITATION_TAG and Mathison (2007) note that [t]here is little evidence of dialogue between [NLP] practitioners and academics, and while the few scientic treatments of it have found scant evidence that NLP techniques`work"" as claimed, NLP has generated a large following, and NLP ideas have also crept into a number of best-selling management and communication texts, as well as being widely used in training (Hartley, 1999, p.175) and even lifestyle coaching. Some of these ideas may be relevant in a design for behaviour change context, even if their background is controversial."	0	"Huczynski bases a chapter on the work of Kipnis et al (1984), who empirically developed a taxonomy of seven inuence strategies that managers use to get their way in their organisations (Kipnis et al, 1984, p.59): reason, friendliness, coalition, bargaining, assertiveness, higher authority and sanctions. Not all of these are directly applicable in a design context, but most have parallels with other strategies encountered in design for behaviour changee.g.`reason"" is seen in Petty and Cacioppo""s central route persuasion (Lockton, 2012b);`friendliness"" sums up a number of Carnegie""s recommendations;`higher authority"" is seen in Cialdini""s`authority"" (Lockton, 2012b). The idea of`coalition"" the mobilization of other people in the organisation (Kipnis et al, 1984, p.60)oers something interesting from a design perspective: can a product or system help`mobilise"" other people, perhaps someone""s peers or friends, to inuence him or her to change behaviour? 6 Neuro-linguistic programming Neuro-linguistic programming (NLP) is a dicult area to review; #CITATION_TAG and Mathison (2007) note that [t]here is little evidence of dialogue between [NLP] practitioners and academics, and while the few scientic treatments of it have found scant evidence that NLP techniques`work"" as claimed, NLP has generated a large following, and NLP ideas have also crept into a number of best-selling management and communication texts, as well as being widely used in training (Hartley, 1999, p.175) and even lifestyle coaching. Some of these ideas may be relevant in a design for behaviour change context, even if their background is controversial."	e
CC1968	Preclinical studies in animal models of TSC, however, have assessed the potential efficacy of mTOR-based treatment on TSC-related cognitive impairments. In adult mice with a heterozygous, inactivating mutation in the TSC2 gene, mTOR inhibition reversed TSC-related learning and memory deficits. These cognitive abnormalities emerged in the absence of neuropathology and seizures (Ehninger et al. 2008(Ehninger et al. , 2009. A recent study by Sato et al. (2012) in TSC mutant adult mice found that impaired social behavior also reversed by mTOR inhibitor treatment, which associated with mTOR inhibition at the molecular level. Others have also provided evidence that autistic-like behavior can be prevented with mTOR treatment in mouse models of TSC (Tsai et al, 2012;#CITATION_TAG et al, 2012;Reith et al. 2013). Altogether, these results propose that considerable therapeutic opportunities may exist for neuropsychiatric impairments associated with TSC, even if treatment is initiated in adulthood.	1	In adult mice with a heterozygous, inactivating mutation in the TSC2 gene, mTOR inhibition reversed TSC-related learning and memory deficits. These cognitive abnormalities emerged in the absence of neuropathology and seizures (Ehninger et al. 2008(Ehninger et al. , 2009. A recent study by Sato et al. (2012) in TSC mutant adult mice found that impaired social behavior also reversed by mTOR inhibitor treatment, which associated with mTOR inhibition at the molecular level. Others have also provided evidence that autistic-like behavior can be prevented with mTOR treatment in mouse models of TSC (Tsai et al, 2012;#CITATION_TAG et al, 2012;Reith et al. 2013). Altogether, these results propose that considerable therapeutic opportunities may exist for neuropsychiatric impairments associated with TSC, even if treatment is initiated in adulthood.	r
CC2930	"Our analysis suggests that the time is ripe for a critical assessment on what RCA has been able to achieve since its introduction almost a decade ago. Some of the challenges cannot be attributed to teething problems or defects in its implementation, but in fact, have much deeper roots. As such, tackling the shortcomings of RCA with more training (Wallace, 2006) or by arguing for increased independence and professionalization of RCA investigators (#CITATION_TAG & Lingham, 2009) may in fact exacerbate, instead of resolve these challenges. Whilst such a solution could improve the quality of the reports and the public confidence in the process, it would merely strengthen RCA""s affordance of governance and legitimation, instead of learning. A different approach needs to be pursued, therefore, if the aim is to foster organizational learning. This would imply, among others, four main things."	2	"Our analysis suggests that the time is ripe for a critical assessment on what RCA has been able to achieve since its introduction almost a decade ago. Some of the challenges cannot be attributed to teething problems or defects in its implementation, but in fact, have much deeper roots. As such, tackling the shortcomings of RCA with more training (Wallace, 2006) or by arguing for increased independence and professionalization of RCA investigators (#CITATION_TAG & Lingham, 2009) may in fact exacerbate, instead of resolve these challenges. Whilst such a solution could improve the quality of the reports and the public confidence in the process, it would merely strengthen RCA""s affordance of governance and legitimation, instead of learning. A different approach needs to be pursued, therefore, if the aim is to foster organizational learning. This would imply, among others, four main things."	 
CC1684	We hypothesize that the use of word embedding features will allow us to detect relevant words which are not present in the criteria descriptions. (#CITATION_TAGet al., 2013b) have shown that an important feature of Word2Vec embeddings is that similar words will have similar vectors because they appear in similar contexts. We utilize this feature to calculate similarity between the criteria descriptions and text segments (such as sentences) extracted from each document. A highlevel overview of our approach is shown in Figure 2.	5	We hypothesize that the use of word embedding features will allow us to detect relevant words which are not present in the criteria descriptions. (#CITATION_TAGet al., 2013b) have shown that an important feature of Word2Vec embeddings is that similar words will have similar vectors because they appear in similar contexts. We utilize this feature to calculate similarity between the criteria descriptions and text segments (such as sentences) extracted from each document. A highlevel overview of our approach is shown in Figure 2.	#
CC2704	Step 2: We decided to set the cardinality of the set S to max{20, min(3M, 50)}. This, and all subsequent settings are based on, and found appropriate by, the experimentation of #CITATION_TAG (2008). We could have chosen to consider all remaining fixed points that do not fall into a restricted region. However, this would have slowed down the algorithm.	4	Step 2: We decided to set the cardinality of the set S to max{20, min(3M, 50)}. This, and all subsequent settings are based on, and found appropriate by, the experimentation of #CITATION_TAG (2008). We could have chosen to consider all remaining fixed points that do not fall into a restricted region. However, this would have slowed down the algorithm.	h
CC1847	"Eysenck""s dimensional model of personality, as discussed for example by , maintains that individual differences in personality can be most adequately and economically summarised in terms of three higher order orthogonal dimensions (extraversion, neuroticism, and psychoticism). This model also takes the view that neurotic and psychotic disorders are not discontinuous from normal personality but occupy the extreme end of two different continua which describe individual differences in normal personality. Eysenck""s three dimensional model of personality has been operationalised in a series of instruments designed for use among both adults and young people, including the Eysenck Personality Questionnaire (#CITATION_TAG, 1975), the Eysenck Personality Questionnaire Revised (Eysenck, Eysenck, & Barrett, 1985) and the Eysenck Personality Scales (Eysenck & Eysenck, 1991). These instruments also routinely include a lie scale alongside the three established measures of extraversion, neuroticism, and psychoticism."	5	"Eysenck""s dimensional model of personality, as discussed for example by , maintains that individual differences in personality can be most adequately and economically summarised in terms of three higher order orthogonal dimensions (extraversion, neuroticism, and psychoticism). This model also takes the view that neurotic and psychotic disorders are not discontinuous from normal personality but occupy the extreme end of two different continua which describe individual differences in normal personality. Eysenck""s three dimensional model of personality has been operationalised in a series of instruments designed for use among both adults and young people, including the Eysenck Personality Questionnaire (#CITATION_TAG, 1975), the Eysenck Personality Questionnaire Revised (Eysenck, Eysenck, & Barrett, 1985) and the Eysenck Personality Scales (Eysenck & Eysenck, 1991). These instruments also routinely include a lie scale alongside the three established measures of extraversion, neuroticism, and psychoticism."	s
CC2583	"The aim would then be to try to match that person""s representational system via the language one uses to speak to him or her (Hartley, 1999, p.177)for example, if the eye movements suggest a visual system, using phrases such as I see what you mean or I get the picture would establish better rapport than I hear you . The academic study of cognitive linguistics, including the investigation of language and metaphor usage to explore people""s mental models (e.g. Lako and Johnson, 1980) is an interesting and potentially extremely useful eld for designers, but NLP""s claims about the existence of preferred representational systems and the validity of eye accessing cues have not been supported by empirical study (Sharpley, 1987;#CITATION_TAG et al, 2012)."	0	"The aim would then be to try to match that person""s representational system via the language one uses to speak to him or her (Hartley, 1999, p.177)for example, if the eye movements suggest a visual system, using phrases such as I see what you mean or I get the picture would establish better rapport than I hear you . The academic study of cognitive linguistics, including the investigation of language and metaphor usage to explore people""s mental models (e.g. Lako and Johnson, 1980) is an interesting and potentially extremely useful eld for designers, but NLP""s claims about the existence of preferred representational systems and the validity of eye accessing cues have not been supported by empirical study (Sharpley, 1987;#CITATION_TAG et al, 2012)."	T
CC1662	PAGE09 uses a simple economic module (Hope et al. 1993;#CITATION_TAG et al. 1997;Hope 2006;Hope 2008;Hope 2011) and expands it to consider climate issues and the linkages between the economic and the climate systems through some stylized equations within the climate module. Uncertainty is taken into account through Latin Hypercube sampling 4 . Functional forms are assumed to be known with certainty, while each of the uncertain model parameters (approximately 80) is represented by a probability distribution. The discount rate in our model is unaltered from the PAGE09 standard form, which discounts according to an equity weighting scheme, and then by the rate of pure time preference Hope 2011. The equity weighting scheme converts changes in consumption into utility giving more (less) weight to consumption per capita in poorer (richer) regions and time periods. The weighting is dependent on the elasticity of marginal utility of conomics: The Open-Access, Open-Assessment E-Journal consumption, which is entered as a triangle distribution (0.5, 1, 2). 5 Following the equity weighting, the damages are further discounted at the rate of pure time preference, which is entered as a triangle distribution (0.1, 1, 2) measured in percentage per year. A full run of the model involves repeating the calculations of the following output variables: global warming over time, damages, adaptive costs and abatement costs.	5	PAGE09 uses a simple economic module (Hope et al. 1993;#CITATION_TAG et al. 1997;Hope 2006;Hope 2008;Hope 2011) and expands it to consider climate issues and the linkages between the economic and the climate systems through some stylized equations within the climate module. Uncertainty is taken into account through Latin Hypercube sampling 4 . Functional forms are assumed to be known with certainty, while each of the uncertain model parameters (approximately 80) is represented by a probability distribution. The discount rate in our model is unaltered from the PAGE09 standard form, which discounts according to an equity weighting scheme, and then by the rate of pure time preference Hope 2011. The equity weighting scheme converts changes in consumption into utility giving more (less) weight to consumption per capita in poorer (richer) regions and time periods.	P
CC307	Implementing technology in anticipation of future audits has also been reported by #CITATION_TAG (2002) in a study on the adoption of legal expert systems in municipal social security systems. In our study, pressure from legislation was also reported, although non-adopters reported this more frequently than adopters. Legislative pressure therefore cannot explain the adoption of personalised e-government.	1	Implementing technology in anticipation of future audits has also been reported by #CITATION_TAG (2002) in a study on the adoption of legal expert systems in municipal social security systems. In our study, pressure from legislation was also reported, although non-adopters reported this more frequently than adopters. Legislative pressure therefore cannot explain the adoption of personalised e-government.	I
CC517	"To fight corruption, it is necessary to have institutions in place that disseminate relevant information, and voters who act on that information to hold politicians accountable. Costas et al. (2011), Freille et al. (2007, Lederman et al. (2005), Adsera et al. (2003), Brunetti and Weder (2003), and Besley and Burgess (2002) provide evidence on the importance of a free press in reducing corruption. However, as Johnson et al. (2011), Chang et al. (2010), and Golden (2006) point out, corruption exists even in advanced democracies. Worse yet, Costas et al. (2011), Fern��ndez-V��zquez andRivero (2010), Chang et al. (2010), and Reed (2005) show, through Spanish, Italian, and Japanese examples, that such corruption usually makes very little difference in the reelection fortunes of politicians even when it is public knowledge. Welch and Hibbing (1997), Dimock and Jacobson (1995), and Peters and Welch (1980) provide similar evidence in the case of U.S. Chang et al. (2010), Fern��ndez-V��zquez andRivero (2010), Manzetti and Wilson (2007) and Golden (2006) offer some explanations as to why this is so. They argue that voters may doubt the information or dismiss it as partisan, especially if the accusations are leveled predominantly against the members of one party. The pool of candidates from which citizens can choose may be seriously restricted in terms of their quality. Also, the voters may take corruptness of an incumbent into account in casting their ballots, but only as one of his/her many attributes. Especially if they believe that the honest challengers will not be able to deliver the same results in terms of economic development and increases in their well-being, many of them may still vote for the 1 It should be noted that some researchers, such as Nye (1989) and Leff (1964) argue that optimal level of corruption may be non-zero. They assert that bribing can be viewed as greasing the wheels of the government, enabling firms to sidestep burdensome government controls. However others dispute the existence of a stable growth-enhancing equilibrium level of corruption. Rose-Ackerman (1997) for example argues that corruption will always escalate to ever higher levels, and Kaufman and Wei (2000) find that in economies where corruption is high and more bribes have to be paid, managers end up allocating more time to public officials and less time to productive work. A more recent and more rigorous study by Swaleheen (2011) shows that the ""grease-in-the-wheel"" argument applies only in the cases of very high-corruption countries, and by Carden and Verdon (2010) only in the cases of low-freedom countries . Since such countries do not have fair elections and voter response to speak of, they do not fall under the subject matter of our paper. corrupt incumbent. Consequently, a politician can offset, at least partially, the negative impact of his/her corrupt behavior by transferring government benefits to his/her constituents, supporting economic policies with which they agree and/or governing competently otherwise. Corrupt incumbents may lose votes but not enough to deny them re-election, as long as they keep the level of corruption in check and do not allow it to damage overall economic performance significantly. This would explain why Pellegrini and Gerlagh (2008) and Lederman et al. (2005) find the level of corruption to be lower, and  #CITATION_TAG et al. (2006), its harm on economic growth to be less, in democratic countries."	1	corrupt incumbent. Consequently, a politician can offset, at least partially, the negative impact of his/her corrupt behavior by transferring government benefits to his/her constituents, supporting economic policies with which they agree and/or governing competently otherwise. Corrupt incumbents may lose votes but not enough to deny them re-election, as long as they keep the level of corruption in check and do not allow it to damage overall economic performance significantly. This would explain why Pellegrini and Gerlagh (2008) and Lederman et al. (2005) find the level of corruption to be lower, and  #CITATION_TAG et al. (2006), its harm on economic growth to be less, in democratic countries.	i
CC1614	"The extent to which cultural processes may be modeled in evolutionary terms remains disputed (e.g., Bamforth 2002;Fracchia & Lewontin 1999, 2005, although the way in which cultural entities and processes closely match Darwin""s original formulation of the theory of evolution has recently been shown in detail by #CITATION_TAG et al. (2006). In the most general terms, they involve parallel mechanisms for inheritance, mutation, selection, and drift."	0	"The extent to which cultural processes may be modeled in evolutionary terms remains disputed (e.g., Bamforth 2002;Fracchia & Lewontin 1999, 2005, although the way in which cultural entities and processes closely match Darwin""s original formulation of the theory of evolution has recently been shown in detail by #CITATION_TAG et al. (2006). In the most general terms, they involve parallel mechanisms for inheritance, mutation, selection, and drift."	T
CC1587	"The literature on industrial marketing seems to agree that companies maintain relationships either because they have to or because they want to. The commitment concept helps understand this duality in bonding rationales. Commitment occupies a central role in the study of successful relationships between firms (Gilliland & Bello, 2002;Gundlach et al., 1995;Hunt et al., 2006;Morgan & Hunt, 1994). While early research models commitment as a onedimensional construct (e.g. #CITATION_TAG, 1996;Anderson & Weitz, 1992;Ganesan, 1994), later studies often highlight its duality. Bendapudi and Berry (1997) approach this dichotomy as ""dedication-based"" versus ""constraint-based"" relationship maintenance. In other studies, ""affective commitment"" is the extent to which partners like to stay in existing relationships while ""calculative commitment"" is the degree to which they need to stay (de Ruyter et al., 2001;Geyskens et al., 1996). Brown et al. (1995) used ""normative"" and ""instrumental"" commitment for very similar notions of bonding between firms. S�_llner (1999) bases his arguments on transaction cost analysis (TCA) and becomes somewhat more specific about the bonding dimensions in buyer-seller relationships: on the one hand, instrumental and attitudinal inputs by one partner are specific economic or social-psychological investments 1 that would be lost outside the focal relationship and hence act as switching costs. In order not to incur those switching costs the partner stays in the relationship. On the other hand, relationship outputs (performance, justice) represent another bonding dimension which makes business partners want to stay in a relationship. More recently, this voluntary dimension is investigated as relationship value (Corsaro & Snehota, 2010;Ulaga, 2003;."	1	"The literature on industrial marketing seems to agree that companies maintain relationships either because they have to or because they want to. The commitment concept helps understand this duality in bonding rationales. Commitment occupies a central role in the study of successful relationships between firms (Gilliland & Bello, 2002;Gundlach et al., 1995;Hunt et al., 2006;Morgan & Hunt, 1994). While early research models commitment as a onedimensional construct (e.g. #CITATION_TAG, 1996;Anderson & Weitz, 1992;Ganesan, 1994), later studies often highlight its duality. Bendapudi and Berry (1997) approach this dichotomy as ""dedication-based"" versus ""constraint-based"" relationship maintenance. In other studies, ""affective commitment"" is the extent to which partners like to stay in existing relationships while ""calculative commitment"" is the degree to which they need to stay (de Ruyter et al., 2001;Geyskens et al., 1996). Brown et al. (1995) used ""normative"" and ""instrumental"" commitment for very similar notions of bonding between firms."	l
CC3000	Since the new trends in concrete construction goes towards self-compacting green concrete, it becomes important to study the possibility and effectiveness of using steel slag aggregate (SSA) in self-compacting concrete (SCC) as a replacement of coarse aggregate. SSA has higher density and higher angularity than normal aggregate #CITATION_TAG.	2	Since the new trends in concrete construction goes towards self-compacting green concrete, it becomes important to study the possibility and effectiveness of using steel slag aggregate (SSA) in self-compacting concrete (SCC) as a replacement of coarse aggregate. SSA has higher density and higher angularity than normal aggregate #CITATION_TAG.	S
CC2572	"Knowledge representation and reasoning techniques. In order to build an accurate and trustable knowledge base and infer the behaviour that better matches users"" expectations, AI techniques for knowledge representation and automated reasoning are promising. Recommender systems (Adomavicius and Tuzhilin, 2005) may provide (even automatically execute) recommendations on which services to apply; some applications in the marketing context, e.g. (de Bruin et al., 2008), have explored this particular aspect. Case-based reasoning (#CITATION_TAG and Plaza, 1994) may be useful to improve the knowledge and reasoning capabilities of individual users case by case. More recently, the use of ontology in combination with statistical models is proposed to provide models of human behaviours in a given context (Codescu, et al., 2011)."	0	"Knowledge representation and reasoning techniques. In order to build an accurate and trustable knowledge base and infer the behaviour that better matches users"" expectations, AI techniques for knowledge representation and automated reasoning are promising. Recommender systems (Adomavicius and Tuzhilin, 2005) may provide (even automatically execute) recommendations on which services to apply; some applications in the marketing context, e.g. (de Bruin et al., 2008), have explored this particular aspect. Case-based reasoning (#CITATION_TAG and Plaza, 1994) may be useful to improve the knowledge and reasoning capabilities of individual users case by case. More recently, the use of ontology in combination with statistical models is proposed to provide models of human behaviours in a given context (Codescu, et al., 2011)."	e
CC41	It has been shown in [4], #CITATION_TAG and that many bibliometric indicators show little correlation with peer review judgments at the article level. This study, and those by [7], [2] and [3], demonstrate that some bibliometric measures can offer a surprisingly high degree of accuracy when used at the institutional or departmental level. Our work has been conducted on a significantly larger dataset and our prediction accuracy is higher than shown in previous studies, despite deliberately using fairly simplistic indicators.	0	It has been shown in [4], #CITATION_TAG and that many bibliometric indicators show little correlation with peer review judgments at the article level. This study, and those by [7], [2] and [3], demonstrate that some bibliometric measures can offer a surprisingly high degree of accuracy when used at the institutional or departmental level. Our work has been conducted on a significantly larger dataset and our prediction accuracy is higher than shown in previous studies, despite deliberately using fairly simplistic indicators.	I
CC367	An analogous challenge has arisen in fMRI studies of visual perception. Early attempts to localize specific perceptions to specific brain areas worked only for grossly different stimuli-visualizing navigating a house compared to imagining playing tennis (Owen et al., 2006)-or for simple stimuli in early, highly specialized, visual areas (Kay et al., 2008). More latterly, however, multivoxel pattern analysis (MVPA), in which patterns of activity across wide areas of the brain are analyzed, has produced a significant increase in cognitive resolution. Fairly similar stimuli, for example chairs and shoes (Norman et al., 2006;deCharms, 2008;Poldrack, 2011), or over-riding categories of images such as living or non-living (Naselaris et al., 2012) can now be recognized from pattern information fMRI (Formisano and Kriegeskorte, 2012) data. Not only perceptions, but also images and memories (Chadwick et al., 2012;#CITATION_TAG and Wagner, 2012) are starting to be distinguished. Beginnings are starting to be made in reproducing data across as well as within subjects (Accamma and Suma, 2012;Raizada and Connolly, 2012). Significantly, cognitive resolution appears to increase as the focus of the analysis is widened to include more brain areas.	0	Early attempts to localize specific perceptions to specific brain areas worked only for grossly different stimuli-visualizing navigating a house compared to imagining playing tennis (Owen et al., 2006)-or for simple stimuli in early, highly specialized, visual areas (Kay et al., 2008). More latterly, however, multivoxel pattern analysis (MVPA), in which patterns of activity across wide areas of the brain are analyzed, has produced a significant increase in cognitive resolution. Fairly similar stimuli, for example chairs and shoes (Norman et al., 2006;deCharms, 2008;Poldrack, 2011), or over-riding categories of images such as living or non-living (Naselaris et al., 2012) can now be recognized from pattern information fMRI (Formisano and Kriegeskorte, 2012) data. Not only perceptions, but also images and memories (Chadwick et al., 2012;#CITATION_TAG and Wagner, 2012) are starting to be distinguished. Beginnings are starting to be made in reproducing data across as well as within subjects (Accamma and Suma, 2012;Raizada and Connolly, 2012). Significantly, cognitive resolution appears to increase as the focus of the analysis is widened to include more brain areas.	o
CC1652	Finally, Prendergast (2002a) argues the degree of multitasking in a job is also likely to be positively related to the use of pay for performance. The earlier literature on multitasking suggested the opposite might be true, because of the difficulty of balancing incentives across tasks with different uncontrollable risk. His counterargument is that jobs with more tasks are more costly to monitor, which might tip the balance toward incentives. An additional argument is suggested by #CITATION_TAG, Levenson & Zoghi (2010), who also provide evidence of a strong positive correlation between delegation and multitasking (but do not have data on incentives). A more complex job with multiple tasks may create greater worker knowledge that can drive continuous improvement. Continuous improvement in turn may require delega-tion so that the worker can use his information to test and implement new methods. Delegation would then be complementary with incentive pay.	0	Finally, Prendergast (2002a) argues the degree of multitasking in a job is also likely to be positively related to the use of pay for performance. The earlier literature on multitasking suggested the opposite might be true, because of the difficulty of balancing incentives across tasks with different uncontrollable risk. His counterargument is that jobs with more tasks are more costly to monitor, which might tip the balance toward incentives. An additional argument is suggested by #CITATION_TAG, Levenson & Zoghi (2010), who also provide evidence of a strong positive correlation between delegation and multitasking (but do not have data on incentives). A more complex job with multiple tasks may create greater worker knowledge that can drive continuous improvement. Continuous improvement in turn may require delega-tion so that the worker can use his information to test and implement new methods. Delegation would then be complementary with incentive pay.	a
CC589	"Repetition also serves to accentuate and exaggerate the rhythmic properties of utterances, while repetition of a short phrase can also induce a change in perception from speech to song (Deutsch et al., 2011). In repeated spoken chants, the form of speech that arises thus blends seamlessly into the musical domain, establishing a continuity between speech and music. The close relation between spoken and sung chant is signaled by the very ambiguous nature of the word ""chant"" in English which applies with equal facility in either domain. It is interesting that a focus on collective speech makes a continuum between speech and music appear natural, even obligatory, while the message passing perspective as articulated most clearly by Pinker (1999) insists on an absolute divide between the two domains. On the messagepassing view, speech is an expression of the highly valued notional faculty of language, and thus central to our human minds, while music is denigrated as ""auditory cheesecake,"" with no-from his perspective-apparent functional significance, thus meriting being grouped together with artistic expression, cheesecake, and pornography (#CITATION_TAG, 1999). If anything illustrates the limited capacity to describe, or even see, that the message passing perspective induces, surely it is this failure to appreciate the continuum we are all familiar with that extends from instrumental music, through song, rap, poetry, rhymes, rhetoric, and chant (Cummins, 2013a). We might note in passing that the contrast between the real time participatory nature of the voice that is here contrasted strongly with the frozen nature of writing finds a strong parallel in contemporary discussion of the relationship between live musical performance and recording (Chanan, 1995)."	1	"In repeated spoken chants, the form of speech that arises thus blends seamlessly into the musical domain, establishing a continuity between speech and music. The close relation between spoken and sung chant is signaled by the very ambiguous nature of the word ""chant"" in English which applies with equal facility in either domain. It is interesting that a focus on collective speech makes a continuum between speech and music appear natural, even obligatory, while the message passing perspective as articulated most clearly by Pinker (1999) insists on an absolute divide between the two domains. On the messagepassing view, speech is an expression of the highly valued notional faculty of language, and thus central to our human minds, while music is denigrated as ""auditory cheesecake,"" with no-from his perspective-apparent functional significance, thus meriting being grouped together with artistic expression, cheesecake, and pornography (#CITATION_TAG, 1999). If anything illustrates the limited capacity to describe, or even see, that the message passing perspective induces, surely it is this failure to appreciate the continuum we are all familiar with that extends from instrumental music, through song, rap, poetry, rhymes, rhetoric, and chant (Cummins, 2013a). We might note in passing that the contrast between the real time participatory nature of the voice that is here contrasted strongly with the frozen nature of writing finds a strong parallel in contemporary discussion of the relationship between live musical performance and recording (Chanan, 1995)."	h
CC11	"Content-based recommender systems [15] rely on a pre-existing domain knowledge to suggest items more similar to the ones that the user seems to like. They usually generate user models that describe user interests according to a set of features [16]. With the advent of the Semantic Web, several recommender systems started to adopt ontologies for representing both user interests and items #CITATION_TAG. Often these systems use an ontology so that, given user interest in an item represented in the ontology, they can then propagate such interest to relevant items and concepts. For example, given a positive feedback on ""beagles a system may infer (correctly or not) that a user is interested in ""dogs and more generally in ""pets"". SBR exploits a similar mechanism when it infers that a publication explicitly linked to a topic (e.g., Linked Data) is also about its skos:broaderGeneric concepts in CSO (e.g., Semantic Web). The main advantages of these solutions are i) the ability to exploit the domain knowledge for improving the user modelling process, ii) the ability to share and reuse system knowledge, and iii) the alleviation of the cold-start and data sparsity problems [16,18]."	0	"Content-based recommender systems [15] rely on a pre-existing domain knowledge to suggest items more similar to the ones that the user seems to like. They usually generate user models that describe user interests according to a set of features [16]. With the advent of the Semantic Web, several recommender systems started to adopt ontologies for representing both user interests and items #CITATION_TAG. Often these systems use an ontology so that, given user interest in an item represented in the ontology, they can then propagate such interest to relevant items and concepts. For example, given a positive feedback on ""beagles a system may infer (correctly or not) that a user is interested in ""dogs and more generally in ""pets"". SBR exploits a similar mechanism when it infers that a publication explicitly linked to a topic (e.g., Linked Data) is also about its skos:broaderGeneric concepts in CSO (e.g., Semantic Web)."	t
CC2687	A puzzling result of our study is the asymmetric effect of regime change on the level of protection: why should a regime change be relevant only for a transition to democracy, and not vice versa ? One possible explanation of such asymmetric effect of transitions to democracy and autocracy could be based on theories explaining (lack of) leadership turnovers and economic performance under autocracies (see #CITATION_TAG and Kudamatsu, 2008;Acemoglu et al. 2004). These studies emphasise the importance of some institutional features of autocracy, in particular political stability. The objective of staying in power could stop a potential policy reversal against the agrarian population by preventing rural unrest. Our data preclude a deeper investigation of this issue but further analysis would certainly constitute a fruitful line of inquiry.	0	A puzzling result of our study is the asymmetric effect of regime change on the level of protection: why should a regime change be relevant only for a transition to democracy, and not vice versa ? One possible explanation of such asymmetric effect of transitions to democracy and autocracy could be based on theories explaining (lack of) leadership turnovers and economic performance under autocracies (see #CITATION_TAG and Kudamatsu, 2008;Acemoglu et al. 2004). These studies emphasise the importance of some institutional features of autocracy, in particular political stability. The objective of staying in power could stop a potential policy reversal against the agrarian population by preventing rural unrest. Our data preclude a deeper investigation of this issue but further analysis would certainly constitute a fruitful line of inquiry.	n
CC1239	"Statistical models have dominated data analysis in the social sciences, including educational psychology (Dekker et al., 2009;Freedman, 1987;Herzog, 2006). For example, the studies cited in section 2 primarily used correlation (78% of the studies) and regression (54% of the studies), with some papers citing path analysis results (14%) and structural equation models (11%). Statistical modelling has a sound theoretical basis, allowing verifiable conclusions to be drawn from model coefficients; therefore, statistical models have made, and will continue to make, a valuable contribution to the understanding of learners and the learning process. However, such models are based on assumptions, including assumptions of normality, independency, linear additively, and constant variance (#CITATION_TAG et al., 2009). It is evident from current knowledge of the factors influencing academic performance, that such factors are interdependent (Prinsloo et al., 2012). While each factor measures unique attributes, overlaps occur in the constructs being measured. In addition, there is evidence to suggest variance is not constant for all attributes. For example, De Feyter et al. (2012) found that low levels of self-efficacy had a positive, direct effect on academic performance for neurotic students only, and for stable students, average or higher levels of self-efficacy had a direct effect on academic performance. In addition, Vancouver and Kendall (2006) found evidence that high levels of self-efficacy can lead to overconfidence regarding exam preparedness, which in turn can have a negative impact on academic performance. Similarly, Poropat (2009) cites evidence of non-linear relationships between factors of personality and academic performance, including conscientiousness and openness. Duff et al. (2004) observed that because academic performance is itself a complex measure, calculated as an aggregate of a variety of assessment types, this weakens the result of correlation analysis with other learning dimensions. While recognizing the continuing importance of statistical models, Freedman (1987) and Breiman (2001) argued that alternative-modelling approaches should be considered when dimensionality is high, and relationships are complex such as in the social sciences. Cox, in a response to Breiman""s paper, notes the importance of the probabilistic base of standard statistical modelling, but agrees with Breiman that in some circumstances, an empirical approach is better (Breiman, 2001, p. 18). It is therefore pertinent to ask if data mining""s empirical modelling approach can add value to psychometric data analysis, in particular their relevance to models of academic achievement."	0	Statistical models have dominated data analysis in the social sciences, including educational psychology (Dekker et al., 2009;Freedman, 1987;Herzog, 2006). For example, the studies cited in section 2 primarily used correlation (78% of the studies) and regression (54% of the studies), with some papers citing path analysis results (14%) and structural equation models (11%). Statistical modelling has a sound theoretical basis, allowing verifiable conclusions to be drawn from model coefficients; therefore, statistical models have made, and will continue to make, a valuable contribution to the understanding of learners and the learning process. However, such models are based on assumptions, including assumptions of normality, independency, linear additively, and constant variance (#CITATION_TAG et al., 2009). It is evident from current knowledge of the factors influencing academic performance, that such factors are interdependent (Prinsloo et al., 2012). While each factor measures unique attributes, overlaps occur in the constructs being measured. In addition, there is evidence to suggest variance is not constant for all attributes.	e
CC2337	"This article examines the dependence of attaining a salariat occupation on parental occupational and educational characteristics, and the extent to which family structure has an effect on this outcome stretching beyond these characteristics. Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008;Kuha and Goldthorpe, 2010) still tend to focus on father""s occupation or the ""dominant"" parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents"" characteristics (Beller, 2009;Lampard, 2007a;Marks, 2009;Schoon, 2008). Furthermore, studies of occupational outcomes in Britain have examined family-type effects surprisingly rarely, given the prominence of family ""disruption"" in numerous educational attainment studies, often using longitudinal data (Cusworth, 2009;Ely et al., 2000;Ermisch and Francesconi, 2001;Ermisch et al., 2004;Kiernan, 1992#CITATION_TAG , 1996Kiernan, , 1997Lampard, 2007b;N�_ Bhrolch͍in et al., 2000;Scott, 2004). Beller (2004) noted the scarcity, internationally, of research on social mobility variations according to family type (although see Biblarz and Raftery, 1999); more specifically, she noted an absence of attempts within mobility analyses to integrate both parents plus family type into conceptualizations of family of origin (Beller, 2003)."	1	"This article examines the dependence of attaining a salariat occupation on parental occupational and educational characteristics, and the extent to which family structure has an effect on this outcome stretching beyond these characteristics. Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008;Kuha and Goldthorpe, 2010) still tend to focus on father""s occupation or the ""dominant"" parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents"" characteristics (Beller, 2009;Lampard, 2007a;Marks, 2009;Schoon, 2008). Furthermore, studies of occupational outcomes in Britain have examined family-type effects surprisingly rarely, given the prominence of family ""disruption"" in numerous educational attainment studies, often using longitudinal data (Cusworth, 2009;Ely et al., 2000;Ermisch and Francesconi, 2001;Ermisch et al., 2004;Kiernan, 1992#CITATION_TAG , 1996Kiernan, , 1997Lampard, 2007b;N�_ Bhrolch͍in et al., 2000;Scott, 2004). Beller (2004) noted the scarcity, internationally, of research on social mobility variations according to family type (although see Biblarz and Raftery, 1999); more specifically, she noted an absence of attempts within mobility analyses to integrate both parents plus family type into conceptualizations of family of origin (Beller, 2003)."	r
CC2689	A series of empirical studies have tried to test these predictions using data on democracy and policies. For example, Besley and Kudamatsu (2006), using panel data, find that health policy interventions are superior in democracies. Other papers have investigated how democracy affect economic liberalization. #CITATION_TAG and Sturm (2003), using a developing country sample, show that greater political freedom furthers economic freedom. However, Giavazzi and Tabellini (2005) argue that economic liberalization often precedes political liberalization.	0	A series of empirical studies have tried to test these predictions using data on democracy and policies. For example, Besley and Kudamatsu (2006), using panel data, find that health policy interventions are superior in democracies. Other papers have investigated how democracy affect economic liberalization. #CITATION_TAG and Sturm (2003), using a developing country sample, show that greater political freedom furthers economic freedom. However, Giavazzi and Tabellini (2005) argue that economic liberalization often precedes political liberalization.	T
CC2342	"This article examines the dependence of attaining a salariat occupation on parental occupational and educational characteristics, and the extent to which family structure has an effect on this outcome stretching beyond these characteristics. Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008;Kuha and Goldthorpe, 2010) still tend to focus on father""s occupation or the ""dominant"" parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents"" characteristics (Beller, 2009;Lampard, 2007a;Marks, 2009;Schoon, 2008). Furthermore, studies of occupational outcomes in Britain have examined family-type effects surprisingly rarely, given the prominence of family ""disruption"" in numerous educational attainment studies, often using longitudinal data (Cusworth, 2009;Ely et al., 2000;#CITATION_TAG and Francesconi, 2001;Ermisch et al., 2004;Kiernan, 1992Kiernan, , 1996Kiernan, , 1997Lampard, 2007b;N�_ Bhrolch͍in et al., 2000;Scott, 2004). Beller (2004) noted the scarcity, internationally, of research on social mobility variations according to family type (although see Biblarz and Raftery, 1999); more specifically, she noted an absence of attempts within mobility analyses to integrate both parents plus family type into conceptualizations of family of origin (Beller, 2003)."	1	"This article examines the dependence of attaining a salariat occupation on parental occupational and educational characteristics, and the extent to which family structure has an effect on this outcome stretching beyond these characteristics. Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008;Kuha and Goldthorpe, 2010) still tend to focus on father""s occupation or the ""dominant"" parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents"" characteristics (Beller, 2009;Lampard, 2007a;Marks, 2009;Schoon, 2008). Furthermore, studies of occupational outcomes in Britain have examined family-type effects surprisingly rarely, given the prominence of family ""disruption"" in numerous educational attainment studies, often using longitudinal data (Cusworth, 2009;Ely et al., 2000;#CITATION_TAG and Francesconi, 2001;Ermisch et al., 2004;Kiernan, 1992Kiernan, , 1996Kiernan, , 1997Lampard, 2007b;N�_ Bhrolch͍in et al., 2000;Scott, 2004). Beller (2004) noted the scarcity, internationally, of research on social mobility variations according to family type (although see Biblarz and Raftery, 1999); more specifically, she noted an absence of attempts within mobility analyses to integrate both parents plus family type into conceptualizations of family of origin (Beller, 2003)."	r
CC2338	"This article examines the dependence of attaining a salariat occupation on parental occupational and educational characteristics, and the extent to which family structure has an effect on this outcome stretching beyond these characteristics. Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008;Kuha and Goldthorpe, 2010) still tend to focus on father""s occupation or the ""dominant"" parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents"" characteristics (Beller, 2009;Lampard, 2007a;Marks, 2009;Schoon, 2008). Furthermore, studies of occupational outcomes in Britain have examined family-type effects surprisingly rarely, given the prominence of family ""disruption"" in numerous educational attainment studies, often using longitudinal data (Cusworth, 2009;Ely et al., 2000;Ermisch and Francesconi, 2001;Ermisch et al., 2004;Kiernan, 1992Kiernan, , 1996Kiernan, , 1997Lampard, 2007b;N�_ Bhrolch͍in et al., 2000;Scott, 2004). Beller (2004) noted the scarcity, internationally, of research on social mobility variations according to family type (although see #CITATION_TAG and Raftery, 1999); more specifically, she noted an absence of attempts within mobility analyses to integrate both parents plus family type into conceptualizations of family of origin (Beller, 2003)."	1	"This article examines the dependence of attaining a salariat occupation on parental occupational and educational characteristics, and the extent to which family structure has an effect on this outcome stretching beyond these characteristics. Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008;Kuha and Goldthorpe, 2010) still tend to focus on father""s occupation or the ""dominant"" parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents"" characteristics (Beller, 2009;Lampard, 2007a;Marks, 2009;Schoon, 2008). Furthermore, studies of occupational outcomes in Britain have examined family-type effects surprisingly rarely, given the prominence of family ""disruption"" in numerous educational attainment studies, often using longitudinal data (Cusworth, 2009;Ely et al., 2000;Ermisch and Francesconi, 2001;Ermisch et al., 2004;Kiernan, 1992Kiernan, , 1996Kiernan, , 1997Lampard, 2007b;N�_ Bhrolch͍in et al., 2000;Scott, 2004). Beller (2004) noted the scarcity, internationally, of research on social mobility variations according to family type (although see #CITATION_TAG and Raftery, 1999); more specifically, she noted an absence of attempts within mobility analyses to integrate both parents plus family type into conceptualizations of family of origin (Beller, 2003)."	l
CC2657	Hence we combine a difference-in-differences methodology with a propensity score matching method. As such we follow the approach discussed by Smith and Todd (2005) and #CITATION_TAG (2005) and applied by Blundell et al. (2004) and Persson and Tabellini (2008).	5	Hence we combine a difference-in-differences methodology with a propensity score matching method. As such we follow the approach discussed by Smith and Todd (2005) and #CITATION_TAG (2005) and applied by Blundell et al. (2004) and Persson and Tabellini (2008).	s
CC1827	The results also demonstrate how choice can be biased towards action. The fact that choices associated with action are preferred over choices associated with inaction suggests a possible source for suboptimal decision making. Given that this bias is independent of valence it may reflect a general action bias observed in instrumental learning under uncertainty [24,[30]#CITATION_TAG[32]. This choice bias is especially interesting because it makes explanations of enhanced revaluation due to effort [33,34] less plausible for the effect of action on the dynamics of choice-induced preference change. If the key press was perceived as effortful one would expect subjects to be biased against pressing the key, as item values were closely matched. By contrast, a general bias towards action could be explained by a higher hedonic value of action itself if inaction is perceived as inhibition [35,36].	0	The results also demonstrate how choice can be biased towards action. The fact that choices associated with action are preferred over choices associated with inaction suggests a possible source for suboptimal decision making. Given that this bias is independent of valence it may reflect a general action bias observed in instrumental learning under uncertainty [24,[30]#CITATION_TAG[32]. This choice bias is especially interesting because it makes explanations of enhanced revaluation due to effort [33,34] less plausible for the effect of action on the dynamics of choice-induced preference change. If the key press was perceived as effortful one would expect subjects to be biased against pressing the key, as item values were closely matched. By contrast, a general bias towards action could be explained by a higher hedonic value of action itself if inaction is perceived as inhibition [35,36].	v
CC2947	Contrary to the Chomskyan view of language, another ingredient of language that may go well back in primate and even mammalian evolution is symbolic understanding. Great apes and even dogs are easily taught to understand symbols in terms of what they represent. The bonobo Kanzi communicates by pointing at nonrepresentational symbols on a keyboard, and can even obey simple requests conveyed through spoken English (Savage-Rumbaugh et al., 1998). The gorilla Koko is said to use over 1000 signs and to understand and express signed requests, and he too can respond meaningfully to simple requests spoken in English (#CITATION_TAG and Gordon, 2001). A border collie called Rico has been shown to rapidly acquire the meanings of some 200 spoken English words (Kaminsky et al., 2004). Rico has since been trumped by another border collie called Chaser, who understands over 1000 proper names as verbal referents (Pilley and Reid, 2011). These accomplishments might also be taken to reflect mental time travel, since they often involve reference to non-present objects or actions. For instance, Kanzi might point to a symbol to request a banana, or ask to be tickled, or invite play, and Rico and Chaser demonstrate their linguistic skills by going on request into another room to fetch a designated object.	0	Contrary to the Chomskyan view of language, another ingredient of language that may go well back in primate and even mammalian evolution is symbolic understanding. Great apes and even dogs are easily taught to understand symbols in terms of what they represent. The bonobo Kanzi communicates by pointing at nonrepresentational symbols on a keyboard, and can even obey simple requests conveyed through spoken English (Savage-Rumbaugh et al., 1998). The gorilla Koko is said to use over 1000 signs and to understand and express signed requests, and he too can respond meaningfully to simple requests spoken in English (#CITATION_TAG and Gordon, 2001). A border collie called Rico has been shown to rapidly acquire the meanings of some 200 spoken English words (Kaminsky et al., 2004). Rico has since been trumped by another border collie called Chaser, who understands over 1000 proper names as verbal referents (Pilley and Reid, 2011). These accomplishments might also be taken to reflect mental time travel, since they often involve reference to non-present objects or actions.	 
CC1376	"Dig""s paper [12] exemplifies the main approach for OO languages in targetting thread-safe libraries and data structures within a general-purpose language, which, once achieved, provides further refactoring opportunities. Alternatively, programs can be targeted at specialised hardware, such as GPUs  #CITATION_TAG and multicore systems [9]. These approaches typically require pointer analyses to identify access to mutable data structures, a problem which is not evident in Erlang -which features single assignment -and other functional languages. Working within parallel languages such as x10, which embodies the partitioned global address space (PGAS) model, some work has been done in loop parallelisation [23], and, while these are not included in the main release, there have been some experiments in parallelisation in the Fortran refactoring tool Photran [25]. Other work on loop parallelisation [16] notes the importance of user input into the parallelisation process."	1	"Dig""s paper [12] exemplifies the main approach for OO languages in targetting thread-safe libraries and data structures within a general-purpose language, which, once achieved, provides further refactoring opportunities. Alternatively, programs can be targeted at specialised hardware, such as GPUs  #CITATION_TAG and multicore systems [9]. These approaches typically require pointer analyses to identify access to mutable data structures, a problem which is not evident in Erlang -which features single assignment -and other functional languages. Working within parallel languages such as x10, which embodies the partitioned global address space (PGAS) model, some work has been done in loop parallelisation [23], and, while these are not included in the main release, there have been some experiments in parallelisation in the Fortran refactoring tool Photran [25]. Other work on loop parallelisation [16] notes the importance of user input into the parallelisation process."	l
CC2797	Adults with autism have high rates of unemployment. A survey by the National Autistic Society (NAS) in the United Kingdom reported that only 15% of adults of working age with autism are in full-time paid employment (Rosenblatt, 2008). Moreover, the majority of jobs held by adults with autism are unskilled and poorly paid ( #CITATION_TAG et al., 2013). Adults with autism are more likely to switch jobs frequently, have difficulty adjusting to new job settings and earn lower wages compared with typically developing peers (Howlin, 2000;Hurlbutt and Chalmers, 2004;Jennes-Coussens et al., 2006;M�_ller et al., 2003).	5	Adults with autism have high rates of unemployment. A survey by the National Autistic Society (NAS) in the United Kingdom reported that only 15% of adults of working age with autism are in full-time paid employment (Rosenblatt, 2008). Moreover, the majority of jobs held by adults with autism are unskilled and poorly paid ( #CITATION_TAG et al., 2013). Adults with autism are more likely to switch jobs frequently, have difficulty adjusting to new job settings and earn lower wages compared with typically developing peers (Howlin, 2000;Hurlbutt and Chalmers, 2004;Jennes-Coussens et al., 2006;M�_ller et al., 2003).	r
CC1219	Learning is a latent variable, typically measured as academic performance in assessment work and examinations (Mislevy, Behrens, & Dicerbo, 2012). Factors affecting academic performance have been the focus of research for many years (Farsides & Woodfield, 2003;Lent, Brown, & Hacket, 1994;Moran & Crowley, 1979). It remains an active research topic (Buckingham Shum & Deakin Crick, 2012;#CITATION_TAG, 2011;Komarraju, Ramsey & Rinella, 2013), indicating the inherent difficulty in both measurement of learning (Knight, Buckinham Shum, & Littleton, 2013;Tempelaar et al., 2013), and modelling the learning process, particularly in tertiary education (Pardos et al., 2011). Cognitive ability remains an important determinant of academic performance (Cassidy, 2011), often measured as prior academic ability. Demographic data, such as age and gender, have been cited as significant (Naderi et al., 2009), as are data gathered from learner activity on online learning systems (Bayer et al., 2012;L�_pez et al., 2012). In addition to the data systematically gathered by providers, other factors can be measured prior to commencing tertiary education, which could be useful in modelling learner academic performance. For example, models predicting academic performance that include factors of motivation (e.g., self-efficacy, goal setting) with cognitive ability yield a lower error variance than models of cognitive ability alone, particularly at tertiary level (reviewed in Boekaerts, 2001;Robbins et al., 2004). Research into personality traits, specifically the BIG 5 factors of openness, conscientiousness, extroversion, agreeableness, and neuroticism, and their impact on academic achievement in tertiary education, suggests some personality factors are indicative of potential academic achievement (Chamorro-Premuzic & Furnham, 2004De Feyter et al., 2012). For example conscientiousness, which is associated with persistence and self-discipline (Chamorro-Premuzic & Furnham, 2004), is correlated with academic performance, but not with IQ, suggesting conscientiousness may compensate for lower ability (Chamorro-Premuzic & Furnham, 2008). Openness, which is associated with curiosity, can be indicative of a deep learning style (Swanberg & Martinsen, 2010). Learning style (deep or shallow) and selfregulated learning strategies are also relevant, and have been shown to mediate between other factors (such as factors of personality and factors of motivation) and academic performance (Biggs et al., 2001;Entwhistle, 2005;Swanberg & Martinsen, 2010). This paper reviews a range of psychometric factors that could be used to predict academic performance in tertiary education (section 2). It lays emphasis on factors that can be measured prior to, or during learner enrolment in tertiary education programmes. The unique focus is to facilitate, and inform, early engagement with students potentially at risk of failing (e.g., Arnold & Pistilli, 2012;Laur�_a et al., 2013). Furthermore, results from learner profiling during student induction can provide useful feedback to the learner on preferred approaches to learning tasks, and development of a personalized learning environment. A review of pertinent data analysis techniques is presented in section 3, with an emphasis on empirical modelling approaches prevalent in educational data mining. Section 4 outlines the benefits of greater collaboration between educational psychology and learning analytics.	1	Learning is a latent variable, typically measured as academic performance in assessment work and examinations (Mislevy, Behrens, & Dicerbo, 2012). Factors affecting academic performance have been the focus of research for many years (Farsides & Woodfield, 2003;Lent, Brown, & Hacket, 1994;Moran & Crowley, 1979). It remains an active research topic (Buckingham Shum & Deakin Crick, 2012;#CITATION_TAG, 2011;Komarraju, Ramsey & Rinella, 2013), indicating the inherent difficulty in both measurement of learning (Knight, Buckinham Shum, & Littleton, 2013;Tempelaar et al., 2013), and modelling the learning process, particularly in tertiary education (Pardos et al., 2011). Cognitive ability remains an important determinant of academic performance (Cassidy, 2011), often measured as prior academic ability. Demographic data, such as age and gender, have been cited as significant (Naderi et al., 2009), as are data gathered from learner activity on online learning systems (Bayer et al., 2012;L�_pez et al., 2012). In addition to the data systematically gathered by providers, other factors can be measured prior to commencing tertiary education, which could be useful in modelling learner academic performance.	 
CC1127	"McVay and Kane""s conceptualization (e.g., Kane and McVay, 2012;#CITATION_TAG and Kane, 2012b), on the other hand, features proactive executive control that keeps people focused on their goal pursuits, from which mind-wandering distracts. Indeed, mindwandering detracts from performance of many ongoing tasks (Schooler et al., 2011). In an extensive individual differences investigation employing structural equation modeling, TUTs mediated effects of executive attention and working memory capacity on reading comprehension with a coefficient of __�0.44 (McVay and Kane, 2012a). Executive control is anticorrelated with the defaultmode network (e.g., Buckner et al., 2008); mind-wandering thus represents a transient ""failure"" of the executive control network rather than a potentially adaptive switch to another network. Smallwood (2010Smallwood ( , 2013a questions this executive-failure view on a number of grounds. First, the content of mind-wandering is internally organized, may be persistent, and hence probably also requires support from an executive control system that supports both attention to a task and the integrity of thought trains that have wandered away from it. Second, mind-wandering interferes with processing of both task-relevant and task-irrelevant cues, suggesting a briefly stable perceptual decoupling that redirects attentional resources to the ongoing stream of thought and is to that extent impervious to distraction from external stimuli. Third, in that individual differences study of the relationship between mind-wandering and reading comprehension (McVay and Kane, 2012b), measures of trait attention control and working memory capacity predicted performance, but after controlling for this, TUTs still accounted for an additional 8% of the variance in comprehension errors, thus suggesting that mind-wandering affects task performance beyond the role of (executive) attention control, presumably because of perceptual decoupling that accompanies mind-wandering (Smallwood, 2013a)."	0	"McVay and Kane""s conceptualization (e.g., Kane and McVay, 2012;#CITATION_TAG and Kane, 2012b), on the other hand, features proactive executive control that keeps people focused on their goal pursuits, from which mind-wandering distracts. Indeed, mindwandering detracts from performance of many ongoing tasks (Schooler et al., 2011). In an extensive individual differences investigation employing structural equation modeling, TUTs mediated effects of executive attention and working memory capacity on reading comprehension with a coefficient of __�0.44 (McVay and Kane, 2012a). Executive control is anticorrelated with the defaultmode network (e.g., Buckner et al., 2008); mind-wandering thus represents a transient ""failure"" of the executive control network rather than a potentially adaptive switch to another network."	M
CC1987	"The term ""effort which received an early definition by Fenichel [12] as ""a set of search variables [including] e.g. number of commands and descriptors [and] connect time is quite often considered in the more general literature on information seeking behavior, with this or a variety of other, more or less similar definitions. Zippf\""s ""law of least effort"" is often invoked to explain users\"" choice of information channel [13] , which refers to a number of studies who take this perspective. When effort is considered in the more restricted environment of information search behavior, however, it is often relatively vaguely defined. Typically, it is treated as in [14], where, in an investigation of the influence of user experience on search outcomes, effort is considered as one of several ""search language use patterns"" and defined to consist of ""mean number of cycles per topic, mean command frequency per topic, and mean number of documents visited per cycle"" without any motivation for this choice of parameters. A number of authors invoke ""cognitive effort"" as distinct from observable, logged actions in their characterization of search [15]. Cognitive effort is a concept well known from fields such as psychology and decision theory, but as a parameter of search effort it is often treated with a similar lack of specific definition as the concept of effort in general. Where it is defined the measurement definitions range widely, from ""pupil dilation"" in an eye-tracking study of search and evaluation behavior [16] to ""number of iterations, i.e. queries in a search"" [17]. The term transition, or parallel expressions such as shifts, state changes etc. is widely used in both the general literature on information seeking and more specifically in studies of information search behavior. It is generally defined in terms of a move from one state to another (or a sequence of such moves). Stages or patterns of stages appear in more and more fine-grained form in models of information seeking behavior from Ellis"" and others"" early models [18,19], and are becoming more and more fine-grained, as in Xie [20], where the interest is in shifting patterns between search stages. Such stages may be identified for instance in information seeking mediation, as in #CITATION_TAG where stages are identified as sets of cognitive and operational elements and transitions between stages are identified through vocabulary changes in dialogue. Transitions have been of particular interest to studies of search system interactions, where it has been thought that being able to detect transitions or distinct shifts in interaction would enable the automatic detection of patterns that might engender some kind of machine assistance or inform interface design. Variants of Markov modeling have often been suggested for such modeling, in [22] weaknesses of this approach is discussed, and an alternative modeling approach with Petri nets are suggested. In this paper and many others the transitions themselves are vaguely defined, and this is a persistent problem in the literature."	1	"The term transition, or parallel expressions such as shifts, state changes etc. is widely used in both the general literature on information seeking and more specifically in studies of information search behavior. It is generally defined in terms of a move from one state to another (or a sequence of such moves). Stages or patterns of stages appear in more and more fine-grained form in models of information seeking behavior from Ellis"" and others"" early models [18,19], and are becoming more and more fine-grained, as in Xie [20], where the interest is in shifting patterns between search stages. Such stages may be identified for instance in information seeking mediation, as in #CITATION_TAG where stages are identified as sets of cognitive and operational elements and transitions between stages are identified through vocabulary changes in dialogue. Transitions have been of particular interest to studies of search system interactions, where it has been thought that being able to detect transitions or distinct shifts in interaction would enable the automatic detection of patterns that might engender some kind of machine assistance or inform interface design. Variants of Markov modeling have often been suggested for such modeling, in [22] weaknesses of this approach is discussed, and an alternative modeling approach with Petri nets are suggested. In this paper and many others the transitions themselves are vaguely defined, and this is a persistent problem in the literature."	s
CC322	"1. The first counterargument explores the concept of Operating System (OS) (von Neumann 1958). Because the machine is separated from the program, a subset of the program must be devoted to the interaction with the machine and its ""users"" (in the most general sense) (Danchin 2009a). If a particular routine is meant to reproduce the machine, then a subset of the program must be somehow linked to the architecture of the machine. Analysis of the genes giving bacteria their shape showed that there is indeed an unexpected coincidence between gene clustering in genomes and shape of bacteria (Tamames et al. 2001). In multicellular organisms, the distribution of control genes, the homeogenes, parallels the body plan: changing the order of some homeogenes in the chromosomes changed the shape of the organism, putting organs in the place of others (Gaunt 1991). Rather than an objection, the existence of a correlation between the organisation of the program and the architecture of the organism fits a prediction of the model. 2. The second counterargument is that the program is carried by some material structure, bringing about contextual information. However, this is true in computers as well: the material support of the program has its saying in permitting the machine to run properly. Different machines may be driven by the same program on different supports. Thus, even the cloning experiment, which does not involve naked DNA but a whole nucleus, with its envelope, its proteins and its RNAs, is not different from a material support of a program in a computer. Indeed, nocturnal animals use chromatin in the nuclei of neurons using the retina in an extraordinary way. Their retina can detect one unique photon. Yet, the photon receptors are located behind neurons, which absorb or diffuse photons rather than preciously conserve them. When light is dimmed, the chromatin changes transcription and reorganises in such a way that its material behaves as a lens, focusing photons on receptors located behind the neurons (#CITATION_TAG et al. 2009)! This novel function for DNA, which has nothing to do with its role in carrying the genetic program, shows that another type of information has to be taken into account. In the same way, in many computers the support of the OS belongs the casing part of the chassis. 3. A third counterargument is that many rules prescribe the organisation of the cell soma, reflecting a large amount of information unrelated to the information in the program. Quite true, but this is true again for computers as well. The design of the interfaces, the microprocessors and the energy supply of the machine require much information."	4	Indeed, nocturnal animals use chromatin in the nuclei of neurons using the retina in an extraordinary way. Their retina can detect one unique photon. Yet, the photon receptors are located behind neurons, which absorb or diffuse photons rather than preciously conserve them. When light is dimmed, the chromatin changes transcription and reorganises in such a way that its material behaves as a lens, focusing photons on receptors located behind the neurons (#CITATION_TAG et al. 2009)! This novel function for DNA, which has nothing to do with its role in carrying the genetic program, shows that another type of information has to be taken into account. In the same way, in many computers the support of the OS belongs the casing part of the chassis. 3.	i
CC2923	"In line with the global patient safety agenda, an increasing number of healthcare institutions have adopted structured investigation processes as a way of learning from clinical incidents. This approach to organisational learning is based on the assumption that through determining the underlying causes of adverse events and drawing the necessary lessons, it is possible to prevent their reoccurrence (Donaldson, 2000). A common way to investigate clinical incidents is through Root Cause Analysis (RCA), a methodology combining elements from engineering, psychology, and the ""human factors"" tradition (Reason, 1990;#CITATION_TAG, Taylor-Adams, & Stanhope, 1998). As indicated by its name, RCA directs analytical attention to the root or latent factors that condition, enable or exacerbate clinical risk with the aim of producing recommendations on how these underlying causes should be managed or eradicated (Carroll, Rudolph, & Hatakenaka, 2002)."	0	"In line with the global patient safety agenda, an increasing number of healthcare institutions have adopted structured investigation processes as a way of learning from clinical incidents. This approach to organisational learning is based on the assumption that through determining the underlying causes of adverse events and drawing the necessary lessons, it is possible to prevent their reoccurrence (Donaldson, 2000). A common way to investigate clinical incidents is through Root Cause Analysis (RCA), a methodology combining elements from engineering, psychology, and the ""human factors"" tradition (Reason, 1990;#CITATION_TAG, Taylor-Adams, & Stanhope, 1998). As indicated by its name, RCA directs analytical attention to the root or latent factors that condition, enable or exacerbate clinical risk with the aim of producing recommendations on how these underlying causes should be managed or eradicated (Carroll, Rudolph, & Hatakenaka, 2002)."	c
CC1407	Better understanding of the intersection of HIV, aging and health is an urgent issue due to the increasing number of people aging with HIV [1,2] as the synergistic result of two concurrent phenomenon: the increased life expectancy of people with HIV undergoing HAART, extensively demonstrated both in high [#CITATION_TAG,4] and middle-and low-income countries [5,6], but also the increasing number of people seroconverting HIV at an older age, as the result of a lower perception of sexual risk in older people [7,8].	0	Better understanding of the intersection of HIV, aging and health is an urgent issue due to the increasing number of people aging with HIV [1,2] as the synergistic result of two concurrent phenomenon: the increased life expectancy of people with HIV undergoing HAART, extensively demonstrated both in high [#CITATION_TAG,4] and middle-and low-income countries [5,6], but also the increasing number of people seroconverting HIV at an older age, as the result of a lower perception of sexual risk in older people [7,8].	B
CC1563	"Direct impacts of the relationship from a buyer""s perspective are, however, more diverse and early in its value chain: a buyer attempts to save costs in lower prices for goods and services, but also in the purchasing and supply process (Cousins & Spekman, 2003;. The buyer values flexibility and reliability of supply and needs to secure and improve quality levels of the supplied goods and services. Other direct functions include innovation and technology transfer as well as process innovation to shorten timeto-market and cycle times (Cousins & Spekman, 2003;#CITATION_TAG et al., 2010). These benefits are dispersed in the value chain of the buying firm such as in purchasing, inbound logistics, manufacturing and assembly, or research and development. Relationship value is therefore not as easily and unambiguously perceived by a buyer as by a seller. If the goals associated with a buyer-seller relationship from a buyer""s perspective are fulfilled, the relationship delivers high relationship value and contributes to the buying company""s economic success, but indirectly."	1	"Direct impacts of the relationship from a buyer""s perspective are, however, more diverse and early in its value chain: a buyer attempts to save costs in lower prices for goods and services, but also in the purchasing and supply process (Cousins & Spekman, 2003;. The buyer values flexibility and reliability of supply and needs to secure and improve quality levels of the supplied goods and services. Other direct functions include innovation and technology transfer as well as process innovation to shorten timeto-market and cycle times (Cousins & Spekman, 2003;#CITATION_TAG et al., 2010). These benefits are dispersed in the value chain of the buying firm such as in purchasing, inbound logistics, manufacturing and assembly, or research and development. Relationship value is therefore not as easily and unambiguously perceived by a buyer as by a seller. If the goals associated with a buyer-seller relationship from a buyer""s perspective are fulfilled, the relationship delivers high relationship value and contributes to the buying company""s economic success, but indirectly."	h
CC397	Some authors classify carpool trips on the basis of the types of matching between origins and destinations (Morency, 2007;#CITATION_TAG et al., 1999). The most simple carpool structure encompasses that both driver and rider(s) have their origin and destination in common. If origins and/or destinations are not the same, more complex structures appear. Furthermore, carpool members can meet at a carpool parking at an intermediate location. All this makes of carpooling a complex and hybrid concept. Accordingly, it is no coincidence that the two data sources used in this paper employ a different definition of carpooling.	5	Some authors classify carpool trips on the basis of the types of matching between origins and destinations (Morency, 2007;#CITATION_TAG et al., 1999). The most simple carpool structure encompasses that both driver and rider(s) have their origin and destination in common. If origins and/or destinations are not the same, more complex structures appear. Furthermore, carpool members can meet at a carpool parking at an intermediate location.	S
CC1434	In Section 3, I mentioned that philosophy majors outperform most other majors on the LSAT and the GRE. In Section 4, I mentioned that philosophical training is positively correlated with performance on the CRT (Livengood et al 2010). This might be unsurprising given that reasoners with higher levels of analytic intelligence-perhaps those who have enjoyed philosophical training and selection-will tend to respond reflectively on various reasoning tasks (McCelroy andSeta 2003, #CITATION_TAG andWest 2000). Naturally, then we should expect that having or being a candidate for a PhD in philosophy will positively correlate with answering reflectively on the CRT.	4	In Section 3, I mentioned that philosophy majors outperform most other majors on the LSAT and the GRE. In Section 4, I mentioned that philosophical training is positively correlated with performance on the CRT (Livengood et al 2010). This might be unsurprising given that reasoners with higher levels of analytic intelligence-perhaps those who have enjoyed philosophical training and selection-will tend to respond reflectively on various reasoning tasks (McCelroy andSeta 2003, #CITATION_TAG andWest 2000). Naturally, then we should expect that having or being a candidate for a PhD in philosophy will positively correlate with answering reflectively on the CRT.	i
CC2146	The simplest choice of a tensor model is to consider one which has a tensor with three indices as its only dynamical variable. Then, by identifying the rank-three tensor with the structure constant of an algebra charactering a fuzzy space, the tensor model can be interpreted as theory of a dynamical fuzzy space. Since one can in principle choose the values of the rank-three tensor to construct fuzzy spaces corresponding to any dimensional spaces, the rank-three tensor models can equally treat spaces in general dimensions. This idea has first been presented in Ref. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19][20][21][22][23][24][25][26] . The purpose of the present paper is to provide a full treatment of the original incomplete presentation of the idea, and to pursue the algebraic description of the tensor models. In the sequel, it is found that 3-ary algebras #CITATION_TAG[28][29] describe the symmetries of the tensor models. 3-ary algebras have been introduced in physics by Nambu , and have recently been widely discussed in the context of M-theory [31][32][33] . This unexpected common appearance of 3-ary algebras suggests the general Tensor models and 3-ary algebras importance of this new way of describing symmetry in the physics of quantum spacetime. This paper is organized as follows. In the following section, the rank-three tensor model is presented. In Section III, the structure of the algebras corresponding to the rank-three tensor models is discussed. In Section IV, the commutative case of the algebras is discussed.	0	Since one can in principle choose the values of the rank-three tensor to construct fuzzy spaces corresponding to any dimensional spaces, the rank-three tensor models can equally treat spaces in general dimensions. This idea has first been presented in Ref. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19][20][21][22][23][24][25][26] . The purpose of the present paper is to provide a full treatment of the original incomplete presentation of the idea, and to pursue the algebraic description of the tensor models. In the sequel, it is found that 3-ary algebras #CITATION_TAG[28][29] describe the symmetries of the tensor models. 3-ary algebras have been introduced in physics by Nambu , and have recently been widely discussed in the context of M-theory [31][32][33] This unexpected common appearance of 3-ary algebras suggests the general Tensor models and 3-ary algebras importance of this new way of describing symmetry in the physics of quantum spacetime. This paper is organized as follows.	e
CC2191	Authors have described how practitioners face challenges in successfully using these devices with clients. Campbell et al. [2] for example highlighted that practitioners are unsure of when and how to implement AAC systems due to a paucity of research evidence. Schlosser #CITATION_TAG described how practitioners faced a difficult task when matching appropriate systems to individuals with disabilities. It has also been suggested [4] that practitioners and users may have limited access to available systems or services due to funding issues and limited specialist knowledge.	1	Authors have described how practitioners face challenges in successfully using these devices with clients. Campbell et al. [2] for example highlighted that practitioners are unsure of when and how to implement AAC systems due to a paucity of research evidence. Schlosser #CITATION_TAG described how practitioners faced a difficult task when matching appropriate systems to individuals with disabilities. It has also been suggested [4] that practitioners and users may have limited access to available systems or services due to funding issues and limited specialist knowledge.	h
CC772	Very recently, a few attempts have been made to reconcile theory with real world practice. Armstrong and Wright (2009) , Jullien, Rey and Sand-Zantman (2009) , and #CITATION_TAG, Inderst and Valletti (2009) 15 have in common that they introduce additional realistic features of the telecommunication industry into the Laffont, Rey and Tirole (1998b) framework and then show that for some parameter range joint profits are maximized at termination charges above cost. Moreover, these papers conclude that the need to regulate termination charges is reduced since the socially optimal termination charge would also be above cost.	1	Very recently, a few attempts have been made to reconcile theory with real world practice. Armstrong and Wright (2009) , Jullien, Rey and Sand-Zantman (2009) , and #CITATION_TAG, Inderst and Valletti (2009) 15 have in common that they introduce additional realistic features of the telecommunication industry into the Laffont, Rey and Tirole (1998b) framework and then show that for some parameter range joint profits are maximized at termination charges above cost. Moreover, these papers conclude that the need to regulate termination charges is reduced since the socially optimal termination charge would also be above cost.	r
CC1242	Theories of temperament focus on aspects of personality discernible at birth (#CITATION_TAG, 2006;John et al., 2008). Historically, research that links temperament with academic achievement has lacked a welldefined referential framework for the interactions between temperament and academic performance. Studies have varied in their perspective of personality, with diverse views on the relevant traits to be considered as measures of temperament, such as factors of persistence, factors relating to motivation and/or moral factors such as honesty (de Raad & Schouwenburg, 1996). While many factors are associated with temperament, factor analysis by a number of researchers, working independently and using different approaches, has resulted in broad agreement of five main personality dimensions (Ackerman & Heggestad, 1997;Boeree, 2006;John et al., 2008). These are commonly referred to as the Big Five (Cattell & Mead, 2008;Goldberg, 1992Goldberg, , 1993Tupes & Cristal, 1961) or the related Five-Factor Model (Costa & McCrae, 1992). The five factors -openness, agreeableness, extroversion, conscientiousness, and neuroticism -are described in Table 3. 1 High School Grade Point Average (HSGPA) is a secondary school, end-of-year, aggregate measure of academic performance, which can be a combination of continuous assessment results and end of term exams. 2 ACT tests are based on high school curriculum in English, Mathematics, Reading, and Science (www.act.org).	0	Theories of temperament focus on aspects of personality discernible at birth (#CITATION_TAG, 2006;John et al., 2008). Historically, research that links temperament with academic achievement has lacked a welldefined referential framework for the interactions between temperament and academic performance. Studies have varied in their perspective of personality, with diverse views on the relevant traits to be considered as measures of temperament, such as factors of persistence, factors relating to motivation and/or moral factors such as honesty (de Raad & Schouwenburg, 1996). While many factors are associated with temperament, factor analysis by a number of researchers, working independently and using different approaches, has resulted in broad agreement of five main personality dimensions (Ackerman & Heggestad, 1997;Boeree, 2006;John et al., 2008).	T
CC2739	"Whether or not change is perceived similarly by different actors in a specific network is of key interest, as differences or similarities in any firm""s perspective can be used to understand strategic behaviour (Bogner & Thomas, 1993;Osborne et al., 2001). However, comparing the perceptions of multiple actors, specifically the time and space specificity of change, remains an under-researched area (#CITATION_TAG, Gadde, HՂkansson, Snehota, & Waluszewski, 2008). Further, comparing different actors"" ascriptions of the explanatory mechanisms is important because companies adapt to perceived changes through interactions based on their understanding or anticipation of the changing environment (Gronhaug & Falkenberg, 1989;Reger & Palmer, 1996)."	2	"Whether or not change is perceived similarly by different actors in a specific network is of key interest, as differences or similarities in any firm""s perspective can be used to understand strategic behaviour (Bogner & Thomas, 1993;Osborne et al., 2001). However, comparing the perceptions of multiple actors, specifically the time and space specificity of change, remains an under-researched area (#CITATION_TAG, Gadde, HՂkansson, Snehota, & Waluszewski, 2008). Further, comparing different actors"" ascriptions of the explanatory mechanisms is important because companies adapt to perceived changes through interactions based on their understanding or anticipation of the changing environment (Gronhaug & Falkenberg, 1989;Reger & Palmer, 1996)."	o
CC2770	Recent research has stressed the importance of network structures in understanding business exchanges (#CITATION_TAG, 1997;M�_ller & Rajala, 2007). These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;Porter, 1985) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980). Business networks have also been characterised in strategic marketing as value-creating systems (Parolini, 1999) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (M�_ller & Svahn, 2006;Normann & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998;Gulati, Nohria, & Zaheer, 2000).	0	Recent research has stressed the importance of network structures in understanding business exchanges (#CITATION_TAG, 1997;M�_ller & Rajala, 2007). These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;Porter, 1985) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980).	R
CC1146	"There is one more kind of association involving emotion. There is evidence that inducing negative moods increases mindwandering, perhaps because it potentiates personal concerns (Smallwood et al., 2009a). When internal cues are associated with threat, they may set up a repetitive sequence of ruminative thoughts. (For a full review of repetitive thought, see #CITATION_TAG, 2008.) The findings described above provide a ready explanation for ruminative sequences. For example, an individual is concerned about one or more important goals, such as keeping a relationship or a job, and feels anxious about the many details involved in these, such as the many things necessary for keeping the other person or the boss happy while meeting one""s own needs and desires."	0	"There is one more kind of association involving emotion. There is evidence that inducing negative moods increases mindwandering, perhaps because it potentiates personal concerns (Smallwood et al., 2009a). When internal cues are associated with threat, they may set up a repetitive sequence of ruminative thoughts. (For a full review of repetitive thought, see #CITATION_TAG, 2008. ) The findings described above provide a ready explanation for ruminative sequences. For example, an individual is concerned about one or more important goals, such as keeping a relationship or a job, and feels anxious about the many details involved in these, such as the many things necessary for keeping the other person or the boss happy while meeting one""s own needs and desires."	r
CC1656	"Climate damages are typically estimated with stylised integrated assessment models (IAMs), which take into account contributions to climate policy from vari-This criticism is made in Weitzman (2010), with reference to results from Nordhaus"" DICE model. www.economics-ejournal.org 1 conomics: The Open-Access, Open-Assessment E-Journal ous disciplines, from climatology to economics. These model the most significant interactions and feedback mechanisms of the human-climate system. They also deal with intergenerational fairness, income regional distribution and, some of them, at least to a certain extent, risk and uncertainty management (#CITATION_TAG et al. 2007)."	0	www. economics-ejournal.org 1 conomics: The Open-Access, Open-Assessment E-Journal ous disciplines, from climatology to economics. These model the most significant interactions and feedback mechanisms of the human-climate system. They also deal with intergenerational fairness, income regional distribution and, some of them, at least to a certain extent, risk and uncertainty management (#CITATION_TAG et al. 2007).	 
CC1754	The so-called tritronquͩe solution to P-I was discovered by #CITATION_TAG (1913) as the unique solution having no poles in the sector | arg �_ | < 4��/5 for sufficiently large |�_ |. Remarkably, the very same solution 3 arises in the critical behavior of solutions to focusing NLS.	0	The so-called tritronquͩe solution to P-I was discovered by #CITATION_TAG (1913) as the unique solution having no poles in the sector | arg �_ | < 4��/5 for sufficiently large |�_ |. Remarkably, the very same solution 3 arises in the critical behavior of solutions to focusing NLS.	T
CC112	"With the prevalence of chronic illness rising worldwide, there is a need to engage patients in health promotion work in order to prevent further deterioration, to strengthen their health and their capacity to participate in society [1] [2]. Interventions based on such work will reflect the philosophical perspective of ""health within illness which holds that individuals living with long-term health problems are capable of experiencing health and wellbeing despite their conditions [3] [4]. Summaries of research concerning people with various long-term conditions show that they have much in common as they face the challenges of trying to live as well as possible within the context of physical, mental, or social discomfort and limitation [5]- [7]. However, patient education and wellness-interventions in the context of chronic illness are often specific to particular diagnostic groups and not designed to be applied across diagnostic categories [8] [9]. Two of the few examples of interventions that are practiced more broadly are the Chronic Disease Self-Management Education (CDSME) developed by Lorig and colleagues in the USA #CITATION_TAG and the Vitality Training Program (VTP) developed by Steen and Haugli [11] in Norway. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [12] [13]. Improved health behavior and health status were also reported in a group of patients with serious mental illness [14]. However, a longitudinal randomized trial of stroke survivors who accomplished CDSME, showed that the intervention did not appear to impact self-efficacy and failed to influence outcomes such as mood or social outcomes [15]. This was also confirmed in a Cochrane review that focused on the outcomes of CDSME [16]."	0	"Interventions based on such work will reflect the philosophical perspective of ""health within illness which holds that individuals living with long-term health problems are capable of experiencing health and wellbeing despite their conditions [3] [4]. Summaries of research concerning people with various long-term conditions show that they have much in common as they face the challenges of trying to live as well as possible within the context of physical, mental, or social discomfort and limitation [5]- [7]. However, patient education and wellness-interventions in the context of chronic illness are often specific to particular diagnostic groups and not designed to be applied across diagnostic categories [8] [9]. Two of the few examples of interventions that are practiced more broadly are the Chronic Disease Self-Management Education (CDSME) developed by Lorig and colleagues in the USA #CITATION_TAG and the Vitality Training Program (VTP) developed by Steen and Haugli [11] in Norway. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [12] [13]. Improved health behavior and health status were also reported in a group of patients with serious mental illness [14]. However, a longitudinal randomized trial of stroke survivors who accomplished CDSME, showed that the intervention did not appear to impact self-efficacy and failed to influence outcomes such as mood or social outcomes [15]."	o
CC1698	"Thirdly, practices of residual use of human tissue and data procured in the context of healthcare often do not directly fall under the remit of most clinical research legislation. Historically, human tissue and data were often regarded as a kind of waste which could be regarded as an impersonal good (Tupasela 2011). Even where personal rights in such resources were involved, current privacy legislation often contains provisos for so-called research exemptions. In this way, a distinction between research and care is upheld by depersonalizing the use of residual tissue and data in research and processing such resources only in aggregate form. As discussed above, such ways of drawing boundaries between research and care no longer apply in clinical biobanking. The boundaries are blurred by design, undercutting any sharp division between data for research and data for care. One area in which this blurring plays up clearly is in current debates over how to deal with the feedback of incidental findings. Many ethicists and legal scholars have argued that researchers and biobanks have duties and responsibilities towards participants and donors with regards to incidental findings generated from banked tissue and data. For instance, Wolf and others consider that ""findings that are analytically valid, reveal an established and substantial risk of a serious health condition, and are clinically actionable should generally be offered to consenting contributors"" (#CITATION_TAG et al. 2012). However, it is often unclear on whom this responsibility specifically falls, and this may require altering conventional roles and duties of researchers. This could extend researchers"" medical responsibilities and would consequently also raise further governance challenges concerning the delineation of their role and remit in research and care. Even the question whether findings should still be considered ""incidental"" given the systematic exploration of data and tissue will come up for debate. Irrespective of if most genomic variants may currently by-and-large seem of unclear significance, such findings are likely to be commonplace in some clinical settings (i.e. genetic diagnostics) and will eventually become more commonplace as similar analytical techniques are adopted in other clinical areas as well. Moreover, once personal tissue and data collected in care settings are processed for open-ended purposes over indeterminate time frames, research data may become a source of data with potential clinical significance as well. Once healthcare practices are modified to accommodate the provision of clinical data for research purposes, qualitative distinctions between clinical and research data are less likely to form a barrier to such feedback."	0	"The boundaries are blurred by design, undercutting any sharp division between data for research and data for care. One area in which this blurring plays up clearly is in current debates over how to deal with the feedback of incidental findings. Many ethicists and legal scholars have argued that researchers and biobanks have duties and responsibilities towards participants and donors with regards to incidental findings generated from banked tissue and data. For instance, Wolf and others consider that ""findings that are analytically valid, reveal an established and substantial risk of a serious health condition, and are clinically actionable should generally be offered to consenting contributors"" (#CITATION_TAG et al. 2012). However, it is often unclear on whom this responsibility specifically falls, and this may require altering conventional roles and duties of researchers. This could extend researchers"" medical responsibilities and would consequently also raise further governance challenges concerning the delineation of their role and remit in research and care. Even the question whether findings should still be considered ""incidental"" given the systematic exploration of data and tissue will come up for debate."	a
CC2668	The relationship between democracy and growth has received much attention in the recent literature. Cross-country studies on the impact of democratic institutions on growth yield ambiguous and inconclusive results (Barro, 1997;Glaeser et al. 2004). Even studies exploiting the within country variation in the data still show that transitions towards democracy are not necessarily associated with large improvements in economic outcomes (#CITATION_TAG and Wacziarg, 2005;Giavazzi and Tabellini, 2005;Persson andTabellini, 2006, 2008). Furthermore, the direction of causation is hard to establish (see Acemoglu et al., 2008;Gundlach and Paldam, 2009). Crucial questions in this debate of course are about the mechanism of how political institutions affect economic growth. In this respect, government policies should play a key role. Political institutions affect (economic) policy making by shaping the rules of the game and determine the context in which key policy decisions are made, such as redistribution of income and the provision of public goods (Persson and Tabellini, 2003).	5	The relationship between democracy and growth has received much attention in the recent literature. Cross-country studies on the impact of democratic institutions on growth yield ambiguous and inconclusive results (Barro, 1997;Glaeser et al. 2004). Even studies exploiting the within country variation in the data still show that transitions towards democracy are not necessarily associated with large improvements in economic outcomes (#CITATION_TAG and Wacziarg, 2005;Giavazzi and Tabellini, 2005;Persson andTabellini, 2006, 2008). Furthermore, the direction of causation is hard to establish (see Acemoglu et al., 2008;Gundlach and Paldam, 2009). Crucial questions in this debate of course are about the mechanism of how political institutions affect economic growth. In this respect, government policies should play a key role.	e
CC2535	"We have previously developed a computational model, or simulation, of pre-natal lymphoid tissue formation to help direct and understand results of laboratory experimentation [14,#CITATION_TAG]. Through the use of a cell culture system, the behaviour of cells was tracked for a period of 1 h, after which key cell behaviour responses were calculated. These responses revealed that there is a statistically significant change in cell behaviour when in the vicinity of developing lymphoid tissue [14]: the reasons for which are currently unclear. Our computational model adopts an agent-based approach, allowing exploration of how system dynamics, in this case changes in cell behaviour responses, might emerge from interactions between cells and their environment [15,16]. For the full detail of our implementation, we direct the reader to our previous descriptions of the biological information captured in the model, and the manner by which this has been translated into a specification that can be encoded as a computer program and simulation platform [14,15]. As a brief overview for the purposes of this paper, three cell populations are known to be involved in lymphoid tissue development, counts of which have been calculated from flow cytometry experiments. Other attributes, such as cell speed, have been determined from either laboratory experiments or from the literature. Our model captures each of these cells: each of which possesses individual attributes and state. Transitions between these states are described in a set of rules, described in detail in unified modelling language (UML) state diagrams [15]. With each simulation time-step, cell behaviour is simulated dependent on the current state of the cell and the cell""s location. The environment, in this case the developing gastrointestinal tract, is modelled as a continuous space, with dimensions set that are representative of measurements taken from stereomicroscopy images. Simulated adhesion and chemokine diffusion pathways influence the behaviour of these cells, causing the emergence of aggregations of cells within the simulated environment: aggregations that become lymphoid organs. Our previously published studies demonstrate that we could use simulation to reproduce emergent cell behaviour that is statistically similar to that observed in ex vivo culture, thus providing us with a strong baseline behaviour from which we can use the tool to explore the mechanisms underlying tissue development [14,15]. Through careful statistical analysis of simulation behaviour, including sensitivity analyses [13], we were able to identify key pathways in the simulated model and suggest how the existence of such pathways in the biological system could be investigated in the laboratory. However, as for any model, the simulation and its results are heavily influenced by the implementation decisions taken when our simulator was developed."	1	We have previously developed a computational model, or simulation, of pre-natal lymphoid tissue formation to help direct and understand results of laboratory experimentation [14,#CITATION_TAG]. Through the use of a cell culture system, the behaviour of cells was tracked for a period of 1 h, after which key cell behaviour responses were calculated. These responses revealed that there is a statistically significant change in cell behaviour when in the vicinity of developing lymphoid tissue [14]: the reasons for which are currently unclear. Our computational model adopts an agent-based approach, allowing exploration of how system dynamics, in this case changes in cell behaviour responses, might emerge from interactions between cells and their environment [15,16].	W
CC2692	A puzzling result of our study is the asymmetric effect of regime change on the level of protection: why should a regime change be relevant only for a transition to democracy, and not vice versa ? One possible explanation of such asymmetric effect of transitions to democracy and autocracy could be based on theories explaining (lack of) leadership turnovers and economic performance under autocracies (see Besley and Kudamatsu, 2008;#CITATION_TAG et al. 2004). These studies emphasise the importance of some institutional features of autocracy, in particular political stability. The objective of staying in power could stop a potential policy reversal against the agrarian population by preventing rural unrest. Our data preclude a deeper investigation of this issue but further analysis would certainly constitute a fruitful line of inquiry.	0	A puzzling result of our study is the asymmetric effect of regime change on the level of protection: why should a regime change be relevant only for a transition to democracy, and not vice versa ? One possible explanation of such asymmetric effect of transitions to democracy and autocracy could be based on theories explaining (lack of) leadership turnovers and economic performance under autocracies (see Besley and Kudamatsu, 2008;#CITATION_TAG et al. 2004). These studies emphasise the importance of some institutional features of autocracy, in particular political stability. The objective of staying in power could stop a potential policy reversal against the agrarian population by preventing rural unrest. Our data preclude a deeper investigation of this issue but further analysis would certainly constitute a fruitful line of inquiry.	n
CC1525	"1.4. The retrieval of free and bound closed class items In Garrett""s original model of production (1982,1984) and other models since (e.g. Levelt et al., 1999; see also Gordon & Dell, 2003), open class items are activated by conceptual-semantic information and then inserted into syntactic frames built around grammatical requirements, e.g. phrasal and constituent ordering (e.g. in English, subject-verb-object, determiner before noun and so on), subcategorisation and tense. In contrast, grammatical morphemes (both free and bound) are activated indirectly, via open class items, and they are intrinsic to the syntactic frame (Bock & Levelt, 1994;Garrett, 1982;Garrett, 1984). It remains a topic of debate whether there is separate representation of closed class items as syntactic entities vs. open class, content words as semantic entities (e.g. Allen & Seidenberg, 1999;Altmann, Kempler, & Andersen, 2001;Bates & Wulfeck, 1989;Biassou, Obler, Nespoulous, Dordain, & Harris, 1997;Bradley & Garrett, 1983;Caramazza, Miozzo, Costa, Schiller, & Alario, 2001;Druks & Froud, 2002;LaPointe & Dell, 1989). The view that closed class items are especially related to syntactic processing is also considered in explorations of agrammatism, in which special problems with these items are cited, amongst other deficits (e.g. Bates & Wulfeck, 1989;Druks & Froud, 2002;Gordon & Dell, 2003;Mesulam, Grossman, Hillis, Kertesz, & Weintraub, 2003;Milman, Dickey, & Thompson, 2008). Closed class items have been divided into those with some semantic content like locative prepositions (up, down, above etc.) versus those that are ""purely"" syntactic, like determiners or auxiliaries, (Garrett, 1984;Levelt, 1989;Bock & Levelt, 1994). If primarily governed by syntactic properties, closed class elements may be selected competitively once grammatical features are specified by an active lexical item; this would make their selection similar to open class elements that are competitively selected once semantic features are specified (Caramazza et al., 2001;Schiller & Caramazza, 2002;Schiller & Caramazza, 2003;Schiller & Caramazza, 2006;Schiller & Costa, 2006; see also LaPointe, 1985;LaPointe & Dell, 1989). In contrast, closed class elements may be chosen once competition amongst grammatical features is resolved: in other words, competitive selection would apply to grammatical feature specifications but not to closed class elements (#CITATION_TAG, Mak, Sander, & Willeboordse, 1998;Levelt et al., 1999;Schriefers, 1993;Schriefers, Jescheniak, & Hantsch, 2005). In both cases, the selection of closed class items depends on grammatical features activated from open class items. If closed class items are primarily syntactic, and syntax is preserved in SD, we do not expect errors on closed class items."	0	"The view that closed class items are especially related to syntactic processing is also considered in explorations of agrammatism, in which special problems with these items are cited, amongst other deficits (e.g. Bates & Wulfeck, 1989;Druks & Froud, 2002;Gordon & Dell, 2003;Mesulam, Grossman, Hillis, Kertesz, & Weintraub, 2003;Milman, Dickey, & Thompson, 2008). Closed class items have been divided into those with some semantic content like locative prepositions (up, down, above etc.) versus those that are ""purely"" syntactic, like determiners or auxiliaries, (Garrett, 1984;Levelt, 1989;Bock & Levelt, 1994). If primarily governed by syntactic properties, closed class elements may be selected competitively once grammatical features are specified by an active lexical item; this would make their selection similar to open class elements that are competitively selected once semantic features are specified (Caramazza et al., 2001;Schiller & Caramazza, 2002;Schiller & Caramazza, 2003;Schiller & Caramazza, 2006;Schiller & Costa, 2006; see also LaPointe, 1985;LaPointe & Dell, 1989). In contrast, closed class elements may be chosen once competition amongst grammatical features is resolved: in other words, competitive selection would apply to grammatical feature specifications but not to closed class elements (#CITATION_TAG, Mak, Sander, & Willeboordse, 1998;Levelt et al., 1999;Schriefers, 1993;Schriefers, Jescheniak, & Hantsch, 2005). In both cases, the selection of closed class items depends on grammatical features activated from open class items. If closed class items are primarily syntactic, and syntax is preserved in SD, we do not expect errors on closed class items."	r
CC331	"In summary, two types of information (coupling of a particular form-not simply shape-with matter, energy, space and time), information of the chassis (casing + metabolism) and information of the program are associated together in a cell (Tanaka 1984). A synthetic cell needs the association of a chassis developing metabolism (not a simple 3D casing) and a program similar to that found in computers. The conclusions of Dyson""s argument on the double origin of life, with reproducing metabolism predating replication are therefore a pre-requisite for synthesis of life (Dyson 1985). This dichotomy is visible in present synthetic biology, with a fairly clear separation between those who study the chassis (and are often also interested in the origin of life) (#CITATION_TAG et al. 2009;Shenhav and Lancet 2004) and those who think that life is essentially due to the genetic program, organising their activity around construction of program biobricks, or even as complete genomes (Gibson et al. 2008)."	1	"In summary, two types of information (coupling of a particular form-not simply shape-with matter, energy, space and time), information of the chassis (casing + metabolism) and information of the program are associated together in a cell (Tanaka 1984). A synthetic cell needs the association of a chassis developing metabolism (not a simple 3D casing) and a program similar to that found in computers. The conclusions of Dyson""s argument on the double origin of life, with reproducing metabolism predating replication are therefore a pre-requisite for synthesis of life (Dyson 1985). This dichotomy is visible in present synthetic biology, with a fairly clear separation between those who study the chassis (and are often also interested in the origin of life) (#CITATION_TAG et al. 2009;Shenhav and Lancet 2004) and those who think that life is essentially due to the genetic program, organising their activity around construction of program biobricks, or even as complete genomes (Gibson et al. 2008)."	s
CC2697	First, we present GRASP in general and the method of #CITATION_TAG et al	4	First, we present GRASP in general and the method of #CITATION_TAG et al	F
CC256	"Assessing the influence of external drivers (e.g., increased greenhouse gas concentrations in the atmosphere) on extreme weather is challenging because the most important events are typically rare, so their observed frequency is dominated by chance. In order to compile robust statistics of extreme weather events, large ensembles of model simulations at relatively high resolution are required. This project makes use of the large-ensemble capability provided by the on-going climateprediction.net weather@home"" volunteer computing network (Allen 1999; #CITATION_TAG), where members of the public are producing multi-thousand-member ensembles of weather simulations using regional climate models (RCMs) of different parts of the world. We have applied the model set-up described in Massey et al. (2012) with the regional climate model (RCM) embedded within a general circulation model (GCM). The increased resolution of the RCM results in a more realistic simulation of localised weather events, including high and low temperatures and extreme precipitation over a relatively small area (Jones et al. 2004). The standard ensemble described below is an initial condition ensemble hindcast experiment over Europe which simulates the historical period 1960-2010 including all observed forcings (AF)(we will only analyse the 1960s (AF1960s) and 2000s (AF2000s) in this study) as well as the decade from 2000 to 2010 with the anthropogenic climate change signal removed (NAT2000s) as described in Section 2.3. For the results used in this study, the RCMs run by volunteers are at 50 km resolution over Europe driven by a global atmospheric model. This is a relatively low resolution for an RCM but given that natural variability is the largest source of uncertainty (Section 2.4) the best methodology to account for this uncertainty (given resources are not unlimited) is to trade accuracy for precision and employ large ensembles of relatively low resolution. The models used are HadAM3P, an atmosphere only GCM at 1.25 �_ 1.875 degrees resolution, forced with observed SSTs from the HadISST data set (Rayner et al. 2003) and the RCM HadRM3P. Both models have been developed by the UK Met Office and are based upon the atmospheric component of HadCM3 (Pope et al. 2000;Gordon et al. 2000) with some improvements in the model physics described in Massey et al. (2012). Both models are run many hundreds of times with varied initial conditions. In this way, very large ensembles of RCM simulations can be computed, of the order of thousands, which in turn allows greater confidence when examining the statistics of rare events. We follow a similar methodology to Massey et al. (2012), which uses very large ensembles of general circulation models (GCMs) to assess the change in risk of very warm Novembers in central England under two different climate scenarios: observed July in the decade between 1960and 1970and observed July between 2000 In addition to that we analyse the same decade, 2000-2010, in a representation of a world that might have been without anthropogenic climate change."	0	"Assessing the influence of external drivers (e.g., increased greenhouse gas concentrations in the atmosphere) on extreme weather is challenging because the most important events are typically rare, so their observed frequency is dominated by chance. In order to compile robust statistics of extreme weather events, large ensembles of model simulations at relatively high resolution are required. This project makes use of the large-ensemble capability provided by the on-going climateprediction.net weather@home"" volunteer computing network (Allen 1999; #CITATION_TAG), where members of the public are producing multi-thousand-member ensembles of weather simulations using regional climate models (RCMs) of different parts of the world. We have applied the model set-up described in Massey et al. (2012) with the regional climate model (RCM) embedded within a general circulation model (GCM). The increased resolution of the RCM results in a more realistic simulation of localised weather events, including high and low temperatures and extreme precipitation over a relatively small area (Jones et al. 2004). The standard ensemble described below is an initial condition ensemble hindcast experiment over Europe which simulates the historical period 1960-2010 including all observed forcings (AF)(we will only analyse the 1960s (AF1960s) and 2000s (AF2000s) in this study) as well as the decade from 2000 to 2010 with the anthropogenic climate change signal removed (NAT2000s) as described in Section 2.3."	i
CC158	The earliest study identified was a 1977 US study investigating why smokers seeking treatment (psychotherapy) often fared no better than smokers who quit unassisted. [28] This was followed in the late 1980s and 1990s by three in-depth sociological studies (from the US and Sweden) investigating unassisted cessation as a phenomenon in its own right, [29,31,32] and one US sociological study in which unassisted cessation data were reported but this was not the primary focus of the study. [30] Subsequent to this, no qualitative studies were identified that focused on unassisted cessation per se: the six post-2000 studies (from Hong Kong, US, UK, Canada and Norway) had as their primary focus either cessation in general [33,,#CITATION_TAG,[36][37][38] or health behaviour change. [35] Research question 2: What are the views and experiences of smokers who quit unassisted?	4	The earliest study identified was a 1977 US study investigating why smokers seeking treatment (psychotherapy) often fared no better than smokers who quit unassisted. [28] This was followed in the late 1980s and 1990s by three in-depth sociological studies (from the US and Sweden) investigating unassisted cessation as a phenomenon in its own right, [29,31,32] and one US sociological study in which unassisted cessation data were reported but this was not the primary focus of the study. [30] Subsequent to this, no qualitative studies were identified that focused on unassisted cessation per se: the six post-2000 studies (from Hong Kong, US, UK, Canada and Norway) had as their primary focus either cessation in general [33,,#CITATION_TAG,[36][37][38] or health behaviour change. [35] Research question 2: What are the views and experiences of smokers who quit unassisted?	0
CC319	The key observation here is that uncertainty about the SCC is currently a great deal larger than uncertainty about the corresponding marginal abatement cost (MAC) of CO2. Using recent reviews of the literature, we concluded that the range of estimates of the present SCC was a factor of ten larger than the corresponding range of estimates of the present MAC (#CITATION_TAG and Fankhauser 2010).	5	The key observation here is that uncertainty about the SCC is currently a great deal larger than uncertainty about the corresponding marginal abatement cost (MAC) of CO2. Using recent reviews of the literature, we concluded that the range of estimates of the present SCC was a factor of ten larger than the corresponding range of estimates of the present MAC (#CITATION_TAG and Fankhauser 2010).	s
CC631	"Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children""s behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007;. For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). A result like this sheds light onto the generalization process: deciding how broadly a category should be applied depends on the timing of experience with exemplars (see #CITATION_TAG and Spalding, 1997;Samuelson and Horst, 2007, for similar findings)."	4	"As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007;. For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). A result like this sheds light onto the generalization process: deciding how broadly a category should be applied depends on the timing of experience with exemplars (see #CITATION_TAG and Spalding, 1997;Samuelson and Horst, 2007, for similar findings)."	s
CC2207	The description and role of fine structuring of coronal loops is certainly a challenge for coronal physics, also on the side of modeling, essentially because we have few constraints from observations (Section 3.2.2). Small-scale structuring is already involved in the magnetic carpet scenario and  (#CITATION_TAG et al., 2002, see also Section 4.4). One of the first times that the internal structuring of coronal loops have been invoked in a modeling context was for the problem of the interpretation of the uniform filter ratio distribution detected with TRACE along warm loops. Standard models of single hydrostatic loops with uniform heating were soon found to be unable to explain such indication of uniform temperature distribution (Lenz et al., 1999). A uniform filter ratio could be reproduced by the superposition of several thin hydrostatic strands at different temperatures (Reale and Peres, 2000). In alternative, also a model of long loops heated at the footpoints leads to mostly isothermal loops (Aschwanden, 2001). The problem with this model is that footpoint-heated loops (with heating scale height less than 1/3 of the loop half-length) had been shown to be thermally unstable (Mendoza-Briceno and Hood, 1997) and, therefore they cannot be long-lived, as instead observed. A further alternative is to explain observations with steady non-static loops, i.e., with significant flows inside (Winebarger et al., 2001(Winebarger et al., , 2002c. Also this hypothesis does not seem to answer the question (Patsourakos et al., 2004).	5	The description and role of fine structuring of coronal loops is certainly a challenge for coronal physics, also on the side of modeling, essentially because we have few constraints from observations (Section 3.2. 2). Small-scale structuring is already involved in the magnetic carpet scenario and  (#CITATION_TAG et al., 2002, see also Section 4.4). One of the first times that the internal structuring of coronal loops have been invoked in a modeling context was for the problem of the interpretation of the uniform filter ratio distribution detected with TRACE along warm loops. Standard models of single hydrostatic loops with uniform heating were soon found to be unable to explain such indication of uniform temperature distribution (Lenz et al., 1999). A uniform filter ratio could be reproduced by the superposition of several thin hydrostatic strands at different temperatures (Reale and Peres, 2000).	a
CC315	"Recent work by, among others, Martin Weitzman (2010) and Frank Ackerman and colleagues (2010) has questioned the prevailing assumptions made about functional form. To take the DICE model as an example (Nordhaus and Boyer 2000;Nordhaus 2008), it can easily be shown that the assumption of a quadratic relationship between damages and temperature, together with the modellers"" specific coefficient values, implies that global warming can reach more than 18��C before the equivalent of 50% of global GDP is lost. This seems remarkable, since, for example, such temperatures are likely to test the limits of human physiology (#CITATION_TAG and Huber 2010). While the parameters of the damage function in PAGE are modelled as random, such that damages reach up to around 10% of global GDP when global warming reaches 5��C, it has equally been argued that 5��C constitutes an environmental transformation, being a larger change in global mean temperature than exists between the present day and the peak of the last ice age. Surely it is at least possible that climate damages will exceed 10% of global GDP upon 5��C warming? As far as FUND is concerned, Figure 1A of the Interagency Working Group shows that its more complex, sectorally disaggregated approach implies total damages are actually slowing as warming passes 5��C, and at 8��C above pre-industrial they are only about 7% of GDP. 2 learly this begs the question of how much higher the SCC might be, if the damage function becomes steeper. A recent paper by Ackerman and Stanton (2011) attempts to answer it using the DICE model, applying a functional form proposed by Weitzman (2010). Furthermore, they also question the damage estimates of the models at low temperatures, drawing on work by Michael Hanemann (2008) that argues damages could also be significantly higher in this realm. Looking at these changes separately and together, they show that the SCC could be several times, even orders of magnitude, higher. This result is in fact corroborated by some recent analysis of my own."	0	"Recent work by, among others, Martin Weitzman (2010) and Frank Ackerman and colleagues (2010) has questioned the prevailing assumptions made about functional form. To take the DICE model as an example (Nordhaus and Boyer 2000;Nordhaus 2008), it can easily be shown that the assumption of a quadratic relationship between damages and temperature, together with the modellers"" specific coefficient values, implies that global warming can reach more than 18��C before the equivalent of 50% of global GDP is lost. This seems remarkable, since, for example, such temperatures are likely to test the limits of human physiology (#CITATION_TAG and Huber 2010). While the parameters of the damage function in PAGE are modelled as random, such that damages reach up to around 10% of global GDP when global warming reaches 5��C, it has equally been argued that 5��C constitutes an environmental transformation, being a larger change in global mean temperature than exists between the present day and the peak of the last ice age. Surely it is at least possible that climate damages will exceed 10% of global GDP upon 5��C warming? As far as FUND is concerned, Figure 1A of the Interagency Working Group shows that its more complex, sectorally disaggregated approach implies total damages are actually slowing as warming passes 5��C, and at 8��C above pre-industrial they are only about 7% of GDP."	i
CC1826	The results also demonstrate how choice can be biased towards action. The fact that choices associated with action are preferred over choices associated with inaction suggests a possible source for suboptimal decision making. Given that this bias is independent of valence it may reflect a general action bias observed in instrumental learning under uncertainty [24,#CITATION_TAG[31][32]. This choice bias is especially interesting because it makes explanations of enhanced revaluation due to effort [33,34] less plausible for the effect of action on the dynamics of choice-induced preference change. If the key press was perceived as effortful one would expect subjects to be biased against pressing the key, as item values were closely matched. By contrast, a general bias towards action could be explained by a higher hedonic value of action itself if inaction is perceived as inhibition [35,36].	0	The results also demonstrate how choice can be biased towards action. The fact that choices associated with action are preferred over choices associated with inaction suggests a possible source for suboptimal decision making. Given that this bias is independent of valence it may reflect a general action bias observed in instrumental learning under uncertainty [24,#CITATION_TAG[31][32]. This choice bias is especially interesting because it makes explanations of enhanced revaluation due to effort [33,34] less plausible for the effect of action on the dynamics of choice-induced preference change. If the key press was perceived as effortful one would expect subjects to be biased against pressing the key, as item values were closely matched. By contrast, a general bias towards action could be explained by a higher hedonic value of action itself if inaction is perceived as inhibition [35,36].	v
CC2960	The critical period for the evolution of language, then, was likely to have been the Pleistocene, when the transition from a forested environment to the more open savanna placed a survival premium on social bonding and the sharing of experiences. The capacity for mental time travel may have been extended to enable longer-term plans and deeper access to the past. As evidence for cognitive enhancement, brain size approximately tripled during the Pleistocene (#CITATION_TAG and Collard, 1999). These developments would have underpinned more effective communication, with the emergence of obligate bipedalism adding to the communicative power of gesture, initially by freeing the hands but also exposing the rest of the body, including the face, as communicative systems.	0	The critical period for the evolution of language, then, was likely to have been the Pleistocene, when the transition from a forested environment to the more open savanna placed a survival premium on social bonding and the sharing of experiences. The capacity for mental time travel may have been extended to enable longer-term plans and deeper access to the past. As evidence for cognitive enhancement, brain size approximately tripled during the Pleistocene (#CITATION_TAG and Collard, 1999). These developments would have underpinned more effective communication, with the emergence of obligate bipedalism adding to the communicative power of gesture, initially by freeing the hands but also exposing the rest of the body, including the face, as communicative systems.	 
CC1545	"Over the past two decades, research in business-to-business markets has increasingly focused on the study of buyer-seller relationships because relationships generate many benefits for organizations, e.g. in terms of increased performance, competitiveness, satisfaction, and innovation (Gummesson, 2004;Lindgreen & Wynstra, 2005;Ravald & Gr�_nroos, 1996;Sharma et al., 1999). A buyer-seller relationship can be defined as a non-accidental sequence of market transactions between independent market actors (Kleinaltenkamp & Ehret, 2006). The literature suggests that companies maintain relationship bonds either because ""they have to due to high switching costs, or because ""they want to because of high relationship value (Bendapudi & Berry, 1997;de Ruyter et al., 2001;Gilliland & Bello, 2002;Gounaris, 2005;Liu, 2006). So far, there is little empirical evidence about the consequences of these two motives on companies"" behavioral intentions in relationships. In well-functioning relationships, buyers and sellers not only show lower tendencies to search for and switch to alternative partners but are also expected to intensify their relationships and even temporarily tolerate disadvantages of being in business together (#CITATION_TAG & Weitz, 1992;Bendapudi & Berry, 1997;Gounaris, 2005). Because there are many benefits at stake in business relationships, it is central for companies to better understand what influences the intentions of their partners."	1	"A buyer-seller relationship can be defined as a non-accidental sequence of market transactions between independent market actors (Kleinaltenkamp & Ehret, 2006). The literature suggests that companies maintain relationship bonds either because ""they have to due to high switching costs, or because ""they want to because of high relationship value (Bendapudi & Berry, 1997;de Ruyter et al., 2001;Gilliland & Bello, 2002;Gounaris, 2005;Liu, 2006). So far, there is little empirical evidence about the consequences of these two motives on companies"" behavioral intentions in relationships. In well-functioning relationships, buyers and sellers not only show lower tendencies to search for and switch to alternative partners but are also expected to intensify their relationships and even temporarily tolerate disadvantages of being in business together (#CITATION_TAG & Weitz, 1992;Bendapudi & Berry, 1997;Gounaris, 2005). Because there are many benefits at stake in business relationships, it is central for companies to better understand what influences the intentions of their partners."	e
CC855	"i. The study illustrated the use of mixed methods for theory building in GSCM using ISM and MICMAC analyses, as well as CFA. The selection of these methods was based on their ability to discover and validate relationships between our enablers. Depending on the research question(s), other methods or their combination may also be used. #CITATION_TAG and Ozanne (1988) suggest that although research paradigm incommensurability exists, it does not imply that ""the two approaches cannot peacefully coexist or that other middle-ground approaches cannot or should not be developed"" (p. 508). ii. Our research can further be enhanced using multiple cases in both developing and developed countries. Such research could qualitatively validate our proposed model before we validate it using quantitative methods. iii. In this research we used focus groups with participants from industry and academia. However, other methods could be used, e.g. Delphi methods. iv. The present study has not included top management beliefs and top management participation as two different constructs. In future, it may be fruitful to include these two constructs to draw further insights on GSCM practices. v. The present study can also be extended to the service sector of developing countries. In India, the service sector is the fastest growing industry and useful results for further development of the sector can be derived from such a study. vi. This study on the barriers of GSCM implementation can be further extended to include developing countries. In this vein a comparison between developed and developing countries could take place, with the aim to inform the literature on the similarities and differences, but also lessons for the practical application and achievement of GSCM."	5	"i. The study illustrated the use of mixed methods for theory building in GSCM using ISM and MICMAC analyses, as well as CFA. The selection of these methods was based on their ability to discover and validate relationships between our enablers. Depending on the research question(s), other methods or their combination may also be used. #CITATION_TAG and Ozanne (1988) suggest that although research paradigm incommensurability exists, it does not imply that ""the two approaches cannot peacefully coexist or that other middle-ground approaches cannot or should not be developed"" (p. 508). ii. Our research can further be enhanced using multiple cases in both developing and developed countries."	T
CC2016	"This article investigates the consequences of private equity takeovers on employment and remuneration, focusing specifically on institutional buy outs (IBOs). There is a growing public policy interest in the role of private equity investors. Central to this debate is a concern that private equity may represent an extreme form of capitalism, seeking to maximize short term shareholder wealth while paying little attention to the interests of broader stakeholder groups or organizational sustainability. Indeed, both politicians and trade union representatives have raised serious concerns regarding the potential consequences of private equity acquisitions for the welfare of employees in acquired firms, calling for stronger regulation and greater transparency in respect of the activities of private equity acquirers (#CITATION_TAG, 2007;Treasury Select Committee, 2007). Central to this is the concern that private equity acquirers may seek to gain at employees"" expense, specifically in terms of layoffs and lower wages, which not only leaves them worse off, but may also undermine the future viability of the Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/eer firm (International Trade Union Confederation, 2007). In contrast, proponents of the industry suggest that closer owner supervision may reduce the agency problem, with the injection of new managers and managerial approaches more closely aligned to the agenda of shareholder value maximization, optimizing organizational outcomes (BVCA, 2006)."	4	"This article investigates the consequences of private equity takeovers on employment and remuneration, focusing specifically on institutional buy outs (IBOs). There is a growing public policy interest in the role of private equity investors. Central to this debate is a concern that private equity may represent an extreme form of capitalism, seeking to maximize short term shareholder wealth while paying little attention to the interests of broader stakeholder groups or organizational sustainability. Indeed, both politicians and trade union representatives have raised serious concerns regarding the potential consequences of private equity acquisitions for the welfare of employees in acquired firms, calling for stronger regulation and greater transparency in respect of the activities of private equity acquirers (#CITATION_TAG, 2007;Treasury Select Committee, 2007). Central to this is the concern that private equity acquirers may seek to gain at employees"" expense, specifically in terms of layoffs and lower wages, which not only leaves them worse off, but may also undermine the future viability of the Contents lists available at ScienceDirect journal homepage: www. elsevier.com/locate/eer firm (International Trade Union Confederation, 2007). In contrast, proponents of the industry suggest that closer owner supervision may reduce the agency problem, with the injection of new managers and managerial approaches more closely aligned to the agenda of shareholder value maximization, optimizing organizational outcomes (BVCA, 2006)."	e
CC2398	"This category of decisions at the end of life, as defined and quantified in the epidemiological studies carried out in the Netherlands and Belgium and also in other European and English-speaking countries where euthanasia is illegal (Kuhse et al. 1997;Mitchell and Owens 2003;van der Heide et al. 2003;Chambaere et al. 2010a), are a major concern of critics of euthanasia (Pereira 2011;Cohen-Almagor 2013). In the empirical studies, a death is classified as LAWER if the physician checks boxes denoting that (1) drugs were administered with the explicit intention of shortening the patient""s life and (2) the patient did not explicitly request this (van der Heide et al. 2003). In clinical terms, most LAWER cases are found to be for patients in extremis, irreversibly unconscious, and having previously expressed a wish of euthanasia. The estimated abbreviation of survival is very short, relatives are usually consulted, and the drugs used are the same as for palliative sedation, namely opioids, which have no or minimal life-ending potential (L�_pez-Saca, Guzm͍n, and Centeno 2013;#CITATION_TAG et al. 2010;Chambaere et al. 2010a). Clinically, just as unreported cases of requested euthanasia (Smets et al. 2010), LAWER cases are therefore very different from euthanasia cases. When in a subsequent question physicians were asked whether they considered a case to be one of euthanasia or life-ending without request, they usually said no and did not feel they had to report them. Most of LAWER cases should probably more realistically be interpreted as ""compassionate intended abbreviation of terminal agony"" (see Chambaere et al. 2010a)."	2	"This category of decisions at the end of life, as defined and quantified in the epidemiological studies carried out in the Netherlands and Belgium and also in other European and English-speaking countries where euthanasia is illegal (Kuhse et al. 1997;Mitchell and Owens 2003;van der Heide et al. 2003;Chambaere et al. 2010a), are a major concern of critics of euthanasia (Pereira 2011;Cohen-Almagor 2013). In the empirical studies, a death is classified as LAWER if the physician checks boxes denoting that (1) drugs were administered with the explicit intention of shortening the patient""s life and (2) the patient did not explicitly request this (van der Heide et al. 2003). In clinical terms, most LAWER cases are found to be for patients in extremis, irreversibly unconscious, and having previously expressed a wish of euthanasia. The estimated abbreviation of survival is very short, relatives are usually consulted, and the drugs used are the same as for palliative sedation, namely opioids, which have no or minimal life-ending potential (L�_pez-Saca, Guzm͍n, and Centeno 2013;#CITATION_TAG et al. 2010;Chambaere et al. 2010a). Clinically, just as unreported cases of requested euthanasia (Smets et al. 2010), LAWER cases are therefore very different from euthanasia cases. When in a subsequent question physicians were asked whether they considered a case to be one of euthanasia or life-ending without request, they usually said no and did not feel they had to report them. Most of LAWER cases should probably more realistically be interpreted as ""compassionate intended abbreviation of terminal agony"" (see Chambaere et al. 2010a)."	 
CC347	"In summary, two types of information (coupling of a particular form-not simply shape-with matter, energy, space and time), information of the chassis (casing + metabolism) and information of the program are associated together in a cell (Tanaka 1984). A synthetic cell needs the association of a chassis developing metabolism (not a simple 3D casing) and a program similar to that found in computers. The conclusions of Dyson""s argument on the double origin of life, with reproducing metabolism predating replication are therefore a pre-requisite for synthesis of life (Dyson 1985). This dichotomy is visible in present synthetic biology, with a fairly clear separation between those who study the chassis (and are often also interested in the origin of life) (Kuruma et al. 2009;Shenhav and Lancet 2004) and those who think that life is essentially due to the genetic program, organising their activity around construction of program biobricks, or even as complete genomes (#CITATION_TAG et al. 2008)."	1	"In summary, two types of information (coupling of a particular form-not simply shape-with matter, energy, space and time), information of the chassis (casing + metabolism) and information of the program are associated together in a cell (Tanaka 1984). A synthetic cell needs the association of a chassis developing metabolism (not a simple 3D casing) and a program similar to that found in computers. The conclusions of Dyson""s argument on the double origin of life, with reproducing metabolism predating replication are therefore a pre-requisite for synthesis of life (Dyson 1985). This dichotomy is visible in present synthetic biology, with a fairly clear separation between those who study the chassis (and are often also interested in the origin of life) (Kuruma et al. 2009;Shenhav and Lancet 2004) and those who think that life is essentially due to the genetic program, organising their activity around construction of program biobricks, or even as complete genomes (#CITATION_TAG et al. 2008)."	s
CC599	Definition. The integrated density of states _� n is the probability measure _� n on R such that ___ __�___ f (x)_� n (dx) = 1 n M n trace f (X)�_ (2) n (dX) (1.2) for all continuous and compactly supported real functions f . The equilibrium measure _� is the probability measure with support S that arises as the weak limit of the _� n so #CITATION_TAG et al [5] prove the existence of this weak limit under general conditions which include the above v. They prove that there exists a constant C v such that v(x) ___ 2 log |x __� y| _�(dy) + C v (x ___ R) (1.4) and that equality holds if and only if x belongs to S. Furthermore, there exists g ___ 0 and	5	Definition. The integrated density of states _� n is the probability measure _� n on R such that ___ __�___ f (x)_� n (dx) = 1 n M n trace f (X)�_ (2) n (dX) (1.2) for all continuous and compactly supported real functions f . The equilibrium measure _� is the probability measure with support S that arises as the weak limit of the _� n so #CITATION_TAG et al [5] prove the existence of this weak limit under general conditions which include the above v. They prove that there exists a constant C v such that v(x) ___ 2 log |x __� y| _�(dy) + C v (x ___ R) (1.4) and that equality holds if and only if x belongs to S. Furthermore, there exists g ___ 0 and	h
CC856	"Our hypotheses were all supported; in particular our study highlights the importance of institutional pressures Schroeder, 2004, Ketchen andHult, 2007;Liu et al., 2010;Sarkis et al., 2011;Bhakoo and Choi, 2013;Kauppi, 2013;Dubey et al., 2014) and top management commitment (Liang et al., 2007;Gattiker and Carter, 2010;Foerstl et al., 2015;Jabbour and Jabbour, in press) in the management of both suppliers and customers within the context of GSCM. Firms that engage in establishing strong relationships with suppliers enjoy superior performance (Giannakis, 2007;Reuter et al., 2010;#CITATION_TAG and Tachizawa, 2012;Burritt and Schaltegger, 2012). In our study, we did not show that CRM and SRM are directly influencing performance (either environmental or financial) but that they enable GTA and TQM and then influence environmental and financial performance. Furthermore, we extended those studies illustrating the role of TMC in mediating the relationships between institutional pressures and intention to adopt best sustainability practices, in that we showed that TMC mediates the relationship between institutional pressures (either external or internal) and customer relationship management. Hence TMC may ""translate"" different pressures into initiatives with target particular customers and aim to maintain relationship with them. In this vein, our study highlights the role of developing appropriate strategies that translate pressures into management and foster collaboration with suppliers and customers (Hsu and Hu, 2009;Ku et al., 2010;Hoejmose et al., 2012;Shaw et al., 2012;Caniels et al., 2013;Govindan et al., 2013;Zhu and Geng, 2013)."	0	"Our hypotheses were all supported; in particular our study highlights the importance of institutional pressures Schroeder, 2004, Ketchen andHult, 2007;Liu et al., 2010;Sarkis et al., 2011;Bhakoo and Choi, 2013;Kauppi, 2013;Dubey et al., 2014) and top management commitment (Liang et al., 2007;Gattiker and Carter, 2010;Foerstl et al., 2015;Jabbour and Jabbour, in press) in the management of both suppliers and customers within the context of GSCM. Firms that engage in establishing strong relationships with suppliers enjoy superior performance (Giannakis, 2007;Reuter et al., 2010;#CITATION_TAG and Tachizawa, 2012;Burritt and Schaltegger, 2012). In our study, we did not show that CRM and SRM are directly influencing performance (either environmental or financial) but that they enable GTA and TQM and then influence environmental and financial performance. Furthermore, we extended those studies illustrating the role of TMC in mediating the relationships between institutional pressures and intention to adopt best sustainability practices, in that we showed that TMC mediates the relationship between institutional pressures (either external or internal) and customer relationship management. Hence TMC may ""translate"" different pressures into initiatives with target particular customers and aim to maintain relationship with them."	i
CC443	"Where __and __ are the relative shifts in supply and demand and __ \u202b__\u202c and __ \u202b__\u202c are the relative shifts in price and quantity between two periods._�and __ are predetermined price elasticities of supply and demand. Asterisks denote percentage change throughout the article. The specification of a demand shift is the horizontal equivalent of Muth""s (1964) description of a vertical demand shift, which appears in his seminal paper that introduced the EDM, and is a common specification of a demand shift in EDMs. It can also be traced back to Leontief\""s definition of ""Niveauverschiebungen"" (Shifts of level) in his (unsuccessful) theory for deriving supply and demand curves from price and quantity data only (#CITATION_TAG 1929)."	0	"Where __and __ are the relative shifts in supply and demand and __ \u202b__\u202c and __ \u202b__\u202c are the relative shifts in price and quantity between two periods. _�and __ are predetermined price elasticities of supply and demand. Asterisks denote percentage change throughout the article. The specification of a demand shift is the horizontal equivalent of Muth""s (1964) description of a vertical demand shift, which appears in his seminal paper that introduced the EDM, and is a common specification of a demand shift in EDMs. It can also be traced back to Leontief\""s definition of ""Niveauverschiebungen"" (Shifts of level) in his (unsuccessful) theory for deriving supply and demand curves from price and quantity data only (#CITATION_TAG 1929)."	 
CC2333	The training methods used here did not lead to larger improvements in vowel identification than seen in the control conditions. It may be that 2 h of training with connected prose is not sufficient. Other studies suggest that training effects may be specific to the test materials. For example, #CITATION_TAG and Summerfield (2008) found that 3 h of sentence training using a similar shifted noise-vocoder led to improved sentence recognition but not to improved identification of vowels or of consonants. A complementary finding reported by  is that vowel recognition was improved by vowel and consonant phoneme training but not by sentence training.	5	The training methods used here did not lead to larger improvements in vowel identification than seen in the control conditions. It may be that 2 h of training with connected prose is not sufficient. Other studies suggest that training effects may be specific to the test materials. For example, #CITATION_TAG and Summerfield (2008) found that 3 h of sentence training using a similar shifted noise-vocoder led to improved sentence recognition but not to improved identification of vowels or of consonants. A complementary finding reported by  is that vowel recognition was improved by vowel and consonant phoneme training but not by sentence training.	 
CC2001	The species specificity of the mycobiont towards its photobiont was quite low for P. decipiens. In contrast, Fulgensia bracteata ssp. deformis (which has so far only been found Other EGMA other eukaryotic green micro algae in samples from Hochtor) only occurred with T. sp. URa4 and A. sp. URa15 (the latter until now only known from this area, Figs. 2, 3). Peltigera rufescens, known to have a cyanobacterium as its primary photobiont (#CITATION_TAG et al. 2005), was also found to be associated with chlorobionts (Henskens et al. 2012). Specimens of P. rufescens from Ruine Homburg were associated with T. sp. URa6 and A. sp. URa16, although other chlorobionts were available at the site; at Hochtor P. rufescens was found with T. impressa (see Online Resource 1, Figs. 2, 3).	1	URa4 and A. sp. URa15 (the latter until now only known from this area, Figs. 2, 3). Peltigera rufescens, known to have a cyanobacterium as its primary photobiont (#CITATION_TAG et al. 2005), was also found to be associated with chlorobionts (Henskens et al. 2012). Specimens of P. rufescens from Ruine Homburg were associated with T. sp. URa6 and A. sp. URa16, although other chlorobionts were available at the site; at Hochtor P. rufescens was found with T. impressa (see Online Resource 1, Figs.	e
CC2848	Godunov-type shallow water models are featured with the inherent ability to accommodate complex flow transitions within the numerical solution (#CITATION_TAG 2001, Guinot 2003, Toro and Garc�_a-Navarro 2007. In recent years, they have received applied improvements and have been incorporated into water industry standard software (Lhomme et al. 2010), and used to support flood risk management (Nթelz and Pender 2010). In this context, applicable Godunov-type water wave models are at most second-order accurate and require a topography discretization technique and a wetting and drying condition (see Delis and Kampanis 2009 for a comprehensive review).	0	Godunov-type shallow water models are featured with the inherent ability to accommodate complex flow transitions within the numerical solution (#CITATION_TAG 2001, Guinot 2003, Toro and Garc�_a-Navarro 2007. In recent years, they have received applied improvements and have been incorporated into water industry standard software (Lhomme et al. 2010), and used to support flood risk management (Nթelz and Pender 2010). In this context, applicable Godunov-type water wave models are at most second-order accurate and require a topography discretization technique and a wetting and drying condition (see Delis and Kampanis 2009 for a comprehensive review).	G
CC1534	In sum, SD presents a remarkably focused deterioration in conceptual-semantic knowledge such that specific, distinctive information is lost and expressions of knowledge become increasingly 0093-934X/$ -see front matter �_ 2009 Elsevier Inc. All rights reserved. doi:10.1016/j.bandl.2009.03.007 general, with better preservation of typical, familiar and highly frequent information #CITATION_TAG, 1975). Given these qualities, the syndrome provides a unique opportunity to explore how a degraded semantic system (and associated lexical deficits) affect speech production. 1 Most models of spoken language production share the assumption that there are three distinct levels of processing. First is a level at which the pre-verbal message is generated and concepts are retrieved to express it (message generation). Second, there is an intermediate level at which lexico-semantic representations (words) are selected to express the concepts (retrieval of open class items); it is also thought that syntactic information is retrieved at this point and sentence structure is generated (closed class items and morphology). Third, phonological representations are retrieved that map onto the selected words (e.g. Bock, 1999;Garrett, 1984;Goldrick, 2006;Levelt, Roelofs, & Meyer, 1999;Vigliocco & Kita, 2006). At present, all models assume cascading of information (i.e. partial information from one level can be accessed by the next) from conceptual to lexico-semantic representations, as this allows multiple candidates to be activated for a particular target (Goldrick, 2006;Levelt et al., 1999); interactivity between other levels is still a topic of much debate (Dell, 1986;Rapp & Goldrick, 2000;Roelofs, 2004;Vigliocco & Hartsuiker, 2002).	0	In sum, SD presents a remarkably focused deterioration in conceptual-semantic knowledge such that specific, distinctive information is lost and expressions of knowledge become increasingly 0093-934X/$ -see front matter �_ 2009 Elsevier Inc. All rights reserved. doi:10.1016/j. bandl. 2009.03.007 general, with better preservation of typical, familiar and highly frequent information #CITATION_TAG, 1975). Given these qualities, the syndrome provides a unique opportunity to explore how a degraded semantic system (and associated lexical deficits) affect speech production. 1 Most models of spoken language production share the assumption that there are three distinct levels of processing. First is a level at which the pre-verbal message is generated and concepts are retrieved to express it (message generation).	9
CC762	"Expectations are important in any model with network effects, not just in the case of telecommunication. Examples include two-sided markets such as newspapers or credit cards. Readers care about the number of adds and advertisers care about the number of readers. Merchants care about the number of users of a particular credit card and users care about the number of merchants accepting a particular credit card. Network effects can also occur in financial markets. The riskiness of a bank may very well depend on its size, that is, the number of depositors. Of course, depositors will care about the riskiness and thus about the number of other people who will deposit in a given bank. (See Matutes and Vives, 1996.) Expectations even play a role in markets without network effects. For example, consider a monopolistic upstream supplier of an input to several downstream firms that compete with each other in a final product market. The prices paid for the inputs determine the marginal costs for the downstream firms. If the prices of inputs are set secretly, each downstream firm needs to form expectations about the prices paid by its competitors in order to know how profitable competition will turn out to be and to determine the demand for inputs. In this context passive beliefs seem very reasonable and widely accepted. 18 ny papers have been written on markets with network effects and some have modeled consumer expectations as passive and some have modeled them as rationally responsive. 19 ry few papers justify or even discuss the assumption about expectations. Katz and Shapiro (1985) do mention the possibility of responsive beliefs in their Appendix, but in their quantity setting framework results are not altered in an important manner. 20 #CITATION_TAG and Mason (2001) take the issue of expectations serious and point out that the results change dramatically if rationally responsive beliefs are used in their pricing game. Matutes and Vives (1996) char-18 See Hart and Tirole (1990), O""Brien and Shaffer (1992), Fontenay and Gans (2005 and Rey and Tirole (2007). McAfee and Schwartz (1994) also consider so called ""wary non-passive, beliefs."	1	"18 ny papers have been written on markets with network effects and some have modeled consumer expectations as passive and some have modeled them as rationally responsive. 19 ry few papers justify or even discuss the assumption about expectations. Katz and Shapiro (1985) do mention the possibility of responsive beliefs in their Appendix, but in their quantity setting framework results are not altered in an important manner. 20 #CITATION_TAG and Mason (2001) take the issue of expectations serious and point out that the results change dramatically if rationally responsive beliefs are used in their pricing game. Matutes and Vives (1996) char-18 See Hart and Tirole (1990), O""Brien and Shaffer (1992), Fontenay and Gans (2005 and Rey and Tirole (2007). McAfee and Schwartz (1994) also consider so called ""wary non-passive, beliefs."	 
CC2012	"Running parallel to concerns in the policy community, an emerging strand of academic enquiry has begun to examine the employment consequences of private equity takeovers, largely motivated by the seminal work of #CITATION_TAG and Summers (1988), who suggested that acquisitions may provide an opportunity for managers to challenge any implicit contracts between employees and the firm, thereby expropriating the former. However, in the context of private equity, much of this work has suffered from a lack of precision as to what exactly constitutes a private equity takeover. This is especially important since the consequences for employees are expected to depend on the type of private equity takeover in question. Private equity is a term frequently used to refer to two substantially different types of corporate investment (Wood and Wright, 2010). The first type is venture capital, which consists of early stage investment in firms that have not yet been floated on the stock market. This type of investment is typically combined with significant input by the venture capitalist in the firm""s strategic direction. The broad consensus is that the effects of venture capital are typically positive (Wood and Wright, 2010). Conversely, the second type of investment, i.e. private equity itself, refers to the purchase by an investor, or the facilitation of the purchase, of a firm. Once the purchase has been completed, either new management is put in place or at least there is a change in management style in an attempt to enhance returns. Many of the initial studies, as Davis et al. (2011:1) note, conflated the two. Private equity takeovers of mature firms can themselves be divided into several subcategories, although, once more, these are often conflated in the literature (Wood and Wright, 2009). Firstly, in the case of management buy outs (MBOs), the existing management team buys out external shareholders, supported by private equity. Secondly, in the case of management buy ins (MBIs), outside managers take over control, again supported by private equity (Wood and Wright, 2009). Finally, institutional buy outs (IBOs) involve private equity and other institutional investors; here managers do not hold any shares at all, unless this is part of their reward package (Renneboog et al., 2007)."	4	Running parallel to concerns in the policy community, an emerging strand of academic enquiry has begun to examine the employment consequences of private equity takeovers, largely motivated by the seminal work of #CITATION_TAG and Summers (1988), who suggested that acquisitions may provide an opportunity for managers to challenge any implicit contracts between employees and the firm, thereby expropriating the former. However, in the context of private equity, much of this work has suffered from a lack of precision as to what exactly constitutes a private equity takeover. This is especially important since the consequences for employees are expected to depend on the type of private equity takeover in question. Private equity is a term frequently used to refer to two substantially different types of corporate investment (Wood and Wright, 2010).	R
CC2901	"The sampling frame was extracted from the Dun & Bradstreet database of Norwegian companies. The original file contained 3082 foreign subsidiaries established by approximately Norwegian MNCs. Going through the entire database including telephone and e-mail contact if necessary, the database was further reduced to a set of foreign subsidiaries and 564 potentially relevant MNCs with one or more foreign subsidiaries established during the chosen time frame. 2 e study relied on a single key informant approach, and the key informants were those persons in the MNC who had appropriate knowledge about the research issue and were willing and able to ""talk"" about it by answering the questionnaire (#CITATION_TAG, 1955). In most cases, this person was the managing director, but division managers, finance directors, marketing directors, and owners of the MNCs were also among the key informants. The procedure to identify these persons was the following. First, all companies were contacted by phone with the intention to; (a) detect whether the company with a respective foreign subsidiary met the criteria for inclusion in the study, and (b) identify a key informant in the A second screening was conducted using annual reports, internet sources and other databases such as Amadeus, focusing on type of activities and ownership circumstances (e.g. that the companies were Norwegian-owned). After this screening, the appropriate sample frame was narrowed down to 370 MNCs, of which 346 MNCs were willing to participate. Questionnaires were sent by mail to these companies."	0	"The sampling frame was extracted from the Dun & Bradstreet database of Norwegian companies. The original file contained 3082 foreign subsidiaries established by approximately Norwegian MNCs. Going through the entire database including telephone and e-mail contact if necessary, the database was further reduced to a set of foreign subsidiaries and 564 potentially relevant MNCs with one or more foreign subsidiaries established during the chosen time frame. 2 e study relied on a single key informant approach, and the key informants were those persons in the MNC who had appropriate knowledge about the research issue and were willing and able to ""talk"" about it by answering the questionnaire (#CITATION_TAG, 1955). In most cases, this person was the managing director, but division managers, finance directors, marketing directors, and owners of the MNCs were also among the key informants. The procedure to identify these persons was the following. First, all companies were contacted by phone with the intention to; (a) detect whether the company with a respective foreign subsidiary met the criteria for inclusion in the study, and (b) identify a key informant in the A second screening was conducted using annual reports, internet sources and other databases such as Amadeus, focusing on type of activities and ownership circumstances (e.g. that the companies were Norwegian-owned)."	 
CC1358	The correspondence between gravitational theories in anti-de Sitter spacetime and certain quantum field theories [1] provides a unique way in which to study the strongly coupled sector of many quantum field theories. This remarkable result from string theory has allowed some insight [2] to be gained into why the quark-gluon plasma produced at the relativistic heavy ion collider (RHIC) behaves like an almost perfect fluid [3] (in contrast to the prediction of a high viscosity by perturbative quantum chromodynamics (QCD) [4]). This remarkable result inspired the application of AdS/CFT techniques to certain condensed matter systems. Phenomena such as the Hall effect and the Nernst effect appear to have their dual gravitational descriptions [#CITATION_TAG,6,7].	2	The correspondence between gravitational theories in anti-de Sitter spacetime and certain quantum field theories [1] provides a unique way in which to study the strongly coupled sector of many quantum field theories. This remarkable result from string theory has allowed some insight [2] to be gained into why the quark-gluon plasma produced at the relativistic heavy ion collider (RHIC) behaves like an almost perfect fluid [3] (in contrast to the prediction of a high viscosity by perturbative quantum chromodynamics (QCD) [4]). This remarkable result inspired the application of AdS/CFT techniques to certain condensed matter systems. Phenomena such as the Hall effect and the Nernst effect appear to have their dual gravitational descriptions [#CITATION_TAG,6,7].	n
CC1638	Firms may also combine performance measures to reduce uncontrollable risk, if measurement errors are negatively correlated. Consider P 3 = b 1 P 1 + b 2 P 2 , where b i is the weight on measure P i , which has measurement error �� i . Measurement error of P 3 is . If �� 12 < 0, lowering overall risk by combining measures may be possible. One important example is relative performance evaluation (Lazear & Rosen 1981;Gibbons & Murphy 1990), in which supervisors evaluate an employee relative to colleagues or some other reference group. This evaluation may be effective if measurement error is common to employees in the group, which relative comparison filters out. Once more, however, tradeoffs can exist. Relative evaluation may also distort incentives if the employee can take actions to affect the group against which he is compared or if he can cooperate with or sabotage colleagues (#CITATION_TAG 1989).	0	One important example is relative performance evaluation (Lazear & Rosen 1981;Gibbons & Murphy 1990), in which supervisors evaluate an employee relative to colleagues or some other reference group. This evaluation may be effective if measurement error is common to employees in the group, which relative comparison filters out. Once more, however, tradeoffs can exist. Relative evaluation may also distort incentives if the employee can take actions to affect the group against which he is compared or if he can cooperate with or sabotage colleagues (#CITATION_TAG 1989).	i
CC1078	Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (#CITATION_TAG et al., 2012). The work was almost entirely performed in protoplast with large use of transient transformation but clearly showed that fusogenic and nonfusogenic functions can be ascribed to the same SNARE and be dependent on protein localization. When anchored to the TGN membrane, AtSYP51, and AtSYP52 behaved as t-SNARE, with a fusogenic role, but when they were sorted to the tonoplast their role become non-fusogenic. Despite a certain level of functional specificity, they both seemed to play a structural role in the tonoplast formation by influencing the arrival of new membrane from prevacuolar compartments. Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008).	4	Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (#CITATION_TAG et al., 2012). The work was almost entirely performed in protoplast with large use of transient transformation but clearly showed that fusogenic and nonfusogenic functions can be ascribed to the same SNARE and be dependent on protein localization. When anchored to the TGN membrane, AtSYP51, and AtSYP52 behaved as t-SNARE, with a fusogenic role, but when they were sorted to the tonoplast their role become non-fusogenic. Despite a certain level of functional specificity, they both seemed to play a structural role in the tonoplast formation by influencing the arrival of new membrane from prevacuolar compartments.	R
CC1991	This evaluation of European lichen-dominated soil crusts from four geographically and climatically diverse sites revealed an unexpectedly high diversity of photobionts in association with the dominant lichen P. decipiens. Until now, only the genus Asterochloris has been described as the photobiont of P. decipiens (Schaper and Ott 2003), but we detected 12 different groups of the genus Trebouxia as well as other eukaryotic green micro algae like C. saccharophilum. Several of these micro algae are already known to exist as lichen photobionts, such as T. impressa, T. asymmetrica or the, as yet undescribed, Trebouxia sp. URa2, URa4, URa6. The latter three species have also been identified as photobionts from crustose lichens (Ruprecht et al. 2012). Other Trebouxia species that are known as freeliving algae (e.g. T. arboricola; Ettl and Gärtner 1995) were included in the analysis but not found in the soil-crust samples. P. decipiens at Hochtor showed a shared use of the available photobionts with other lichen species that were present (see Online Resource 1) with each species having a different level of specificity towards to its photobiont. We can conclude for P. decipiens that this lichen is not limited to a single species or even genus of photobiont but instead associates with a broad range of apparently locally available algae. The low specificity of P. decipiens for its photobiont might contribute to the broad ecological amplitude of this lichen, a possibility already described for other lichens (Blaha et al. 2006). Most photobiont species, especially from the genus Trebouxia, are cosmopolitan with more or less broad ecological preferences (Fernandez-Mendoza et al. 2011;Ruprecht et al. 2012) and this was true for the most commonly detected clades in this study. However, several distinct and strongly supported clades of the genera Asterochloris and Trebouxia (Online Resource 2, Figs. 2, 3) do not seem to be cosmopolitan, e.g. T. sp URa8 which, to date, has only been found at Tabernas. This clade is sister to T. gigantea, a photobiont which is widely distributed in temperate habitats (#CITATION_TAG and Gärtner 1995). This is a somewhat similar situation to that found in another study of the cosmopolitan photobiont T. jamesii. Ruprecht et al. (2012) which showed that one sub-clade was only present in the most extreme habitat of the cold deserts in the Darwin Area (Antarctica). More investigations with much more extended taxon sampling needs to be done in order to decide which adaptations have occurred in response to extreme climatic conditions or particular ecological niches, and which speciation model applies.	0	Most photobiont species, especially from the genus Trebouxia, are cosmopolitan with more or less broad ecological preferences (Fernandez-Mendoza et al. 2011;Ruprecht et al. 2012) and this was true for the most commonly detected clades in this study. However, several distinct and strongly supported clades of the genera Asterochloris and Trebouxia (Online Resource 2, Figs. 2, 3) do not seem to be cosmopolitan, e.g. T. sp URa8 which, to date, has only been found at Tabernas. This clade is sister to T. gigantea, a photobiont which is widely distributed in temperate habitats (#CITATION_TAG and Gärtner 1995). This is a somewhat similar situation to that found in another study of the cosmopolitan photobiont T. jamesii. Ruprecht et al. (2012) which showed that one sub-clade was only present in the most extreme habitat of the cold deserts in the Darwin Area (Antarctica). More investigations with much more extended taxon sampling needs to be done in order to decide which adaptations have occurred in response to extreme climatic conditions or particular ecological niches, and which speciation model applies.	s
CC966	In more detail, OT and OT-A are the best methods, except if we are particularly interested in minimizing PND and we suspect conjunctions are present (when we might want to consider CBN) or FPF (when we might want to consider DiProg -recalling that FPF is generally of minor value compared to measures PND and PFD). Since it is impossible for OT to return any conjunctions, further research on computationally efficient methods to recover conjunctions is sorely needed. 3. Using frequency-based statistics when we do not know which mutations are true passengers can lead to a heavy performance penalty mainly in the form of failure to discover existing restrictions. In addition, having to filter genes makes it much harder to intuitively understand, and reason about, what is likely to happen in any scenario, and this in turn makes interpretation of results and reconciliation of output from different methods much harder. Thus, it probably pays off to try to use other approaches that incorporate information about non-silent mutation rates, pathway information together with combinatorial properties of drivers in pathwayws, or functional consequences of mutations to differentiate drivers from passengers [73][74][75][76][77]#CITATION_TAG. It might not always be possible to use these other methods. If we need to rely on frequency-based approaches, selecting those mutations with a frequency larger than 5% is an overall reasonable choice (but not the single best choice for any performance measure other than Diff). 4. Sampling time and type, by themselves, had minor effects compared to, say, filtering or method choice (and we will rarely have control over these factors when we use data already available in databases).	3	3. Using frequency-based statistics when we do not know which mutations are true passengers can lead to a heavy performance penalty mainly in the form of failure to discover existing restrictions. In addition, having to filter genes makes it much harder to intuitively understand, and reason about, what is likely to happen in any scenario, and this in turn makes interpretation of results and reconciliation of output from different methods much harder. Thus, it probably pays off to try to use other approaches that incorporate information about non-silent mutation rates, pathway information together with combinatorial properties of drivers in pathwayws, or functional consequences of mutations to differentiate drivers from passengers [73][74][75][76][77]#CITATION_TAG. It might not always be possible to use these other methods. If we need to rely on frequency-based approaches, selecting those mutations with a frequency larger than 5% is an overall reasonable choice (but not the single best choice for any performance measure other than Diff). 4.	 
CC2276	"A big effort has been devoted to the possible detection of hot plasma outside of evident flares. This would be a conclusive evidence of the presence of impulsive heating mechanisms in coronal loops (e.g., Klimchuk, 2006, see Section 4.4). Hinode instruments appear to be able to provide new interesting contributions to this topic. The analysis of spectroscopic observations of hot lines in solar active regions from Hinode/EIS allows to construct emission measure distributions in the 1 -5 MK temperature range, and shows that the distributions are flat or slowly increasing up to approximately 3 MK and then fall off rapidly at higher temperatures (Patsourakos and Klimchuk, 2009). Emission from very hot lines has been early found in other Hinode/EIS observations, and in particular from the Ca xvii at 192.858��, formed near a temperature of 6 �� 10 6 K, in active regions (Ko et al., 2009). Thanks to its multifilter observations, also Hinode/XRT is providing useful information about the thermal structure of the bright X-ray corona. Temperature maps derived with combined filter ratios show fine structuring to the limit of the instrument resolution and evidence of multithermal Living Reviews in Solar Physics http://www.livingreviews.org/lrsp-2014-4 components (Reale et al., 2007), as complemented by TRACE images. Observations including flare filters show evidence of a hot component in active regions outside of flares (Schmelz et al., 2009), and data in the medium thickness filters appear to constrain better this component of hot plasma as widespread, although minor, and peaking around log ___ 6.8 -6.9, with a tail above 10 MK (Reale et al., 2009b). Further support comes from RHESSI data (Reale et al., 2009a;McTiernan, 2009). Further evidence for minor components of hot plasma in non-flaring active regions have been found from various other instruments. Analysis in the waveband 3.3 -6.1�� and 280 -330�� with the RESIK and SPIRIT instruments, respectively, confirm the presence of a 0.1% ___ 10 MK component at various activity levels (Sylwester et al., 2010;Shestov et al., 2010). Low-resolution SphinX spectra integrated on 17 days in the 2 -10�� band still show a small but highly-significant component at about 7 MK from active regions outside of microflares (Miceli et al., 2012). The separation of the hot from the cool components in the SDO/AIA 94�� channel indicates finelystructured Fe xviii line emission in the core of bright active regions (Reale et al., 2011). This filamented emission at high temperature has been previously predicted with a model of multistranded pulse-heated loops (Guarrasi et al., 2010). The emission from hot emission lines (Ca xvii and Fe xviii) has been confirmed from simultaneous observations with SDO/AIA and with the Hinode/EIS spectrometer (#CITATION_TAG and Reale, 2012;Teriaca et al., 2012). However, while it has been proposed that AIA imaging observations of the solar corona can be used to track hot plasma (6 -8 MK), it has been questioned that such emission is really at the temperature of the line sensitivity peak (Teriaca et al., 2012). Other analysis of a limb active region with EUV spectral data from Hinode/EIS does not find evidence for plasma at temperature log > 7 (O""Dwyer et al., 2011) and puts an upper limit on the same track as remarked by Winebarger et al. (2012). So a final conclusion on this topic is still to be reached. There is some evidence that the amount of high-temperature plasma might correlate with the intensity of the active region magnetic fields because of increasing frequency of energy release (Warren et al., 2012)."	5	"Low-resolution SphinX spectra integrated on 17 days in the 2 -10�� band still show a small but highly-significant component at about 7 MK from active regions outside of microflares (Miceli et al., 2012). The separation of the hot from the cool components in the SDO/AIA 94�� channel indicates finelystructured Fe xviii line emission in the core of bright active regions (Reale et al., 2011). This filamented emission at high temperature has been previously predicted with a model of multistranded pulse-heated loops (Guarrasi et al., 2010). The emission from hot emission lines (Ca xvii and Fe xviii) has been confirmed from simultaneous observations with SDO/AIA and with the Hinode/EIS spectrometer (#CITATION_TAG and Reale, 2012;Teriaca et al., 2012). However, while it has been proposed that AIA imaging observations of the solar corona can be used to track hot plasma (6 -8 MK), it has been questioned that such emission is really at the temperature of the line sensitivity peak (Teriaca et al., 2012). Other analysis of a limb active region with EUV spectral data from Hinode/EIS does not find evidence for plasma at temperature log > 7 (O""Dwyer et al., 2011) and puts an upper limit on the same track as remarked by Winebarger et al. (2012). So a final conclusion on this topic is still to be reached."	o
CC2195	where is the cross-section area, is the temperature, _�_ = 10 6 K, �_ = 30, and = 3.6. Myriads of loops populate the solar corona and constitute statistical ensembles. Attempts to define and classify coronal loops were never easy, and no finally established result exists to-date. Early attempts were based on morphological criteria, i.e., bright points, active-region loops, and large-scale structures (Vaiana et al., 1973, Figure 2), largely observed with instruments in the X-ray band. In addition to such classification, more recently, the observation of loops in different spectral bands and the suspicion that the difference lies not only in the band, but also in intrinsic properties, have stimulated another classification based on the temperature regime, i.e., cool, warm, hot loops (Table 2). Cool loops are generally detected in UV lines at temperatures between 10 5 and 10 6 K. They were first addressed by Foukal (1976) and later explored more with SoHO observations (Brekke et al., 1997). Warm loops are well observed by EUV imagers such as SoHO/EIT, TRACE, and in most channels of SDO/AIA, and confine plasma at temperature around 1 -1.5 MK (#CITATION_TAG et al., 1999). Hot loops are those typically observed in the X-ray band, and in hot UV and EUV lines (e.g., Fe xvi) and channels (SDO/AIA 335��), with temperatures around or above 2 MK (Table 1). These are the coronal loops already identified, for instance, in the early rocket missions (Vaiana et al., 1973). This distinction is not only due to observation with different instruments and in different bands, but there are hints that it may be more substantial and physical, i.e., there may be two or more classes of loops that may be governed by different regimes of physical processes. For instance, the temperature along warm loops appears to be distributed uniformly and the density to be higher than that predicted by equilibrium conditions. Does this make such loops intrinsically different from hot loops, or is it just the signature that warm loops are a transient conditions of hot loops? New state-of-art methods, like differential emission measure tomography (DEMT), have proposed a new classification of coronal loops based on whether the temperature increases or decreases with height (Huang et al., 2012). A real progress in the insight into coronal loops is expected from the study of large samples of loops or of loop populations. Systematic studies of coronal loops suffer from the problem of the sample selection and loop identification, because, for instance, loops in active regions overlap along the line sight. Attempts of systematic studies have been performed in the past on Yohkoh and TRACE data (e.g., Porter and Klimchuk, 1995;. A large number of loops were analyzed and it was possible to obtain meaningful statistics. However, it is difficult to generalize the results because of limited samples and/or selection effects, e.g., best observed loops, specific instrument. One basic problem for statistical studies of coronal loops is that it is very difficult to define an objective criterion for loop identification. In fact, loops are rarely isolated; they coexist with other loops that intersect or even overlap along the line of sight. This is especially true in active regions where most of the loops are found. In order to make a real progress along this line, we should obtain loop samples and populations selected on totally objective and unbiased criteria, which is difficult due to the problems outlined above. Some steps are coming in this direction (Aschwanden et al., 2013) and we will see results in the future.	5	Early attempts were based on morphological criteria, i.e., bright points, active-region loops, and large-scale structures (Vaiana et al., 1973, Figure 2), largely observed with instruments in the X-ray band. In addition to such classification, more recently, the observation of loops in different spectral bands and the suspicion that the difference lies not only in the band, but also in intrinsic properties, have stimulated another classification based on the temperature regime, i.e., cool, warm, hot loops (Table 2). Cool loops are generally detected in UV lines at temperatures between 10 5 and 10 6 K. They were first addressed by Foukal (1976) and later explored more with SoHO observations (Brekke et al., 1997). Warm loops are well observed by EUV imagers such as SoHO/EIT, TRACE, and in most channels of SDO/AIA, and confine plasma at temperature around 1 -1.5 MK (#CITATION_TAG et al., 1999). Hot loops are those typically observed in the X-ray band, and in hot UV and EUV lines (e.g., Fe xvi) and channels (SDO/AIA 335��), with temperatures around or above 2 MK (Table 1). These are the coronal loops already identified, for instance, in the early rocket missions (Vaiana et al., 1973). This distinction is not only due to observation with different instruments and in different bands, but there are hints that it may be more substantial and physical, i.e., there may be two or more classes of loops that may be governed by different regimes of physical processes.	o
CC587	The empirical phenomena described above strongly highlight the importance of real time dynamic interaction among people in generating the subject-pole to which beliefs can sensibly be attributed. The neural signature of collective speaking is found when speaking with a live speaker, but not with a recording (jasmin et al., in preparation). Live conversational partners become entangled not only in ways that fit a linguistic description (lexical priming, syntactic biasing, phonological, and phonetic imitation, #CITATION_TAG and Garrod, 2004), but in a host of subtle ways that have hitherto been treated of as non-linguistic. These include gaze, posture, gestures, and blinks, but this set might conceivably be considerably extended as researchers turn their attention more and more to physiological markers of interaction (Campbell, 2007;Richardson et al., 2007;Shockley et al., 2009;Cummins, 2012;Wagner et al., 2014). The voice is an important part of the means by which a collective perspective is established and maintained, but it is one among many. The interaction of voice and gaze may play a particularly strong role in allowing the protracted sustainment of conditions of joint attention, which appears as a possible foundation for the shared intentionality required to ground a human cultural world (Tomasello et al., 2005) 5 .	1	The empirical phenomena described above strongly highlight the importance of real time dynamic interaction among people in generating the subject-pole to which beliefs can sensibly be attributed. The neural signature of collective speaking is found when speaking with a live speaker, but not with a recording (jasmin et al., in preparation). Live conversational partners become entangled not only in ways that fit a linguistic description (lexical priming, syntactic biasing, phonological, and phonetic imitation, #CITATION_TAG and Garrod, 2004), but in a host of subtle ways that have hitherto been treated of as non-linguistic. These include gaze, posture, gestures, and blinks, but this set might conceivably be considerably extended as researchers turn their attention more and more to physiological markers of interaction (Campbell, 2007;Richardson et al., 2007;Shockley et al., 2009;Cummins, 2012;Wagner et al., 2014). The voice is an important part of the means by which a collective perspective is established and maintained, but it is one among many. The interaction of voice and gaze may play a particularly strong role in allowing the protracted sustainment of conditions of joint attention, which appears as a possible foundation for the shared intentionality required to ground a human cultural world (Tomasello et al., 2005) 5 .	v
CC66	Climate change has emerged as a significant scientific, social and economic challenge to society (IPCC, 2014). Understanding how climate change may evolve over the coming decades requires significant investment in research about carbon and how it cycles, through both living and nonliving states, (Smil, 1996). Scientists frequently study these biogeochemical cycles in the context of subsystems such as the terrestrial biosphere (land-based living systems), oceanic systems (both organic and inorganic forms of carbon), and the atmosphere (Falkowski et al., 2000). These investigations may also include the specific role humans play in the carbon cycle, such as the impact of human-generated emissions or the consequences of climate change to agriculture and food systems (Berthelot et al., 2002;Bradbear and Friel, 2013;#CITATION_TAG et al., 2014;Shindell et al., 2012). Carbon cycle science is relevant to a great many aspects of life as we know it: the condition of our environment, the quality of air we breathe, water resources, the food that we eat, and the energy we consume.	0	Climate change has emerged as a significant scientific, social and economic challenge to society (IPCC, 2014). Understanding how climate change may evolve over the coming decades requires significant investment in research about carbon and how it cycles, through both living and nonliving states, (Smil, 1996). Scientists frequently study these biogeochemical cycles in the context of subsystems such as the terrestrial biosphere (land-based living systems), oceanic systems (both organic and inorganic forms of carbon), and the atmosphere (Falkowski et al., 2000). These investigations may also include the specific role humans play in the carbon cycle, such as the impact of human-generated emissions or the consequences of climate change to agriculture and food systems (Berthelot et al., 2002;Bradbear and Friel, 2013;#CITATION_TAG et al., 2014;Shindell et al., 2012). Carbon cycle science is relevant to a great many aspects of life as we know it: the condition of our environment, the quality of air we breathe, water resources, the food that we eat, and the energy we consume.	s
CC937	"Cancer progression is caused by the sequential accumulation of somatic mutations, including changes in copy number (structural variants), single nucleotides (SNP variants) and DNA methylation patterns during the life of an individual [1][2][3]. Among the mutations causally responsible for the development of cancer (drivers) not all possible orders of accumulation seem equally likely, and the fixation of some mutations can depend on the presence of other mutations. For example, in colorectal cancer APC mutations are an early event that precedes mutations in KRAS [4][5]#CITATION_TAG. Understanding the restrictions in the temporal order of accumulation of driver mutations not only provides insights into cancer biology, but can help identify early markers of disease as well as therapeutic targets [5][6][7][8][9], and can be an instrumental tool in the search for the ""Achilles\"" Heel"" of oncogene addiction [3,10,11]. In addition, understanding the correct order of events is necessary for the assessment of the validity of the genetic context of cell lines and animal models of human cancer [7,8]."	0	"Cancer progression is caused by the sequential accumulation of somatic mutations, including changes in copy number (structural variants), single nucleotides (SNP variants) and DNA methylation patterns during the life of an individual [1][2][3]. Among the mutations causally responsible for the development of cancer (drivers) not all possible orders of accumulation seem equally likely, and the fixation of some mutations can depend on the presence of other mutations. For example, in colorectal cancer APC mutations are an early event that precedes mutations in KRAS [4][5]#CITATION_TAG. Understanding the restrictions in the temporal order of accumulation of driver mutations not only provides insights into cancer biology, but can help identify early markers of disease as well as therapeutic targets [5][6][7][8][9], and can be an instrumental tool in the search for the ""Achilles\"" Heel"" of oncogene addiction [3,10,11]. In addition, understanding the correct order of events is necessary for the assessment of the validity of the genetic context of cell lines and animal models of human cancer [7,8]."	r
CC2485	The scale-space representation that is adopted involves an acyclic graph, or tree. Tree geometry has been used for reduced modeling of turbulence intermittency [2,4,5,#CITATION_TAG,22], in some instances formulated as generalizations of the zero-dimensional shell models [6,15] in which the scale space of turbulence is parameterized by wavenumber modulus. In such formulations, the system state consists of real variables representing velocity values that reside at each node of the tree and are time advanced. In the present approach, the system state consists of thermochemical state variables (and optionally velocity; see Sect. 5.1) associated with fluid parcels that reside at tree endpoints (the nodes at the base of the tree, below which there are no further links or nodes). Variables residing at other nodes of the tree are auxiliary variables in the present context. They are used to specify operations performed on the tree structure as time advances. In this regard, the formulation is fundamentally different from previously formulated hierarchical models in which dynamical variables at all levels, representing either filtered quantities or terms in a notional mode decomposition, are time advanced.	0	The scale-space representation that is adopted involves an acyclic graph, or tree. Tree geometry has been used for reduced modeling of turbulence intermittency [2,4,5,#CITATION_TAG,22], in some instances formulated as generalizations of the zero-dimensional shell models [6,15] in which the scale space of turbulence is parameterized by wavenumber modulus. In such formulations, the system state consists of real variables representing velocity values that reside at each node of the tree and are time advanced. In the present approach, the system state consists of thermochemical state variables (and optionally velocity; see Sect. 5.1) associated with fluid parcels that reside at tree endpoints (the nodes at the base of the tree, below which there are no further links or nodes). Variables residing at other nodes of the tree are auxiliary variables in the present context.	r
CC731	The bigest strength of our analysis is the utilisation of efficacy data derived from a systematic literature review and NMA. This methodology enabled us to consider information from both direct and indirect comparisons between interventions, and allowed simultaneous inference on all treatment options examined in trial pair-wise comparisons while preserving randomisation [#CITATION_TAG,48]. This approach for evidence synthesis is essential for populating model-based economic studies assessing more than two competing interventions. The NMA principally utilised continuous data to estimate the relative treatment effects of interventions, and then transformed the estimated SMDs into probabilities of recovery. Such a transformation is valid as long as the assumed relationship between the treatment effect based on continuous data and the treatment effect estimated using recovery data holds. This assumption could not be checked for all interventions, but available data indicated a strong relationship and therefore this transformation is unlikely to have introduced substantial bias into the analysis [14]. These assumptions along with the limitations of the NMA model and the limitations of the RCTs considered in the NMA [14] may have impacted on the quality of the respective input parameters used to populate the economic model.	5	The bigest strength of our analysis is the utilisation of efficacy data derived from a systematic literature review and NMA. This methodology enabled us to consider information from both direct and indirect comparisons between interventions, and allowed simultaneous inference on all treatment options examined in trial pair-wise comparisons while preserving randomisation [#CITATION_TAG,48]. This approach for evidence synthesis is essential for populating model-based economic studies assessing more than two competing interventions. The NMA principally utilised continuous data to estimate the relative treatment effects of interventions, and then transformed the estimated SMDs into probabilities of recovery. Such a transformation is valid as long as the assumed relationship between the treatment effect based on continuous data and the treatment effect estimated using recovery data holds.	h
CC1235	Learning is a latent variable, typically measured as academic performance in assessment work and examinations (Mislevy, Behrens, & Dicerbo, 2012). Factors affecting academic performance have been the focus of research for many years (Farsides & Woodfield, 2003;Lent, Brown, & Hacket, 1994;Moran & Crowley, 1979). It remains an active research topic (Buckingham Shum & Deakin Crick, 2012;Cassidy, 2011;Komarraju, Ramsey & Rinella, 2013), indicating the inherent difficulty in both measurement of learning (Knight, Buckinham Shum, & Littleton, 2013;Tempelaar et al., 2013), and modelling the learning process, particularly in tertiary education (Pardos et al., 2011). Cognitive ability remains an important determinant of academic performance (Cassidy, 2011), often measured as prior academic ability. Demographic data, such as age and gender, have been cited as significant (Naderi et al., 2009), as are data gathered from learner activity on online learning systems (Bayer et al., 2012;L�_pez et al., 2012). In addition to the data systematically gathered by providers, other factors can be measured prior to commencing tertiary education, which could be useful in modelling learner academic performance. For example, models predicting academic performance that include factors of motivation (e.g., self-efficacy, goal setting) with cognitive ability yield a lower error variance than models of cognitive ability alone, particularly at tertiary level (reviewed in Boekaerts, 2001;Robbins et al., 2004). Research into personality traits, specifically the BIG 5 factors of openness, conscientiousness, extroversion, agreeableness, and neuroticism, and their impact on academic achievement in tertiary education, suggests some personality factors are indicative of potential academic achievement (Chamorro-Premuzic & Furnham, 2004#CITATION_TAG et al., 2012). For example conscientiousness, which is associated with persistence and self-discipline (Chamorro-Premuzic & Furnham, 2004), is correlated with academic performance, but not with IQ, suggesting conscientiousness may compensate for lower ability (Chamorro-Premuzic & Furnham, 2008). Openness, which is associated with curiosity, can be indicative of a deep learning style (Swanberg & Martinsen, 2010). Learning style (deep or shallow) and selfregulated learning strategies are also relevant, and have been shown to mediate between other factors (such as factors of personality and factors of motivation) and academic performance (Biggs et al., 2001;Entwhistle, 2005;Swanberg & Martinsen, 2010). This paper reviews a range of psychometric factors that could be used to predict academic performance in tertiary education (section 2). It lays emphasis on factors that can be measured prior to, or during learner enrolment in tertiary education programmes. The unique focus is to facilitate, and inform, early engagement with students potentially at risk of failing (e.g., Arnold & Pistilli, 2012;Laur�_a et al., 2013). Furthermore, results from learner profiling during student induction can provide useful feedback to the learner on preferred approaches to learning tasks, and development of a personalized learning environment. A review of pertinent data analysis techniques is presented in section 3, with an emphasis on empirical modelling approaches prevalent in educational data mining. Section 4 outlines the benefits of greater collaboration between educational psychology and learning analytics.	0	Demographic data, such as age and gender, have been cited as significant (Naderi et al., 2009), as are data gathered from learner activity on online learning systems (Bayer et al., 2012;L�_pez et al., 2012). In addition to the data systematically gathered by providers, other factors can be measured prior to commencing tertiary education, which could be useful in modelling learner academic performance. For example, models predicting academic performance that include factors of motivation (e.g., self-efficacy, goal setting) with cognitive ability yield a lower error variance than models of cognitive ability alone, particularly at tertiary level (reviewed in Boekaerts, 2001;Robbins et al., 2004). Research into personality traits, specifically the BIG 5 factors of openness, conscientiousness, extroversion, agreeableness, and neuroticism, and their impact on academic achievement in tertiary education, suggests some personality factors are indicative of potential academic achievement (Chamorro-Premuzic & Furnham, 2004#CITATION_TAG et al., 2012). For example conscientiousness, which is associated with persistence and self-discipline (Chamorro-Premuzic & Furnham, 2004), is correlated with academic performance, but not with IQ, suggesting conscientiousness may compensate for lower ability (Chamorro-Premuzic & Furnham, 2008). Openness, which is associated with curiosity, can be indicative of a deep learning style (Swanberg & Martinsen, 2010). Learning style (deep or shallow) and selfregulated learning strategies are also relevant, and have been shown to mediate between other factors (such as factors of personality and factors of motivation) and academic performance (Biggs et al., 2001;Entwhistle, 2005;Swanberg & Martinsen, 2010).	h
CC1132	"One conceptual possibility is that momentary shifts in incentives, in the form of actual or anticipated affect regarding the direction of attention, control the person""s executive processes with corresponding shifts between external perception or action and the fostering of thoughts. The decision function could be similar to that operating in choices among alternative actions (e.g., #CITATION_TAG et al., 2005;Tobler et al., 2005). Rather than a failure of executive control, this would entail a flexible executive that moves attention to whatever focus appears at the moment optimal not only for task performance but also for brain refreshment and for all of the other goals on the individual""s agenda. The model proposed by Spreng et al. (2010), with a frontoparietal control network that flexibly joins with other networks to selectively empower their functions, seems compatible with this view."	0	"One conceptual possibility is that momentary shifts in incentives, in the form of actual or anticipated affect regarding the direction of attention, control the person""s executive processes with corresponding shifts between external perception or action and the fostering of thoughts. The decision function could be similar to that operating in choices among alternative actions (e.g., #CITATION_TAG et al., 2005;Tobler et al., 2005). Rather than a failure of executive control, this would entail a flexible executive that moves attention to whatever focus appears at the moment optimal not only for task performance but also for brain refreshment and for all of the other goals on the individual""s agenda. The model proposed by Spreng et al. (2010), with a frontoparietal control network that flexibly joins with other networks to selectively empower their functions, seems compatible with this view."	h
CC1472	"Cue Gerd Gigerenzer (2008). Gigerenzer and colleagues have skillfully provided an account of how recruiting heuristical and biased reasoning strategies instead of more complex and thorough reasoning strategies might, overall, be a good idea. I imagine some readers will already have thought of a reason to rely on heuristics: they are more economical than the alternatives! And if there is a limited economy of cognitive energy, then recruiting cognitively efficient reasoning strategies is rational, right? This is roughly how Tversky and Kahneman (1974) and their ilk explain the use of heuristics and biases. This, however, is not Gigerenzer and colleagues"" claim. With Henry Brighton (2009) and Wolfgang Gaissmaier (2011), Gigerenzer demonstrates that recruiting heuristics might be more mathematically rational, and not just more economically rational, then recruiting their alternatives. First, Gigerenzer and colleagues introduce their audience to a series of strategies, each of which is ""biased"" to varying degrees, that can be used to make estimations under conditions of uncertaintye.g., the ""take the best"" strategy (Gigerenzer and Goldstein 1996) inspired by statistical models using ""equal weights"" or ""tallying"" strategies (Dawes 1974, #CITATION_TAG and Corrigan 1974, Einhorn and Hogarth 1975, Schmidt 1971. Then Gigerenzer and colleagues show how the differential performance of these strategies can be modeled computationally, allowing for a quantitative adjudication between strategies. Comparing these estimation strategies across multiple data sets reveals that simpler-or more biased-strategies actually outperform models that are more thorough and sensitive to variance in the evidence-i.e., variance in the sample data set (Chater et al 2003, Goldstein and Gigerenzer 2002, Gigerenzer and Gaissmaier 2011, and Schooler and Hertwig 2005. If these differentially biased models of estimation strategies are taken to be analogous to certain kinds of reasoning, then what Gigerenzer and Brighton show is that recruiting biased reasoning strategies-e.g., heuristics-might actually be more rational than recruiting unbiased strategies. Not coincidentally, these simulations seem to translate into ecologically valid human reasoning tasks (Hertwig and Todd 2003, meaning, simple-is-best or ""less-is-more"" cognitive strategies-where reasoners unconsciously estimate rather than fully calculate-are not only economically rational, but mathematically rational as well."	5	"This is roughly how Tversky and Kahneman (1974) and their ilk explain the use of heuristics and biases. This, however, is not Gigerenzer and colleagues"" claim. With Henry Brighton (2009) and Wolfgang Gaissmaier (2011), Gigerenzer demonstrates that recruiting heuristics might be more mathematically rational, and not just more economically rational, then recruiting their alternatives. First, Gigerenzer and colleagues introduce their audience to a series of strategies, each of which is ""biased"" to varying degrees, that can be used to make estimations under conditions of uncertaintye.g., the ""take the best"" strategy (Gigerenzer and Goldstein 1996) inspired by statistical models using ""equal weights"" or ""tallying"" strategies (Dawes 1974, #CITATION_TAG and Corrigan 1974, Einhorn and Hogarth 1975, Schmidt 1971. Then Gigerenzer and colleagues show how the differential performance of these strategies can be modeled computationally, allowing for a quantitative adjudication between strategies. Comparing these estimation strategies across multiple data sets reveals that simpler-or more biased-strategies actually outperform models that are more thorough and sensitive to variance in the evidence-i.e., variance in the sample data set (Chater et al 2003, Goldstein and Gigerenzer 2002, Gigerenzer and Gaissmaier 2011, and Schooler and Hertwig 2005. If these differentially biased models of estimation strategies are taken to be analogous to certain kinds of reasoning, then what Gigerenzer and Brighton show is that recruiting biased reasoning strategies-e.g., heuristics-might actually be more rational than recruiting unbiased strategies."	G
CC2317	Recent studies have suggested that macrophage expression of Programmed Death Ligand (PDL)-1 is important in regulating T cell responses to influenza infection [7]. PDL1 is the ligand for the Programmed Cell Death (PD) receptor 1, which is a member of the CD28 family of T cell receptors with CD80 and CD86 being ligands for CD28. In the standard model of T cell receptor (TCR) activation, activation of CD28 provides a necessary co-stimulation to prevent T cell anergy [8]. In contrast, binding of PDL1 to PD1 causes inhibition of TCR-mediated phosphatidylinositol-3-kinase (PI3Kinase) activation leading to inhibition of T cell proliferation and cytokine release [9]. The increased expression and activation of the PD1/PDL1 axis in chronic viral infections such as HIV and Hepatitis C (HCV) can lead to progressive loss of T cell function [10,11]. However, it was only recently that a role for this PD1/PDL1 pathway has been elucidated in the control of immune function in acute infections and increased PDL1 expression in response to pathogens was demonstrated to be crucial for impairment of CD8 cytotoxicity [7] and the development of regulatory T cells [12]. We have recently demonstrated that CD4 cytotoxic T cells play an important role in protecting against severe influenza infection. This work demonstrated an additional role for MHC class II expressing cells and T helper cell responses #CITATION_TAG highlighting the potential importance of macrophage-T cell interactions in the control of influenza infection.	3	The increased expression and activation of the PD1/PDL1 axis in chronic viral infections such as HIV and Hepatitis C (HCV) can lead to progressive loss of T cell function [10,11]. However, it was only recently that a role for this PD1/PDL1 pathway has been elucidated in the control of immune function in acute infections and increased PDL1 expression in response to pathogens was demonstrated to be crucial for impairment of CD8 cytotoxicity [7] and the development of regulatory T cells [12]. We have recently demonstrated that CD4 cytotoxic T cells play an important role in protecting against severe influenza infection. This work demonstrated an additional role for MHC class II expressing cells and T helper cell responses #CITATION_TAG highlighting the potential importance of macrophage-T cell interactions in the control of influenza infection.	r
CC757	In this paper we propose to relax the assumption of rationally responsive expectations and to replace it by one of fulfilled equilibrium expectations. This concept was first proposed by #CITATION_TAG and Shapiro (1985). Katz and Shapiro (1985) assume that first consumers form expectations about network sizes, then firms compete (in their Cournot model by setting quantities), and finally consumers make optimal subscription or purchasing decisions, given the expectations. These decisions then lead to actual market shares and network sizes.	5	In this paper we propose to relax the assumption of rationally responsive expectations and to replace it by one of fulfilled equilibrium expectations. This concept was first proposed by #CITATION_TAG and Shapiro (1985). Katz and Shapiro (1985) assume that first consumers form expectations about network sizes, then firms compete (in their Cournot model by setting quantities), and finally consumers make optimal subscription or purchasing decisions, given the expectations. These decisions then lead to actual market shares and network sizes.	h
CC2027	The current migration of dunes on Cape Cod is inferred to reflect a legacy of landscape disturbance, specifically forest clear-cutting, grazing and agricultural practices, associated with European settlement starting in the early 17th century and continuing into the 20th century (McCaffrey and Stilgoe, 1981;Rubertone, 1985;Motzkin et al., 2002;Eberhardt et al., 2003;Forman et al., 2008). Other factors such as storminess, hurricane-force winds (Bosse et al., 2001;Eberhardt et al., 2003) and forest fires (Motzkin et al., 2002;Parshall et al., 2003) may have contributed to historic disturbance of this dune landscape. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009;#CITATION_TAG and Hall, 2009). However, uncertainty remains if periods of increased storminess and/or hurricane landfalls for the exposed Cape Cod spit during the Holocene (Mann et al., 2009;Toomey et al., 2013) were of sufficient magnitude to disturb this forest ecosystem and reactivated dunes. Proxy records of hurricane occurrence from coastal overwash timeseries (e.g., Donnelly et al., 2001;Scileppi and Donnelly, 2007) and from marine sediment cores extracted strategically along the Bahama Bank (Williams, 2013) to reflect wave climate indicate heightened hurricane activity in the North Atlantic Ocean between 4900 and 3600, 2500, and 1000, and 600 and 400 years BP (Mann et al., 2009;Toomey et al., 2013).	0	The current migration of dunes on Cape Cod is inferred to reflect a legacy of landscape disturbance, specifically forest clear-cutting, grazing and agricultural practices, associated with European settlement starting in the early 17th century and continuing into the 20th century (McCaffrey and Stilgoe, 1981;Rubertone, 1985;Motzkin et al., 2002;Eberhardt et al., 2003;Forman et al., 2008). Other factors such as storminess, hurricane-force winds (Bosse et al., 2001;Eberhardt et al., 2003) and forest fires (Motzkin et al., 2002;Parshall et al., 2003) may have contributed to historic disturbance of this dune landscape. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009;#CITATION_TAG and Hall, 2009). However, uncertainty remains if periods of increased storminess and/or hurricane landfalls for the exposed Cape Cod spit during the Holocene (Mann et al., 2009;Toomey et al., 2013) were of sufficient magnitude to disturb this forest ecosystem and reactivated dunes. Proxy records of hurricane occurrence from coastal overwash timeseries (e.g., Donnelly et al., 2001;Scileppi and Donnelly, 2007) and from marine sediment cores extracted strategically along the Bahama Bank (Williams, 2013) to reflect wave climate indicate heightened hurricane activity in the North Atlantic Ocean between 4900 and 3600, 2500, and 1000, and 600 and 400 years BP (Mann et al., 2009;Toomey et al., 2013).	m
CC2250	It has been long claimed that coronal loops consist of bundles of thin strands, to scales below the instrumental resolution (e.g., G�_mez et al., 1993). The issue of fine loop structure is of critical importance because it constrains the elementary processes that determine the loop ignition. The task to investigate this substructuring is not easy. Studies based both on models and on analysis of observations independently suggest that elementary loop components should be very fine with typical cross-sections of the strands on the order of 10 -100 km (Beveridge et al., 2003;Cargill and Klimchuk, 2004;Vekstein, 2009). First limited evidence of fine structuring was the low filling factor inferred for loops observed with NIXT (#CITATION_TAG et al., 1999, see Section 3.3.2). The high spatial resolution achieved by the TRACE normal-incidence telescope allowed to address the transverse structure of the imaged coronal loops. EUV images visibly show that coronal loops are substructured (Figure 5).	5	The issue of fine loop structure is of critical importance because it constrains the elementary processes that determine the loop ignition. The task to investigate this substructuring is not easy. Studies based both on models and on analysis of observations independently suggest that elementary loop components should be very fine with typical cross-sections of the strands on the order of 10 -100 km (Beveridge et al., 2003;Cargill and Klimchuk, 2004;Vekstein, 2009). First limited evidence of fine structuring was the low filling factor inferred for loops observed with NIXT (#CITATION_TAG et al., 1999, see Section 3.3. 2). The high spatial resolution achieved by the TRACE normal-incidence telescope allowed to address the transverse structure of the imaged coronal loops. EUV images visibly show that coronal loops are substructured (Figure 5).	t
CC2515	As far as bioactivity of marine oligosaccharides is concerned it is evident that more and more articles are appearing in literature in recent years. Few years ago some authors summarized the bioactivity of oligosaccharides derived from seaweed in stimulating defense responses and protection against pathogens in plants. In particular seaweed oligosaccharides were able to mediate to a series of actions, such as (i) expression of genes for antifungal and antibacterial protein production, (ii) activation of other defense enzymes and (iii) production of active natural products (Weinberger et al., 2010;#CITATION_TAG et al., 2011), all converging toward enhanced protection against pathogens. The antioxidant activities of three types of marine oligosaccharides from alginate, chitosan and fucoidan, were investigated using several antioxidant assays with mixtures of compounds of average molecular weight of ca. 5000 Da corresponding to 20-25 monosaccharide units, obtained by enzymatic hydrolysis of corresponding polymers. Intriguingly, the results showed that these oligosaccharides exhibited different activities in various assays (Wang et al., 2007) in relation to their structures. Attenuation of neurotoxicity induced by amyloid protein and hydrogen superoxide in human neuroblastoma cells has been reported for acidic oligosaccharide obtained from brown algae Ecklonia kurome by depolimerization. Amino and hydroxyl groups attached to free positions of the pyranoses rings can react with unstable free radicals to form stable macromolecule chelating metal ions. Alginate-derived oligosaccharides of 373-571 Da and chitooligosaccharides of 855-1671 Da obtained by enzymolysis with alginate lyase and chitosanase respectively, were investigated for cell regulation, erythrocytes haemolysis inhibition and antioxidant capacity (Hu et al., 2004). Others stressed on the anti-UV radiation potential of both alginate-derived oligosaccharides and chito-oligosaccharides and discussed the potential for development of UV radiation protector agent in the area of functional foods (He et al., 2013). Particular attention has been reserved to fucoidans for complex structures and for bioactivity they revealed. Studies to improve polymer extraction efficiency are of current interest (Rodriguez-Jasso et al., 2011) as well as those related to enzymes (Table 1) such as the fucoidanase reported from Lambis possessing endo-1,4 cleaving type action (Silchenko et al., 2014) that will be of interest as a tool for structure determination and for the access to specific products as also previously indicated (Pomin et al., 2005).	0	As far as bioactivity of marine oligosaccharides is concerned it is evident that more and more articles are appearing in literature in recent years. Few years ago some authors summarized the bioactivity of oligosaccharides derived from seaweed in stimulating defense responses and protection against pathogens in plants. In particular seaweed oligosaccharides were able to mediate to a series of actions, such as (i) expression of genes for antifungal and antibacterial protein production, (ii) activation of other defense enzymes and (iii) production of active natural products (Weinberger et al., 2010;#CITATION_TAG et al., 2011), all converging toward enhanced protection against pathogens. The antioxidant activities of three types of marine oligosaccharides from alginate, chitosan and fucoidan, were investigated using several antioxidant assays with mixtures of compounds of average molecular weight of ca. 5000 Da corresponding to 20-25 monosaccharide units, obtained by enzymatic hydrolysis of corresponding polymers. Intriguingly, the results showed that these oligosaccharides exhibited different activities in various assays (Wang et al., 2007) in relation to their structures.	 
CC2256	It has been long claimed that coronal loops consist of bundles of thin strands, to scales below the instrumental resolution (e.g., G�_mez et al., 1993). The issue of fine loop structure is of critical importance because it constrains the elementary processes that determine the loop ignition. The task to investigate this substructuring is not easy. Studies based both on models and on analysis of observations independently suggest that elementary loop components should be very fine with typical cross-sections of the strands on the order of 10 -100 km (Beveridge et al., 2003;Cargill and Klimchuk, 2004;#CITATION_TAG, 2009). First limited evidence of fine structuring was the low filling factor inferred for loops observed with NIXT (Di Matteo et al., 1999, see Section 3.3.2). The high spatial resolution achieved by the TRACE normal-incidence telescope allowed to address the transverse structure of the imaged coronal loops. EUV images visibly show that coronal loops are substructured (Figure 5).	5	It has been long claimed that coronal loops consist of bundles of thin strands, to scales below the instrumental resolution (e.g., G�_mez et al., 1993). The issue of fine loop structure is of critical importance because it constrains the elementary processes that determine the loop ignition. The task to investigate this substructuring is not easy. Studies based both on models and on analysis of observations independently suggest that elementary loop components should be very fine with typical cross-sections of the strands on the order of 10 -100 km (Beveridge et al., 2003;Cargill and Klimchuk, 2004;#CITATION_TAG, 2009). First limited evidence of fine structuring was the low filling factor inferred for loops observed with NIXT (Di Matteo et al., 1999, see Section 3.3. 2). The high spatial resolution achieved by the TRACE normal-incidence telescope allowed to address the transverse structure of the imaged coronal loops.	d
CC2953	Second, the hippocampus appears to be critically involved in declarative memory systems and, in humans at least, in mental time travel generally. Loss of hippocampal function in humans results in severe amnesia, including an apparent inability to imagine possible future events as well as failure to recall past ones (Hassabis et al., 2007a,b;#CITATION_TAG et al., 2010;Race et al., 2011). Conversely, the hippocampus is activated in neurologically intact individuals when they bring to mind past episodes and imagine possible ones As suggested earlier, the hippocampus appears to be the hub of the system, drawing detailed information from other regions of the brain, including the default-mode network (Addis et al., 2007), for the reconstruction of past or future events. There is some differentiation along the long axis of the hippocampus, with the posterior hippocampus more involved in storage and the retrieval of past episodes and the anterior hippocampus more activated by the imagining of future ones (Szpunar et al., 2007;Martin et al., 2011).	2	Second, the hippocampus appears to be critically involved in declarative memory systems and, in humans at least, in mental time travel generally. Loss of hippocampal function in humans results in severe amnesia, including an apparent inability to imagine possible future events as well as failure to recall past ones (Hassabis et al., 2007a,b;#CITATION_TAG et al., 2010;Race et al., 2011). Conversely, the hippocampus is activated in neurologically intact individuals when they bring to mind past episodes and imagine possible ones As suggested earlier, the hippocampus appears to be the hub of the system, drawing detailed information from other regions of the brain, including the default-mode network (Addis et al., 2007), for the reconstruction of past or future events. There is some differentiation along the long axis of the hippocampus, with the posterior hippocampus more involved in storage and the retrieval of past episodes and the anterior hippocampus more activated by the imagining of future ones (Szpunar et al., 2007;Martin et al., 2011).	o
CC1594	"In buyer-seller relationships some assets possess value in only this focal relationship and thus require relationship-specific investments. These can be consciously-made specific investments into physical or site-specific assets (#CITATION_TAG & Bechtel, 2002;Haugland, 1999;Heide, 1994) or can be unconsciously developed relationship-specific assets like workforce skills (S�_llner, 1999) or trust in the partner. Dissolution of the current relationship necessitates actors investing in new relationship-specific assets in new relationships. These investments together with relationship termination costs and search costs for new partners represent an economic actor""s switching costs (Bendapudi & Berry, 1997). They tend to force partners to stay within buyer-seller relationships as the ""have to"" bonding mechanism."	1	"In buyer-seller relationships some assets possess value in only this focal relationship and thus require relationship-specific investments. These can be consciously-made specific investments into physical or site-specific assets (#CITATION_TAG & Bechtel, 2002;Haugland, 1999;Heide, 1994) or can be unconsciously developed relationship-specific assets like workforce skills (S�_llner, 1999) or trust in the partner. Dissolution of the current relationship necessitates actors investing in new relationship-specific assets in new relationships. These investments together with relationship termination costs and search costs for new partners represent an economic actor""s switching costs (Bendapudi & Berry, 1997). They tend to force partners to stay within buyer-seller relationships as the ""have to"" bonding mechanism."	h
CC1609	"Following the logic of genetic drift, in cultural drift, variation is the result of random copying of cultural attributes, with some possibility of innovation, and the results of the process depend solely on the innovation rate and the effective population size, itself dependent on the scale of interaction. It is very unlikely that any individual act of copying, for example, of a ceramic decorative motif, will be random, but if everyone has their own reasons for copying one person rather than another, the result will be that there are no directional forces affecting what or who is copied. Neiman""s original case study indicated that patterning in the rim attributes of eastern North American Woodland period pottery was a result of drift, but Shennan & Wilkinson (2001) showed that patterning in the frequency of decorative attributes of early Neolithic pottery from a small region of Germany indicated a pronovelty bias in the later periods and #CITATION_TAG et al. (2004) in a case study from the U.S. Southwest were able to show a departure in the direction of conformity. Thus, these methods do provide a potential basis for distinguishing some of the transmission forces postulated by DIT. All these studies followed Neiman in using an assemblage diversity measure to identify drift, but subsequently Bentley & colleagues (2004) also showed that the frequencies of different variants resulting from a random copying process followed a power law, with a small number of the variants attaining very high frequencies but most occurring only very few times. In such cases, although one can predict that a small number of variants will attain very high frequencies, it is impossible to predict which ones. It is increasingly clear that such processes occur in an enormous range of phenomena and follow universal laws . Eerkens & Lipo (2005) have developed a similar approach to the characterization of neutral variation in continuous measurements and the measurement of departures from it. They applied it to explaining variation in projectile point dimensions in Owens Valley and in Illinois Woodland ceramic vessel diameters. They showed that drift was sufficient to explain the variation in projectile point thickness, but base width showed less variation than expected, so some biasing process leading to a reduction in variation must have been operating while, in the case of the pottery vessel diameters, variationincreasing mechanisms were at work."	1	"Following the logic of genetic drift, in cultural drift, variation is the result of random copying of cultural attributes, with some possibility of innovation, and the results of the process depend solely on the innovation rate and the effective population size, itself dependent on the scale of interaction. It is very unlikely that any individual act of copying, for example, of a ceramic decorative motif, will be random, but if everyone has their own reasons for copying one person rather than another, the result will be that there are no directional forces affecting what or who is copied. Neiman""s original case study indicated that patterning in the rim attributes of eastern North American Woodland period pottery was a result of drift, but Shennan & Wilkinson (2001) showed that patterning in the frequency of decorative attributes of early Neolithic pottery from a small region of Germany indicated a pronovelty bias in the later periods and #CITATION_TAG et al. (2004) in a case study from the U.S. Southwest were able to show a departure in the direction of conformity. Thus, these methods do provide a potential basis for distinguishing some of the transmission forces postulated by DIT. All these studies followed Neiman in using an assemblage diversity measure to identify drift, but subsequently Bentley & colleagues (2004) also showed that the frequencies of different variants resulting from a random copying process followed a power law, with a small number of the variants attaining very high frequencies but most occurring only very few times. In such cases, although one can predict that a small number of variants will attain very high frequencies, it is impossible to predict which ones."	i
CC2052	The sands analyzed have a SiO content of >90% of the noncarbonate fraction and are predominantly a moderately to well sorted medium to coarse sand with >80% quartz grains, reflecting the mineralogy of underlying Pleistocene outwash, a likely source for the eolian sand (Ockay and Hubert, 1996). The quartz fraction was isolated by density separations (at 2.55 and 2.70 g/cc) using the heavy liquid Na-polytungstate and a 40-min immersion in HF (40%) was applied to etch the outer ___10 ��m of grains, which is affected by alpha radiation (#CITATION_TAG and Christiansen, 1994). Quartz grains were rinsed finally in HCl (10%) to remove any insoluble fluorides. The purity of quartz separate was evaluated by petrographic inspection and point counting of a representative aliquot. Samples that showed >1% of non-quartz minerals were retreated with HF and rechecked petrographically and sieved again. In addition, the optical purity of quartz separates was tested by exposing aliquots to infrared excitation, which preferentially excites feldspar minerals or feldspathic inclusions (cf. Duller et al., 2003). Aliquots which showed infrared emissions appreciably above background counts (> counts/s) where omitted from equivalent dose calculations.	5	The sands analyzed have a SiO content of >90% of the noncarbonate fraction and are predominantly a moderately to well sorted medium to coarse sand with >80% quartz grains, reflecting the mineralogy of underlying Pleistocene outwash, a likely source for the eolian sand (Ockay and Hubert, 1996). The quartz fraction was isolated by density separations (at 2.55 and 2.70 g/cc) using the heavy liquid Na-polytungstate and a 40-min immersion in HF (40%) was applied to etch the outer ___10 ��m of grains, which is affected by alpha radiation (#CITATION_TAG and Christiansen, 1994). Quartz grains were rinsed finally in HCl (10%) to remove any insoluble fluorides. The purity of quartz separate was evaluated by petrographic inspection and point counting of a representative aliquot. Samples that showed >1% of non-quartz minerals were retreated with HF and rechecked petrographically and sieved again.	h
CC798	"In adults, !3-5 days of monitoring are normally considered appropriate, which is in accordance with recommendations given [2]. However, estimates of how many days of monitoring that should be included to obtain a reliable result vary considerably between studies [3-7, 2, 8], and might also vary between outcome variables of interest [6,8]. According to Matthews et al [6], inclusion of more days may be needed to arrive at reliable estimates (intraclass correlation coefficient (ICC) !0.80) for ""physical inactivity"" (<500 cpm from the Actigraph 7164) (!7 days), compared to PA (! 3-4 days). A comparable finding has been shown in older adults, where 2-3 days was needed for PA, whereas 5 days of monitoring was needed for SED (<50 cpm from the Actigraph 7164). The possible impaired reliability for SED compared to other variables may be of critical importance, given the increased interest in SED in the primary and secondary prevention of a range of chronic diseases as well as premature death [9][10][11]. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included !3-5 days of measurement [12][13][14][15][16][17], with some exceptions (!1 day #CITATION_TAG; !6-7 days [19,20]). Moreover, as SED are likely to be related to wear time, correction for wear time might improve reliability. Consistent with this hypothesis, percent SED has previously been shown to be superior to minutes of SED as a predictor of metabolic risk [18]."	0	The possible impaired reliability for SED compared to other variables may be of critical importance, given the increased interest in SED in the primary and secondary prevention of a range of chronic diseases as well as premature death [9][10][11]. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included ! 3-5 days of measurement [12][13][14][15][16][17], with some exceptions (! 1 day #CITATION_TAG; ! 6-7 days [19,20]). Moreover, as SED are likely to be related to wear time, correction for wear time might improve reliability. Consistent with this hypothesis, percent SED has previously been shown to be superior to minutes of SED as a predictor of metabolic risk [18].	T
CC1269	Learning is a latent variable, typically measured as academic performance in assessment work and examinations (Mislevy, Behrens, & Dicerbo, 2012). Factors affecting academic performance have been the focus of research for many years (Farsides & Woodfield, 2003;Lent, Brown, & Hacket, 1994;#CITATION_TAG & Crowley, 1979). It remains an active research topic (Buckingham Shum & Deakin Crick, 2012;Cassidy, 2011;Komarraju, Ramsey & Rinella, 2013), indicating the inherent difficulty in both measurement of learning (Knight, Buckinham Shum, & Littleton, 2013;Tempelaar et al., 2013), and modelling the learning process, particularly in tertiary education (Pardos et al., 2011). Cognitive ability remains an important determinant of academic performance (Cassidy, 2011), often measured as prior academic ability. Demographic data, such as age and gender, have been cited as significant (Naderi et al., 2009), as are data gathered from learner activity on online learning systems (Bayer et al., 2012;L�_pez et al., 2012). In addition to the data systematically gathered by providers, other factors can be measured prior to commencing tertiary education, which could be useful in modelling learner academic performance. For example, models predicting academic performance that include factors of motivation (e.g., self-efficacy, goal setting) with cognitive ability yield a lower error variance than models of cognitive ability alone, particularly at tertiary level (reviewed in Boekaerts, 2001;Robbins et al., 2004). Research into personality traits, specifically the BIG 5 factors of openness, conscientiousness, extroversion, agreeableness, and neuroticism, and their impact on academic achievement in tertiary education, suggests some personality factors are indicative of potential academic achievement (Chamorro-Premuzic & Furnham, 2004De Feyter et al., 2012). For example conscientiousness, which is associated with persistence and self-discipline (Chamorro-Premuzic & Furnham, 2004), is correlated with academic performance, but not with IQ, suggesting conscientiousness may compensate for lower ability (Chamorro-Premuzic & Furnham, 2008). Openness, which is associated with curiosity, can be indicative of a deep learning style (Swanberg & Martinsen, 2010). Learning style (deep or shallow) and selfregulated learning strategies are also relevant, and have been shown to mediate between other factors (such as factors of personality and factors of motivation) and academic performance (Biggs et al., 2001;Entwhistle, 2005;Swanberg & Martinsen, 2010). This paper reviews a range of psychometric factors that could be used to predict academic performance in tertiary education (section 2). It lays emphasis on factors that can be measured prior to, or during learner enrolment in tertiary education programmes. The unique focus is to facilitate, and inform, early engagement with students potentially at risk of failing (e.g., Arnold & Pistilli, 2012;Laur�_a et al., 2013). Furthermore, results from learner profiling during student induction can provide useful feedback to the learner on preferred approaches to learning tasks, and development of a personalized learning environment. A review of pertinent data analysis techniques is presented in section 3, with an emphasis on empirical modelling approaches prevalent in educational data mining. Section 4 outlines the benefits of greater collaboration between educational psychology and learning analytics.	0	Learning is a latent variable, typically measured as academic performance in assessment work and examinations (Mislevy, Behrens, & Dicerbo, 2012). Factors affecting academic performance have been the focus of research for many years (Farsides & Woodfield, 2003;Lent, Brown, & Hacket, 1994;#CITATION_TAG & Crowley, 1979). It remains an active research topic (Buckingham Shum & Deakin Crick, 2012;Cassidy, 2011;Komarraju, Ramsey & Rinella, 2013), indicating the inherent difficulty in both measurement of learning (Knight, Buckinham Shum, & Littleton, 2013;Tempelaar et al., 2013), and modelling the learning process, particularly in tertiary education (Pardos et al., 2011). Cognitive ability remains an important determinant of academic performance (Cassidy, 2011), often measured as prior academic ability. Demographic data, such as age and gender, have been cited as significant (Naderi et al., 2009), as are data gathered from learner activity on online learning systems (Bayer et al., 2012;L�_pez et al., 2012).	a
CC1774	P 2 1 equation). The existence of the needed solution to P 2 1 has been rigorously established in #CITATION_TAG and Vanlessen (2006). Moreover, it was argued in Dubrovin (2006) that this behavior is essentially independent on the choice of the Hamiltonian perturbation. Some of these universality conjectures have been partially confirmed by numerical analysis carried out in . More recently, the universality conjecture of Dubrovin (2006) has been proven in Claeys and Grava (2008) for solutions to the KdV equation with analytic initial data vanishing at infinity.	0	P 2 1 equation). The existence of the needed solution to P 2 1 has been rigorously established in #CITATION_TAG and Vanlessen (2006). Moreover, it was argued in Dubrovin (2006) that this behavior is essentially independent on the choice of the Hamiltonian perturbation. Some of these universality conjectures have been partially confirmed by numerical analysis carried out in . More recently, the universality conjecture of Dubrovin (2006) has been proven in Claeys and Grava (2008) for solutions to the KdV equation with analytic initial data vanishing at infinity.	h
CC249	"In recent years wet British summers have been reported frequently, with record-breaking rainfall in June in 2012 and a wet July following a wet spring, and the exceptionally rainy July in 2007, not record breaking in the monthly totals but with very intense shorter rain events within the month, which resulted in heavy summer floods in middle England and Wales. Although large scale flooding held off in 2012, the extreme rain caused damage to the harvest and put strain on infrastructure systems. However the heavy flooding in 2007, following very high daily and 5-day totals (with 3 top 12 5-day totals in July compared to the whole observed period, and the 5th highest total daily precipitation), was classified as a national disaster by the Environment Agency causing economic losses of approximately ��3 billion (Report Environment Agency UK, 2010). There is no significant trend in July rainfall extremes,however, as extremes are by definition rare, a trend would only be detectable if the risk of extremes changed dramatically. Trend analysis will thus not reveal subtle changes in extreme precipitation. In times of anthropogenic climate change with increasing global mean temperatures the question arises if the observed extreme events were just bad luck or whether heavy rainfall in summer, especially in July, is something we have reasons to expect in the future and should begin adapting to in terms of infrastructure planning? Even before a trend might be detectable, attribution analysis can reveal the influence of anthropogenic climate change on very rare events, and such is the aim of this paper. July precipitation on a timescale relevant for flooding will be the focus of this study. It is important to highlight here that precipitation in the relevant month, although important, is only one factor determining flood risk (Kay et al. 2011). We attempt to answer the question of how the odds of extreme July precipitation relevant for flooding have changed, not the odds of floods occurring in July. Increasingly, politicians, decision-makers and non-governmental organisations (NGOs) want to know whether and to what extent global human-influenced climate change is affecting localised extreme weather events, highlighted by the fact that the Conference of the Party (COP) meeting in Doha in November 2012 established a work programme specifically on loss and damage due to climate change (UNFCCC report FCCC/SBI/2012/29 2012), further developed into the ""Warsaw mechanism"" at the COP in 2013. Studies into the European heat wave of 2003(Stott et al. 2004, the England and Wales floods of 2000 (Pall et al. 2011), and the Russian heat wave of 2010 (Dole et al. 2011;Rahmstorf and Coumou 2011;Otto et al. 2012) have sought to determine to what extent the risks of these events occurring have changed because of anthropogenic greenhouse gas emissions, many of them using the emerging method of probabilistic event attribution (PEA). Under the assumption of an unchanging relationship between hazard and resulting damage, event probability can be seen as a proxy for risk (#CITATION_TAG et al. 2011). One of the most recent series of studies in this field (Peterson et al. 2012;Peterson et al. 2013) attempts to answer this question for several extreme events that occurred in 2011 and 2012, with one of the studies being an attribution study of exceptionally warm Novembers in central England (Massey et al. 2012), which explicitly applies PEA. It finds a 40 times increase in the risk of such warm Novembers occurring, when comparing simulated temperatures of the 2000s with the 1960s. Pall et al. (2011) compare river runoff derived from daily precipitation in the autumn of 2000, with an autumn 2000 in a world that might have been without anthropogenic greenhouse gas forcing and associated sea surface temperature warming. In our study we use a PEA approach combining both methods, as outlined in the following Section 2. In Section 3 the results are presented and analysed with emphasis on the quantification of uncertainty (see Section 2.4) and concluding remarks are made in Section 4."	0	"We attempt to answer the question of how the odds of extreme July precipitation relevant for flooding have changed, not the odds of floods occurring in July. Increasingly, politicians, decision-makers and non-governmental organisations (NGOs) want to know whether and to what extent global human-influenced climate change is affecting localised extreme weather events, highlighted by the fact that the Conference of the Party (COP) meeting in Doha in November 2012 established a work programme specifically on loss and damage due to climate change (UNFCCC report FCCC/SBI/2012/29 2012), further developed into the ""Warsaw mechanism"" at the COP in 2013. Studies into the European heat wave of 2003(Stott et al. 2004, the England and Wales floods of 2000 (Pall et al. 2011), and the Russian heat wave of 2010 (Dole et al. 2011;Rahmstorf and Coumou 2011;Otto et al. 2012) have sought to determine to what extent the risks of these events occurring have changed because of anthropogenic greenhouse gas emissions, many of them using the emerging method of probabilistic event attribution (PEA). Under the assumption of an unchanging relationship between hazard and resulting damage, event probability can be seen as a proxy for risk (#CITATION_TAG et al. 2011). One of the most recent series of studies in this field (Peterson et al. 2012;Peterson et al. 2013) attempts to answer this question for several extreme events that occurred in 2011 and 2012, with one of the studies being an attribution study of exceptionally warm Novembers in central England (Massey et al. 2012), which explicitly applies PEA. It finds a 40 times increase in the risk of such warm Novembers occurring, when comparing simulated temperatures of the 2000s with the 1960s. Pall et al. (2011) compare river runoff derived from daily precipitation in the autumn of 2000, with an autumn 2000 in a world that might have been without anthropogenic greenhouse gas forcing and associated sea surface temperature warming."	s
CC108	"With the prevalence of chronic illness rising worldwide, there is a need to engage patients in health promotion work in order to prevent further deterioration, to strengthen their health and their capacity to participate in society [1] [2]. Interventions based on such work will reflect the philosophical perspective of ""health within illness which holds that individuals living with long-term health problems are capable of experiencing health and wellbeing despite their conditions [3] [4]. Summaries of research concerning people with various long-term conditions show that they have much in common as they face the challenges of trying to live as well as possible within the context of physical, mental, or social discomfort and limitation [5]- [7]. However, patient education and wellness-interventions in the context of chronic illness are often specific to particular diagnostic groups and not designed to be applied across diagnostic categories [8] [9]. Two of the few examples of interventions that are practiced more broadly are the Chronic Disease Self-Management Education (CDSME) developed by Lorig and colleagues in the USA [10] and the Vitality Training Program (VTP) developed by Steen and Haugli [11] in Norway. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [12] [13]. Improved health behavior and health status were also reported in a group of patients with serious mental illness [14]. However, a longitudinal randomized trial of stroke survivors who accomplished CDSME, showed that the intervention did not appear to impact self-efficacy and failed to influence outcomes such as mood or social outcomes #CITATION_TAG. This was also confirmed in a Cochrane review that focused on the outcomes of CDSME [16]."	1	Two of the few examples of interventions that are practiced more broadly are the Chronic Disease Self-Management Education (CDSME) developed by Lorig and colleagues in the USA [10] and the Vitality Training Program (VTP) developed by Steen and Haugli [11] in Norway. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [12] [13]. Improved health behavior and health status were also reported in a group of patients with serious mental illness [14]. However, a longitudinal randomized trial of stroke survivors who accomplished CDSME, showed that the intervention did not appear to impact self-efficacy and failed to influence outcomes such as mood or social outcomes #CITATION_TAG. This was also confirmed in a Cochrane review that focused on the outcomes of CDSME [16].	,
CC2419	"The illegality of life-ending of minors, an empirical reality (#CITATION_TAG et al. 2005;Pousset et al. 2010), became somewhat less problematic when in February 2014 the Belgian parliament enacted an extension of the euthanasia law to ""capable"" adolescents (Moniteur Belge 2014). Similar to the concept of so-called ""Gillick"" competence (Wheeler 2006) in the English common law (where the cut-off is 16 years), for children under 12 their parents are entrusted with decisions, but requests of euthanasia by adolescents who are judged to be capable of making their own decisions can be honoured in the Netherlands since 2002. Only a handful are known to have occurred. After extensive hearings, a majority consensus emerged in the Belgian Senate that some children, especially after a long disease history with an illness such as leukaemia, could be considered competent to make valid requests if, after professional psychological consultation, so judged by the caregiving team. Euthanasia has become legal in case of refractory physical suffering if the minor is determined competent by a panel of professional caregivers and the parents agree. On the basis of a favourable experience with euthanasia in refractory suffering by mental illness, proponents had argued for also including mental illness. There is casuistic evidence that suicidal adult refractory psychiatric patients, after having been assured that a request of euthanasia might at some time in the future be honoured, refrained from further suicidal attempts and did not during a follow-up of several years demand euthanasia. The assurance of a desired death when they would judge the time had come apparently sufficed to keep them alive (Callebert, Van Audenhove, and De Coster 2012). For adolescent psychiatric patients it was surmised that the mere possibility of euthanasia could reduce the alarming rate of suicide among children and adolescents. In the end, refractory mental suffering was excluded because at the age of 18 a mental condition cannot have been treated long enough to be judged hopeless. Christian-democrat spokespersons opposed the extension, wanted a cut-off at age 15 and insisted on emphasising the relational dimension of endof-life decision-making, with more legally enforced involvement of the family and all caregivers. The law was enacted in February 2014, with 86 votes in favour, 44 opposed, and 12 abstentions, an almost identical majority as the 2002 law for adults, and most Christian-democrats and the extreme-right parties still dissenting. This extension of the law to capable adolescents has been denounced as a ""logical"" slippery slope effect in much of the world media. Critics often misunderstood or misrepresentated the actual changes to the law and disregarded that, everywhere, capable adolescents already can refuse further life-sustainng treatment, and do so frequently (Pousset et al. 2010). They probably also included people who would not have disputed the competency for self-determination of e.g. Anne Frank, another child who impressively matured under conditions of existential threat."	4	"The illegality of life-ending of minors, an empirical reality (#CITATION_TAG et al. 2005;Pousset et al. 2010), became somewhat less problematic when in February 2014 the Belgian parliament enacted an extension of the euthanasia law to ""capable"" adolescents (Moniteur Belge 2014). Similar to the concept of so-called ""Gillick"" competence (Wheeler 2006) in the English common law (where the cut-off is 16 years), for children under 12 their parents are entrusted with decisions, but requests of euthanasia by adolescents who are judged to be capable of making their own decisions can be honoured in the Netherlands since 2002. Only a handful are known to have occurred. After extensive hearings, a majority consensus emerged in the Belgian Senate that some children, especially after a long disease history with an illness such as leukaemia, could be considered competent to make valid requests if, after professional psychological consultation, so judged by the caregiving team."	T
CC2750	Recent research has stressed the importance of network structures in understanding business exchanges (Achrol, 1997;M�_ller & Rajala, 2007). These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;Porter, 1985) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980). Business networks have also been characterised in strategic marketing as value-creating systems (Parolini, 1999) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (M�_ller & Svahn, 2006;#CITATION_TAG & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998;Gulati, Nohria, & Zaheer, 2000).	0	These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;Porter, 1985) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980). Business networks have also been characterised in strategic marketing as value-creating systems (Parolini, 1999) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (M�_ller & Svahn, 2006;#CITATION_TAG & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998;Gulati, Nohria, & Zaheer, 2000).	n
CC2129	"The fractional quantum Hall (FQH) phases realized by two-dimensional electron gases in very large magnetic fields are among the most intriguing states of matter (see, for instance, #CITATION_TAG). In such systems, electrons ""bind"" to magnetic vortices, forming strongly correlated phases with striking properties, such as exotic excitations (""anyons"") which obey fractional statistics [2]. Analogous quantum Hall phases should also arise in cold atomic gases when they are set into fast rotation (see [3] and references therein). Due to the mathematical similarity between Coriolis and Lorentz forces, rotating neutral gases are indeed the exact analogue of an assembly of charged particles plunged in a magnetic field. Observing these highly correlated phases is one of the major goals in the field of trapped quantum gases [3,4]. This goal has, however, not yet been reached, because of the difficulty of communicating the required amount of angular momentum (on the order of N per atom, with N the number of particles) to the system [5,6]. In practice, the residual static trap anisotropy limits the total angular momentum to much smaller values, for which the rotating gas is well described by a mean field approach [7]."	4	"The fractional quantum Hall (FQH) phases realized by two-dimensional electron gases in very large magnetic fields are among the most intriguing states of matter (see, for instance, #CITATION_TAG). In such systems, electrons ""bind"" to magnetic vortices, forming strongly correlated phases with striking properties, such as exotic excitations (""anyons"") which obey fractional statistics [2]. Analogous quantum Hall phases should also arise in cold atomic gases when they are set into fast rotation (see [3] and references therein). Due to the mathematical similarity between Coriolis and Lorentz forces, rotating neutral gases are indeed the exact analogue of an assembly of charged particles plunged in a magnetic field."	T
CC1901	Pyrroles are not present in newly-formed proteins but occur when lipid oxidation products such as 4,5-Epoxy-2-alkenals, e.g. 4,5(E)-epoxy-2(E)-heptenal (EH) which is produced from oxidation of n __� 3 polyunsaturated fatty acids, react with free amino groups such as lysine residues on proteins [1;2]. Sugars such as glucose can also react nonenzymaticly with free amino groups. This non-enzymic glycosylation is known as the Maillard reaction and is well known in food science. Both the Maillard reaction and lipid peroxidation follow similar reaction pathways, producing carbonyl derivatives which then form advanced glycation end products (AGE) or advanced lipid peroxidation end products (ALE) by means of carbonyl-amine reactions and aldol condensations forming protein crosslinks #CITATION_TAG. Pyrrole cross-links have been identified in long lived proteins such as lens crystallins and skin collagen and implicated in the stiffening of arteries and joints associated with aging [4;5]. Increased (carboxyalkyl) pyrrole immunoreactivity was detected in plasma from patients with renal failure and artherosclerosis compared with healthy volunteers [1]. AGE and ALE increasingly accumulate during aging and in chronic diseases [6], suggesting that detecting pyrroles in proteins should be a good way to develop biomarkers for early stage disease.	0	Pyrroles are not present in newly-formed proteins but occur when lipid oxidation products such as 4,5-Epoxy-2-alkenals, e.g. 4,5(E)-epoxy-2(E)-heptenal (EH) which is produced from oxidation of n __� 3 polyunsaturated fatty acids, react with free amino groups such as lysine residues on proteins [1;2]. Sugars such as glucose can also react nonenzymaticly with free amino groups. This non-enzymic glycosylation is known as the Maillard reaction and is well known in food science. Both the Maillard reaction and lipid peroxidation follow similar reaction pathways, producing carbonyl derivatives which then form advanced glycation end products (AGE) or advanced lipid peroxidation end products (ALE) by means of carbonyl-amine reactions and aldol condensations forming protein crosslinks #CITATION_TAG. Pyrrole cross-links have been identified in long lived proteins such as lens crystallins and skin collagen and implicated in the stiffening of arteries and joints associated with aging [4;5]. Increased (carboxyalkyl) pyrrole immunoreactivity was detected in plasma from patients with renal failure and artherosclerosis compared with healthy volunteers [1]. AGE and ALE increasingly accumulate during aging and in chronic diseases [6], suggesting that detecting pyrroles in proteins should be a good way to develop biomarkers for early stage disease.	h
CC2891	"Why companies expand across borders by means of foreign direct investment (FDI) has been one of the central questions in international business research, and the subject of numerous studies since Stephen Hymer""s seminal study of the economic rationale for FDI more than 50 years ago (Hymer, 1960). The transaction cost (or internalization) theory of multinational companies (MNC), arguably the key theoretical perspective in this line of inquiry, claims that companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. From this perspective, MNCs -i.e. companies that have made FDIs -are a particular but increasingly common case of the general ""boundaries of the firm""-problem (Hennart, 2000): companies extend (or re-trench) their boundaries beyond the boundaries of their home countries in their efforts to reach an optimal degree of integr ation. Transaction cost theory points to a comparative analysis of governance forms, where the relatively more efficient ones are selected and win out. In the case of FDI, internal governance (the use of hierarchy) supersedes external governance (the use of markets and contracts) due to market inefficiencies and failures (Buckley and Casson, 1976;Hennart, 1982;#CITATION_TAG, 1986;Williamson, 1981). 1 Efficiency refers to the minimization of costs of operations -such as production and logistical costs -and costs of organization, which typically are termed transaction costs (in market governance modes) or governance costs (in internal governance modes). Governance According to Hennart (2000), MNCs integrate backwards as a response to market failures in factor and input markets; they deal with failures in markets for intermediate goods and technology by integrating horizontally; and, they engage in forward integration to deal with failures in the distribution and marketing of products. costs are evidently crucial for the relative efficiency of FDI as a mode of operating and expanding abroad, but despite their key role in explaining the internationalization of companies, systematic analyses of these costs amount so far to just a few conceptual discussions (e.g. Benito and Tomassen, 2010;Buckley and Strange, 2011;Slangen and Hennart, 2008) and empirical studies (e.g. Buvik and Andersen, 2002;Tomassen and Benito, 2009). Tomassen and Benito (2009) demonstrate that governance costs have a sizeable impact on MNCs"" performance. What factors drive these costs? Developing a better understanding of the nature and drivers of such costs would seem important for MNC headquarters in their strive to manage foreign operations in the best possible manner. Specifically, we point to three major reasons for why it is essential to examine governance costs in the context of the relationship between MNC headquarters and foreign subsidiaries."	4	"The transaction cost (or internalization) theory of multinational companies (MNC), arguably the key theoretical perspective in this line of inquiry, claims that companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. From this perspective, MNCs -i.e. companies that have made FDIs -are a particular but increasingly common case of the general ""boundaries of the firm""-problem (Hennart, 2000): companies extend (or re-trench) their boundaries beyond the boundaries of their home countries in their efforts to reach an optimal degree of integr ation. Transaction cost theory points to a comparative analysis of governance forms, where the relatively more efficient ones are selected and win out. In the case of FDI, internal governance (the use of hierarchy) supersedes external governance (the use of markets and contracts) due to market inefficiencies and failures (Buckley and Casson, 1976;Hennart, 1982;#CITATION_TAG, 1986;Williamson, 1981). 1 Efficiency refers to the minimization of costs of operations -such as production and logistical costs -and costs of organization, which typically are termed transaction costs (in market governance modes) or governance costs (in internal governance modes). Governance According to Hennart (2000), MNCs integrate backwards as a response to market failures in factor and input markets; they deal with failures in markets for intermediate goods and technology by integrating horizontally; and, they engage in forward integration to deal with failures in the distribution and marketing of products. costs are evidently crucial for the relative efficiency of FDI as a mode of operating and expanding abroad, but despite their key role in explaining the internationalization of companies, systematic analyses of these costs amount so far to just a few conceptual discussions (e.g. Benito and Tomassen, 2010;Buckley and Strange, 2011;Slangen and Hennart, 2008) and empirical studies (e.g. Buvik and Andersen, 2002;Tomassen and Benito, 2009)."	h
CC2763	"Following this discussion of an integrated framework which will be used to conceptually underpin our empirical study, the concept of network pictures is introduced as a way of capturing the perceptions of actors regarding the time-, space-, and ascription-specific aspect of change. Network pictures are well suited to this purpose as they represent the subjective sensemaking of managers, due to the fact that ""a concrete market is the result of operations of disentanglement, framing, internalization and externalization"" (Callon, 1999, p. 192), many of which have no \""objective\"" properties but are dependent on participants\"" beliefs and interpretations (Henneberg et al., 2006). As such, network pictures share some characteristics of concepts developed in the strategy literature on companies"" shared understanding of market phenomena, i.e. ""cognitive strategic groups"" (Fiegenbaum & Thomas, 1993;Osborne et al., 2001). Recent research shows increasing interest in the concept of network pictures (Henneberg, Mouzas, & Naudթ, 2009;Henneberg, Naudթ, & Mouzas, 2010;Leek & Mason, 2010;#CITATION_TAG, 2010). They provide understanding of how managers react to changing environments (Halinen & T�_rnroos, 1998;Oberg et al., 2007) and help to explain strategic decision-making behaviour (Borders et al., 2001). As such, the concept of network pictures is influenced by, and related to, the research themes of cognitive strategic groups (Osborne et al., 2001;Porac, Thomas, & Baden-Fuller, 1989) cognitive scripts and cognitive mapping (Fiol & Huff, 1992;Johnson, Daniels, & Asch, 1998), and managerial cognition/sensemaking in organisations (Colville & Pye, 2010;Daft & Weick, 1984;Weick, 1995)."	0	"Following this discussion of an integrated framework which will be used to conceptually underpin our empirical study, the concept of network pictures is introduced as a way of capturing the perceptions of actors regarding the time-, space-, and ascription-specific aspect of change. Network pictures are well suited to this purpose as they represent the subjective sensemaking of managers, due to the fact that ""a concrete market is the result of operations of disentanglement, framing, internalization and externalization"" (Callon, 1999, p. 192), many of which have no \""objective\"" properties but are dependent on participants\"" beliefs and interpretations (Henneberg et al., 2006). As such, network pictures share some characteristics of concepts developed in the strategy literature on companies"" shared understanding of market phenomena, i.e. ""cognitive strategic groups"" (Fiegenbaum & Thomas, 1993;Osborne et al., 2001). Recent research shows increasing interest in the concept of network pictures (Henneberg, Mouzas, & Naudթ, 2009;Henneberg, Naudթ, & Mouzas, 2010;Leek & Mason, 2010;#CITATION_TAG, 2010). They provide understanding of how managers react to changing environments (Halinen & T�_rnroos, 1998;Oberg et al., 2007) and help to explain strategic decision-making behaviour (Borders et al., 2001). As such, the concept of network pictures is influenced by, and related to, the research themes of cognitive strategic groups (Osborne et al., 2001;Porac, Thomas, & Baden-Fuller, 1989) cognitive scripts and cognitive mapping (Fiol & Huff, 1992;Johnson, Daniels, & Asch, 1998), and managerial cognition/sensemaking in organisations (Colville & Pye, 2010;Daft & Weick, 1984;Weick, 1995)."	e
CC1505	"Alternatively, closed and open class items could share representation and processing, but occupy different positions along crucial parameters. For example, closed class words are semantically ""shallow having fewer semantic features than open class items, which could make them more vulnerable to damage within the semantic system or between semantics and word form representations (Allen & Seidenberg, 1999;Goodglass & Menn, 1985;#CITATION_TAG & Shallice, 1993). Bates and Wulfeck (1989) point out that, in comparison to open class items, closed class items are less phonologically salient (being typically short and unstressed), less distinct from each other (i.e. more confusable, consider: it, is, if and in), lower in semantic content and given less contextual support in the sense that they appear in almost every possible semantic context. Here, the selection of closed and open class items is similar -governed by the same lexico-semantic retrieval processes -but the nature of the items makes them more or less vulnerable to semantic degradation. If this is the case, we expect to see errors on these items in SD."	0	"Alternatively, closed and open class items could share representation and processing, but occupy different positions along crucial parameters. For example, closed class words are semantically ""shallow having fewer semantic features than open class items, which could make them more vulnerable to damage within the semantic system or between semantics and word form representations (Allen & Seidenberg, 1999;Goodglass & Menn, 1985;#CITATION_TAG & Shallice, 1993). Bates and Wulfeck (1989) point out that, in comparison to open class items, closed class items are less phonologically salient (being typically short and unstressed), less distinct from each other (i.e. more confusable, consider: it, is, if and in), lower in semantic content and given less contextual support in the sense that they appear in almost every possible semantic context. Here, the selection of closed and open class items is similar -governed by the same lexico-semantic retrieval processes -but the nature of the items makes them more or less vulnerable to semantic degradation. If this is the case, we expect to see errors on these items in SD."	o
CC2318	Recent studies have suggested that macrophage expression of Programmed Death Ligand (PDL)-1 is important in regulating T cell responses to influenza infection [7]. PDL1 is the ligand for the Programmed Cell Death (PD) receptor 1, which is a member of the CD28 family of T cell receptors with CD80 and CD86 being ligands for CD28. In the standard model of T cell receptor (TCR) activation, activation of CD28 provides a necessary co-stimulation to prevent T cell anergy [8]. In contrast, binding of PDL1 to PD1 causes inhibition of TCR-mediated phosphatidylinositol-3-kinase (PI3Kinase) activation leading to inhibition of T cell proliferation and cytokine release [9]. The increased expression and activation of the PD1/PDL1 axis in chronic viral infections such as HIV and Hepatitis C (HCV) can lead to progressive loss of T cell function [10,11]. However, it was only recently that a role for this PD1/PDL1 pathway has been elucidated in the control of immune function in acute infections and increased PDL1 expression in response to pathogens was demonstrated to be crucial for impairment of CD8 cytotoxicity [7] and the development of regulatory T cells #CITATION_TAG. We have recently demonstrated that CD4 cytotoxic T cells play an important role in protecting against severe influenza infection. This work demonstrated an additional role for MHC class II expressing cells and T helper cell responses [13] highlighting the potential importance of macrophage-T cell interactions in the control of influenza infection.	0	In the standard model of T cell receptor (TCR) activation, activation of CD28 provides a necessary co-stimulation to prevent T cell anergy [8]. In contrast, binding of PDL1 to PD1 causes inhibition of TCR-mediated phosphatidylinositol-3-kinase (PI3Kinase) activation leading to inhibition of T cell proliferation and cytokine release [9]. The increased expression and activation of the PD1/PDL1 axis in chronic viral infections such as HIV and Hepatitis C (HCV) can lead to progressive loss of T cell function [10,11]. However, it was only recently that a role for this PD1/PDL1 pathway has been elucidated in the control of immune function in acute infections and increased PDL1 expression in response to pathogens was demonstrated to be crucial for impairment of CD8 cytotoxicity [7] and the development of regulatory T cells #CITATION_TAG. We have recently demonstrated that CD4 cytotoxic T cells play an important role in protecting against severe influenza infection. This work demonstrated an additional role for MHC class II expressing cells and T helper cell responses [13] highlighting the potential importance of macrophage-T cell interactions in the control of influenza infection.	e
CC1393	"The literature highlights an important role for local governments, frequently neglected in national-level analyses of possible hydrogen transitions. Yeh [18] shows that uptake of CNG and LPG has been significant in places with significant urban pollution problems, where local government regulatory decisions have been decisive. Local governments can be important both because they often have strong regulatory powers with respect to local air quality and public health, and second, because they often have strong powers with respect to local public transport and taxi fleets. National governments have played key roles in both successful and failed alternative vehicle and fuel programmes. In the US, the major policy drivers for alternative fuels have largely come from federal governments, with limited success [19]. National programmes have been successful elsewhere. Argentina""s national programme to promote CNG vehicles relied on price controls on fuel, and has been relatively successful despite almost no government involvement in infrastructure provision #CITATION_TAG."	0	"National governments have played key roles in both successful and failed alternative vehicle and fuel programmes. In the US, the major policy drivers for alternative fuels have largely come from federal governments, with limited success [19]. National programmes have been successful elsewhere. Argentina""s national programme to promote CNG vehicles relied on price controls on fuel, and has been relatively successful despite almost no government involvement in infrastructure provision #CITATION_TAG."	i
CC228	The rising prevalence in BC cannot be explained by increasing numbers of new MS cases; our incidence rates remained relatively stable over the 13-year period despite changes in MS diagnostic criteria [24] and increasing availability of disease-modifying drugs. While this seems in contrast to some other regions of the world where recent increases in incidence rates have been reported [5], a stable incidence rate has also been found over a similar time period in other Canadian provinces [10,11,19,25,26] and the UK [23]. Taken together with our findings, this suggests that the incidence of MS has stabilised in some areas over recent years. In the absence of increasing incidence, the rising prevalence may reflect longer disease duration due to earlier diagnosis, improved survival with MS or both. Survival has improved for both the BC general population and for people with MS in BC over the past 30 years #CITATION_TAG. Similarly, improved survival has been found in other MS populations, including those from Denmark [28] and Norway [29]. Immigration of prevalent cases can also influence prevalence trends and the population of British Columbia increased by nearly 30 % between 1991 and 2008, mostly due to immigration from other Canadian provinces and other countries. The prevalence estimates include MS cases that were resident throughout, as well as those that immigrated to BC, during the observation period. Newly immigrant prevalent cases would have contributed to the increasing prevalence over time if there was a greater proportion of MS cases among those immigrating to BC. The average age at the time of the incident MS-related claim was 44 years. This age is comparable to that recorded in other Canadian provinces [10,11] and was found to be within 3 years of the MS diagnosis date from medical charts or by personal report for 74 and 76 % of cases, respectively [10]. It is also equivalent to that identified as the first date of diagnosis in the General Practice Research Database for Fig. 2 Age-standardized prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008) of multiple sclerosis cases identified by the primary case definition (a) and the more sensitive but less specific case definition (b) in British Columbia, Canada cases of MS in the UK [23]. While the date of diagnosis, or of first medical recognition, is frequently used to measure MS incidence [2][3][4], symptom onset can often occur several years before the disease is first recognized or diagnosed.	0	While this seems in contrast to some other regions of the world where recent increases in incidence rates have been reported [5], a stable incidence rate has also been found over a similar time period in other Canadian provinces [10,11,19,25,26] and the UK [23]. Taken together with our findings, this suggests that the incidence of MS has stabilised in some areas over recent years. In the absence of increasing incidence, the rising prevalence may reflect longer disease duration due to earlier diagnosis, improved survival with MS or both. Survival has improved for both the BC general population and for people with MS in BC over the past 30 years #CITATION_TAG. Similarly, improved survival has been found in other MS populations, including those from Denmark [28] and Norway [29]. Immigration of prevalent cases can also influence prevalence trends and the population of British Columbia increased by nearly 30 % between 1991 and 2008, mostly due to immigration from other Canadian provinces and other countries. The prevalence estimates include MS cases that were resident throughout, as well as those that immigrated to BC, during the observation period.	i
CC2602	"A nal consideration before leaving NLP is the concept of mimicry or mirroringmatching someone""s body language and other behaviour to increase rapport. This does perhaps have some validity outside NLP (#CITATION_TAG, 2008). Heap describes the NLP recommendation that communicators. . . should match, mirror or pace the other person""s verbal and non-verbal behaviour (e.g. aspects of speech, body posture, breathing and blinking), thereby tuning into his or her representation of the world. This can be done directly, such as by matching the person""s body movements or breathing pattern with one""s own, or 2 Rowland (2008) discusses how`psychics"" practising cold reading need not actually present notably vague statements, instead making use of a range of strategies and techniques to extract desired information, which can then be fed back to the person being`read"" in an impressively specic and targeted form."	0	"A nal consideration before leaving NLP is the concept of mimicry or mirroringmatching someone""s body language and other behaviour to increase rapport. This does perhaps have some validity outside NLP (#CITATION_TAG, 2008). Heap describes the NLP recommendation that communicators. . . should match, mirror or pace the other person""s verbal and non-verbal behaviour (e.g. aspects of speech, body posture, breathing and blinking), thereby tuning into his or her representation of the world. This can be done directly, such as by matching the person""s body movements or breathing pattern with one""s own, or 2 Rowland (2008) discusses how`psychics"" practising cold reading need not actually present notably vague statements, instead making use of a range of strategies and techniques to extract desired information, which can then be fed back to the person being`read"" in an impressively specic and targeted form."	h
CC2037	Stratigraphic, sedimentologic and geochronologic data indicate that there may be six distinct periods of eolian deposition in the Cape Cod dune field (Figure 9). The oldest recognized depositional event is at the base of the Highway 6 section, where quartz grains yielded an OSL age of 3.7 �� 0.3 ka (Figure 6). This is a single site, with one OSL age, and thus it is unknown how pervasive is this early eolian depositional event. Lacustrine sediment cores from the northeastern U.S. show a decline in Hemlock (Tsuga sp.), a decrease in lake level, or an increase in clastic sedimentation ca. 5.5-3.8 5.5-3.8 ka and is associated with a broad scale drying (e.g., Newby et al., 2000;Shuman et al., 2001;Shuman and Donnelly, 2006;#CITATION_TAG et al., 2007;Marsicek et al., 2013). Recent analysis of paleolimnologic data indicates that this drying is associated with cooling of coastal air by 0.5-2.5 _�� C and a precipitation decrease of ___100 mm, linked to cooler surface temperatures in the North Atlantic Ocean (Marsicek et al., 2013). Wetter conditions prevailed post 3.8 ka (cf. Shuman and Donnelly, 2006;Oswald et al., 2007;Marsicek et al., 2013) (Figure 9) with an inferred period of heightened hurricane activity ca. 3.6-3.4 ka (Toomey et al., 2013). Cores from the Makepeace Cedar Swamp in far eastern Massachusetts indicate the onset of the wettest conditions post-glacial at ca. 3.2 ka (Newby et al., 2000). The penultimate eolian depositional event is represented by only the Trailhead site with two OSL ages of ca. 2.4 ka (Figure 6). The spatial extent of this eolian depositional event is unknown though these ages may be coincident with a pronounced period of increased hurricane activity in the North Atlantic Ocean (Toomey et al., 2013), but with dry and cool conditions on Cape Cod (Shuman and Donnelly, 2006;Marsicek et al., 2013) (Figure 9). It is salient to note that there is an absence in the presented stratigraphic record of eolian sand deposited ca. 3 ka ago (Figure 9), when inferred hurricane activity was heightened (Toomey et al., 2013).	1	This is a single site, with one OSL age, and thus it is unknown how pervasive is this early eolian depositional event. Lacustrine sediment cores from the northeastern U.S. show a decline in Hemlock (Tsuga sp. ), a decrease in lake level, or an increase in clastic sedimentation ca. 5.5-3.8 5.5-3.8 ka and is associated with a broad scale drying (e.g., Newby et al., 2000;Shuman et al., 2001;Shuman and Donnelly, 2006;#CITATION_TAG et al., 2007;Marsicek et al., 2013). Recent analysis of paleolimnologic data indicates that this drying is associated with cooling of coastal air by 0.5-2.5 _�� C and a precipitation decrease of ___100 mm, linked to cooler surface temperatures in the North Atlantic Ocean (Marsicek et al., 2013). Wetter conditions prevailed post 3.8 ka (cf. Shuman and Donnelly, 2006;Oswald et al., 2007;Marsicek et al., 2013) (Figure 9) with an inferred period of heightened hurricane activity ca. 3.6-3.4 ka (Toomey et al., 2013).	.
CC2684	A few previous studies have investigated the impact of democracy on agricultural policy outcomes. All studies but one, exploit the cross-country variation in the data and find mixed and often weak evidence on the effect of democracy on agricultural protection (see #CITATION_TAG and Kherallah, 1994;Swinnen et al. 2000;Olper, 2001) 4 . Swinnen et al. (2001), using long-run data for a single country exploit the within-country variation and show that only those democratic reforms that caused a significant shift in the political balance towards agricultural interests -in particular the extension of voting rights to small farmers in Belgium in the early 20th century -induced an increase in agricultural protection. Our paper will make use of a much larger dataset and better econometric techniques to test the impact of regime changes on agricultural protection.	0	A few previous studies have investigated the impact of democracy on agricultural policy outcomes. All studies but one, exploit the cross-country variation in the data and find mixed and often weak evidence on the effect of democracy on agricultural protection (see #CITATION_TAG and Kherallah, 1994;Swinnen et al. 2000;Olper, 2001) 4 . Swinnen et al. (2001), using long-run data for a single country exploit the within-country variation and show that only those democratic reforms that caused a significant shift in the political balance towards agricultural interests -in particular the extension of voting rights to small farmers in Belgium in the early 20th century -induced an increase in agricultural protection. Our paper will make use of a much larger dataset and better econometric techniques to test the impact of regime changes on agricultural protection.	l
CC976	"Cognitive neuroscience research has also investigated the neural mechanisms of motor imagery in the control of action (e.g. see deLange, Roelofs & Toni, 2008). In the same way that visual imagery and visual perception recruit similar brain regions (Ganis et al., 2004), several studies have shown that during imagination of a movement, the same sensorimotor regions are activated as when we observe a movement or actually execute it ourselves (Decety et al., 1994;Gr��zes & Decety, 2001). Some studies have made use of the motor expertise model to investigate the link between the action execution and action perception network (#CITATION_TAG et al., 2005;Calvo-Merino et al., 2006;Orgs et al., 2008) and motor learning (Cross et al. 2006). Other studies have focused on the underlying neural mechanisms of creativity in realms other than dance (Jung et al., 2010), especially in music (Limb and Braun, 2008) and drawing (Bhattacharya & Petsche, 2005). The bulk of these studies have been based upon the researchers"" expectations concerning motor responses, rather than seeking to correlate activity with subjective reports of motor imagery, so while they have focused upon motor or movement related brain processes, wide ranging networks are implicated and the possible involvement of multiple forms of imagery in such tasks remains to be clarified."	5	"Cognitive neuroscience research has also investigated the neural mechanisms of motor imagery in the control of action (e.g. see deLange, Roelofs & Toni, 2008). In the same way that visual imagery and visual perception recruit similar brain regions (Ganis et al., 2004), several studies have shown that during imagination of a movement, the same sensorimotor regions are activated as when we observe a movement or actually execute it ourselves (Decety et al., 1994;Gr��zes & Decety, 2001). Some studies have made use of the motor expertise model to investigate the link between the action execution and action perception network (#CITATION_TAG et al., 2005;Calvo-Merino et al., 2006;Orgs et al., 2008) and motor learning (Cross et al. 2006). Other studies have focused on the underlying neural mechanisms of creativity in realms other than dance (Jung et al., 2010), especially in music (Limb and Braun, 2008) and drawing (Bhattacharya & Petsche, 2005). The bulk of these studies have been based upon the researchers"" expectations concerning motor responses, rather than seeking to correlate activity with subjective reports of motor imagery, so while they have focused upon motor or movement related brain processes, wide ranging networks are implicated and the possible involvement of multiple forms of imagery in such tasks remains to be clarified."	m
CC886	"We can see likewise see that research trajectories and designs are powerfully impacted not only by formal governance and legislation, but also by the everyday ethics of researchers-as well as by study coordinators and managers who may attempt to use """"informal ethical practices"""" in attempt to """"reinsert care into research"""" [16: 689]. The significance of everyday ethics has been documented, for instance, in studies of controversial areas of investigations such as stem cell science. Within this field, the collection of ""spare embryos"" is central to research; yet, the construal of an embryo as ""spare"" must be achieved through careful ethical argumentation and deliberation which is itself experimental [14,#CITATION_TAG]. Here, as elsewhere, boundaries between un/ethical forms of investigations are discursively constructed which at one """"define and defend the work of scientists involved in ethically sensitive research"""" [67: 745]."	0	"We can see likewise see that research trajectories and designs are powerfully impacted not only by formal governance and legislation, but also by the everyday ethics of researchers-as well as by study coordinators and managers who may attempt to use """"informal ethical practices"""" in attempt to """"reinsert care into research"""" [16: 689]. The significance of everyday ethics has been documented, for instance, in studies of controversial areas of investigations such as stem cell science. Within this field, the collection of ""spare embryos"" is central to research; yet, the construal of an embryo as ""spare"" must be achieved through careful ethical argumentation and deliberation which is itself experimental [14,#CITATION_TAG]. Here, as elsewhere, boundaries between un/ethical forms of investigations are discursively constructed which at one """"define and defend the work of scientists involved in ethically sensitive research"""" [67: 745]."	t
CC2596	"Essentially NLP as currently commonly presented is a set of patterns for communication, based on models of understanding how people think, speak and act, both oneself and others. It emphasises noticing the small but crucial signals that let you know how [others] are responding to your behaviour, and heightened awareness of your internal images, sounds and feelings when thinking (#CITATION_TAG and Seymour, 2002, p.9), via patterns such as`eye accessing cues""watching others"" eye movements as they think, with certain movements said to be associated with dierent`preferred representational systems"" ( thinking in pictures, sounds or feelings (#CITATION_TAG and Seymour, 2002, p.35), or, for example, telling the truth versus lying."	0	"Essentially NLP as currently commonly presented is a set of patterns for communication, based on models of understanding how people think, speak and act, both oneself and others. It emphasises noticing the small but crucial signals that let you know how [others] are responding to your behaviour, and heightened awareness of your internal images, sounds and feelings when thinking (#CITATION_TAG and Seymour, 2002, p.9), via patterns such as`eye accessing cues""watching others"" eye movements as they think, with certain movements said to be associated with dierent`preferred representational systems"" ( thinking in pictures, sounds or feelings (#CITATION_TAG and Seymour, 2002, p.35), or, for example, telling the truth versus lying."	t
CC225	"MS cases were identified using administrative case definitions, which have previously been validated in two Canadian provinces (Manitoba and Nova Scotia) [10,11], and are based on hospital and physician-derived diagnostic codes. The primary case definition used was C7 hospital or physician claims specifically for MS for people who were resident in BC for [3 years following their first demyelinating disease (""MS-related"") claim (i.e. a claim for MS, optic neuritis, acute transverse myelitis, acute disseminated encephalomyelitis, demyelinating disease of the CNS unspecified, other acute disseminated demyelination, or neuromyelitis optica), and C3 MS claims for those with B3 years of residency. When validated against the clinical MS definition, this algorithm was found to provide the best balance of sensitivity and specificity compared to a series of alternative administrative case definitions. For the validation population (Nova Scotia, Canada), all of whom had at least one claim for a demyelinating disease, this definition had a sensitivity of 88 % and specificity of 68 %; the specificity would, however, be substantially higher in the general population given that [99.9 % of individuals have no demyelinating claims [11,#CITATION_TAG]. A second case definition was also used which required C3 MS claims irrespective of the cumulative residency in BC, for which previous validation has demonstrated greater sensitivity (95 %) but less specificity (48 %) among those with at least one demyelinating claim [11]."	0	"MS cases were identified using administrative case definitions, which have previously been validated in two Canadian provinces (Manitoba and Nova Scotia) [10,11], and are based on hospital and physician-derived diagnostic codes. The primary case definition used was C7 hospital or physician claims specifically for MS for people who were resident in BC for [3 years following their first demyelinating disease (""MS-related"") claim (i.e. a claim for MS, optic neuritis, acute transverse myelitis, acute disseminated encephalomyelitis, demyelinating disease of the CNS unspecified, other acute disseminated demyelination, or neuromyelitis optica), and C3 MS claims for those with B3 years of residency. When validated against the clinical MS definition, this algorithm was found to provide the best balance of sensitivity and specificity compared to a series of alternative administrative case definitions. For the validation population (Nova Scotia, Canada), all of whom had at least one claim for a demyelinating disease, this definition had a sensitivity of 88 % and specificity of 68 %; the specificity would, however, be substantially higher in the general population given that [99.9 % of individuals have no demyelinating claims [11,#CITATION_TAG]. A second case definition was also used which required C3 MS claims irrespective of the cumulative residency in BC, for which previous validation has demonstrated greater sensitivity (95 %) but less specificity (48 %) among those with at least one demyelinating claim [11]."	 
CC917	The two liverworts in our study, Lophozia lycopodioides and Ptilidium ciliare, showed no fixation at all. They even showed negative values of fixation (see Table 1), as a likely artefact of our method of subtracting N concentrations in control samples from those in incubated ones. Thus, in practice these values show a lack of nitrogen fixation activity. Despite the frequent abundance of the two birch forest species in the subarctic region, the results for degree of colonisation by cyanobacteria should not be surprising. Although in scientific literature close (internal) symbiotic relationships between a few liverwort species and cyanobacteria have been reported (see Adams 2002), none of them listed L. lycopodioides or P. ciliare. Liverworts are known for rich composition of secondary compounds (Asakawa 1995(Asakawa , 2004#CITATION_TAG 2000), many of which exhibit antimicrobial properties (Asakawa 2008). We speculate these compounds might also thwart colonization of liverworts by epiphytic cyanobacteria.	1	Thus, in practice these values show a lack of nitrogen fixation activity. Despite the frequent abundance of the two birch forest species in the subarctic region, the results for degree of colonisation by cyanobacteria should not be surprising. Although in scientific literature close (internal) symbiotic relationships between a few liverwort species and cyanobacteria have been reported (see Adams 2002), none of them listed L. lycopodioides or P. ciliare. Liverworts are known for rich composition of secondary compounds (Asakawa 1995(Asakawa , 2004#CITATION_TAG 2000), many of which exhibit antimicrobial properties (Asakawa 2008). We speculate these compounds might also thwart colonization of liverworts by epiphytic cyanobacteria.	w
CC2183	"Hawley et al. [57] developed a limited vocabulary system with computerised training package for a home environment which achieved a recognition accuracy of around 95%, operating around twice as fast as a switch control system. Another study #CITATION_TAG employed user movements to access a computer via a ""camera mouse"" with 6 of the 10 participants able to use the technology to spell out communications. A system which detects minute facial muscle or eye movements in addition to brain waves, enabling movement of a computer cursor to make communication choices (Cyberlink TM ), was evaluated in one paper [59] . Two children achieved an 80% success rate in changing a picture on a computer using the system. An EEG-based brain-computer interface system was used by an adult with severe cerebral palsy with a 70% correct re-sponse rate for copy spelling following training in one study [601] ."	2	"Hawley et al. [57] developed a limited vocabulary system with computerised training package for a home environment which achieved a recognition accuracy of around 95%, operating around twice as fast as a switch control system. Another study #CITATION_TAG employed user movements to access a computer via a ""camera mouse"" with 6 of the 10 participants able to use the technology to spell out communications. A system which detects minute facial muscle or eye movements in addition to brain waves, enabling movement of a computer cursor to make communication choices (Cyberlink TM ), was evaluated in one paper [59] . Two children achieved an 80% success rate in changing a picture on a computer using the system. An EEG-based brain-computer interface system was used by an adult with severe cerebral palsy with a 70% correct re-sponse rate for copy spelling following training in one study [601] ."	n
CC2403	"The #CITATION_TAG and Black (2013) article exhaustively reviews all extant data in ""permissive"" countries on reported cases of granted and refused assisted dying"	4	"The #CITATION_TAG and Black (2013) article exhaustively reviews all extant data in ""permissive"" countries on reported cases of granted and refused assisted dying"	T
CC2625	"This absence of a direct theoretical engagement with the spatiality of markets is also remarkable given that popular, policy and social scientific accounts are replete with spatialised descriptions and, more problematically, prescriptions concerning markets. The ""common sense"" spatalised attributes of markets are also clearly linked to economic action and outcomes in the global economy. The examples are numerous. Perhaps foremost is the idea that ""globalised"" financial markets now exist and are responsible for creating financial instability (Cable, 2010;Wolf, 2009;Harvey, 2007Harvey, [1982Harvey, ], 2010. However, social scientists also now see the fate of national economic space as bound to the development of ""global marketplaces"" for goods and services (Ikeda, 2002;Bhagwati, 2007;Friedman, 2007). Likewisemarkets are both geographically constituted (Kozel, 2005) and have geographical consequences (Meric and Meric, 2001;#CITATION_TAG, 2010). In the case of the former, the market differentiations that transnational retail or service firms grapple with in operating across many nations bear witness to this (Wrigley et al., 2005;Faulconbridge, 2008). Equally, the complex geographies of labour market space are increasingly clear in the ongoing ""offshoring"" of jobs from Europe to Asia respectively (Jensen et al., 2009;Crino, 2009)."	0	"The examples are numerous. Perhaps foremost is the idea that ""globalised"" financial markets now exist and are responsible for creating financial instability (Cable, 2010;Wolf, 2009;Harvey, 2007Harvey, [1982Harvey, ], 2010. However, social scientists also now see the fate of national economic space as bound to the development of ""global marketplaces"" for goods and services (Ikeda, 2002;Bhagwati, 2007;Friedman, 2007). Likewisemarkets are both geographically constituted (Kozel, 2005) and have geographical consequences (Meric and Meric, 2001;#CITATION_TAG, 2010). In the case of the former, the market differentiations that transnational retail or service firms grapple with in operating across many nations bear witness to this (Wrigley et al., 2005;Faulconbridge, 2008). Equally, the complex geographies of labour market space are increasingly clear in the ongoing ""offshoring"" of jobs from Europe to Asia respectively (Jensen et al., 2009;Crino, 2009)."	i
CC1675	Extracting the data elements needed in these tasks is a time-consuming and at present a largely manual process which requires domain expertise. For example, in systematic review preparation, information extraction generally constitutes the most time consuming task (#CITATION_TAG et al., 2014). This situation is made worse by the rapidly expanding body of potentially relevant literature with more than one million papers added into PubMed each year (Landhuis, 2016). Therefore, data annotation and extraction presents an important challenge for automation.	5	Extracting the data elements needed in these tasks is a time-consuming and at present a largely manual process which requires domain expertise. For example, in systematic review preparation, information extraction generally constitutes the most time consuming task (#CITATION_TAG et al., 2014). This situation is made worse by the rapidly expanding body of potentially relevant literature with more than one million papers added into PubMed each year (Landhuis, 2016). Therefore, data annotation and extraction presents an important challenge for automation.	o
CC2238	Plasma waves have been more recently proposed to have an important role in driving flows within loops. Acoustic waves excited by heat pulses at the chromospheric loop footpoints and damped by thermal conduction in corona are possible candidates (Taroyan et al., 2005). Even more attention received the propagation of Alfvͩn waves in coronal loops. Hydrodynamic loop modeling showed that Alfvͩn waves deposit significant momentum in the plasma, and that steady state conditions with significant flows and relatively high density can be reached (#CITATION_TAG and Li, 2005). Analogous results were obtained independently with a different approach: considering a wind-like model to describe a long isothermal loop, Grappin et al. (2003Grappin et al. ( , 2005 showed that the waves can drive pressure variations along the loop which trigger siphon flows. Alfvͩn disturbances have been more recently shown to be amplified by the presence of loop flows (Taroyan, 2009). Large-scale MHD models have also addressed the presence of flows in the low corona. These models show that heat pulses released low in the corona in places of strong magnetic field braiding trigger downflows and slight upflows (Figure 19, Hansteen et al., 2010;Zacharias et al., 2011). The corresponding Doppler-shifts are similar to those often observed (see Section 3.5). Most of the mass circulating across the transition region is probably confined in very short loops (___ 2 �� 10 8 cm) (Guerreiro et al., 2013).	5	Plasma waves have been more recently proposed to have an important role in driving flows within loops. Acoustic waves excited by heat pulses at the chromospheric loop footpoints and damped by thermal conduction in corona are possible candidates (Taroyan et al., 2005). Even more attention received the propagation of Alfvͩn waves in coronal loops. Hydrodynamic loop modeling showed that Alfvͩn waves deposit significant momentum in the plasma, and that steady state conditions with significant flows and relatively high density can be reached (#CITATION_TAG and Li, 2005). Analogous results were obtained independently with a different approach: considering a wind-like model to describe a long isothermal loop, Grappin et al. (2003Grappin et al. ( , 2005 showed that the waves can drive pressure variations along the loop which trigger siphon flows. Alfvͩn disturbances have been more recently shown to be amplified by the presence of loop flows (Taroyan, 2009). Large-scale MHD models have also addressed the presence of flows in the low corona.	r
CC2046	The current migration of dunes on Cape Cod is inferred to reflect a legacy of landscape disturbance, specifically forest clear-cutting, grazing and agricultural practices, associated with European settlement starting in the early 17th century and continuing into the 20th century (McCaffrey and Stilgoe, 1981;Rubertone, 1985;Motzkin et al., 2002;Eberhardt et al., 2003;Forman et al., 2008). Other factors such as storminess, hurricane-force winds (Bosse et al., 2001;Eberhardt et al., 2003) and forest fires (Motzkin et al., 2002;Parshall et al., 2003) may have contributed to historic disturbance of this dune landscape. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009;Hansom and Hall, 2009). However, uncertainty remains if periods of increased storminess and/or hurricane landfalls for the exposed Cape Cod spit during the Holocene (Mann et al., 2009;Toomey et al., 2013) were of sufficient magnitude to disturb this forest ecosystem and reactivated dunes. Proxy records of hurricane occurrence from coastal overwash timeseries (e.g., Donnelly et al., 2001;#CITATION_TAG and Donnelly, 2007) and from marine sediment cores extracted strategically along the Bahama Bank (Williams, 2013) to reflect wave climate indicate heightened hurricane activity in the North Atlantic Ocean between 4900 and 3600, 2500, and 1000, and 600 and 400 years BP (Mann et al., 2009;Toomey et al., 2013).	0	Other factors such as storminess, hurricane-force winds (Bosse et al., 2001;Eberhardt et al., 2003) and forest fires (Motzkin et al., 2002;Parshall et al., 2003) may have contributed to historic disturbance of this dune landscape. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009;Hansom and Hall, 2009). However, uncertainty remains if periods of increased storminess and/or hurricane landfalls for the exposed Cape Cod spit during the Holocene (Mann et al., 2009;Toomey et al., 2013) were of sufficient magnitude to disturb this forest ecosystem and reactivated dunes. Proxy records of hurricane occurrence from coastal overwash timeseries (e.g., Donnelly et al., 2001;#CITATION_TAG and Donnelly, 2007) and from marine sediment cores extracted strategically along the Bahama Bank (Williams, 2013) to reflect wave climate indicate heightened hurricane activity in the North Atlantic Ocean between 4900 and 3600, 2500, and 1000, and 600 and 400 years BP (Mann et al., 2009;Toomey et al., 2013).	y
CC1527	"The selection of lexical items and the building of syntactic frames are intimately linked and, more importantly, the correct assignment of lexical items to phrasal roles (who did what to whom) involves both semantic and syntactic information. Lexico-semantic representations are assigned to particular roles (e.g. the agent of an action) according to a conceptual message; thus this assignment cannot be independent of conceptual-semantic content (Bock, 1982;#CITATION_TAG & Warren, 1985). During syntactic encoding, when phrases are being built, lexico-syntactic information marks the appropriate context for a given word (Bock, 1999;Bock & Levelt, 1994) and is likely to influence the encoding of an item""s syntactic environment. Verbs are thought to dictate the argument structure of the sentences in which they appear (Levin 1993;Altmann, 2004), and Levelt et al. (1999) proposed that ""lemmas"" (holistic lexical representations) are marked for syntactic information (e.g. lexical class) amongst other things. For example, a selected lemma that is marked as a noun should/will not be assigned to a verb position in a syntactic frame."	0	"The selection of lexical items and the building of syntactic frames are intimately linked and, more importantly, the correct assignment of lexical items to phrasal roles (who did what to whom) involves both semantic and syntactic information. Lexico-semantic representations are assigned to particular roles (e.g. the agent of an action) according to a conceptual message; thus this assignment cannot be independent of conceptual-semantic content (Bock, 1982;#CITATION_TAG & Warren, 1985). During syntactic encoding, when phrases are being built, lexico-syntactic information marks the appropriate context for a given word (Bock, 1999;Bock & Levelt, 1994) and is likely to influence the encoding of an item""s syntactic environment. Verbs are thought to dictate the argument structure of the sentences in which they appear (Levin 1993;Altmann, 2004), and Levelt et al. (1999) proposed that ""lemmas"" (holistic lexical representations) are marked for syntactic information (e.g. lexical class) amongst other things. For example, a selected lemma that is marked as a noun should/will not be assigned to a verb position in a syntactic frame."	e
CC2287	"The Bel-Robinson tensor B �_�_���_ was first discovered in ordinary Einstein gravity (GR) at D = 4, in a search for a gravitational counterpart of the usual matter stress tensor T �_�_ . Since there can be neither local tensorial gravitational candidates of second derivative order (because quantities ___ ___g �_�_ ___g ���_ , being frame-dependent, can be made to vanish at any point) nor any non-covariantly conserved ones, the successful candidate was indeed quadratic in the tensorial ""field strengths""-curvatures B ___ RR, and covariantly conserved, in analogy with the quadratic Maxwell T �_�_ ___ F F. Subsequently, conserved B were found for arbitrary dimension, and matter analogs have also been constructed (see, e.g., #CITATION_TAG for earlier references)."	0	"The Bel-Robinson tensor B �_�_���_ was first discovered in ordinary Einstein gravity (GR) at D = 4, in a search for a gravitational counterpart of the usual matter stress tensor T �_�_ . Since there can be neither local tensorial gravitational candidates of second derivative order (because quantities ___ ___g �_�_ ___g ���_ , being frame-dependent, can be made to vanish at any point) nor any non-covariantly conserved ones, the successful candidate was indeed quadratic in the tensorial ""field strengths""-curvatures B ___ RR, and covariantly conserved, in analogy with the quadratic Maxwell T �_�_ ___ F F. Subsequently, conserved B were found for arbitrary dimension, and matter analogs have also been constructed (see, e.g., #CITATION_TAG for earlier references)."	T
CC488	"These results can reconcile those from previous studies in which infants showed speed gains exceeding race models in a localisation task (Neil et al., 2006), but children aged below 7 or 10-12 years did not do so reliably in a central task where the stimulus was always at the same location, straight ahead (Barutchu et al., 2009;Barutchu et al., 2010;Brandwein et al., 2011). In the present left/right localisation task, we saw evidence for pooling of signals from 4 years. One might expect that the additional need to make correct spatial decisions would work against finding the greatest speed improvements. However, it may be that the need to localise the stimulus in space on every trial tapped into multisensory decision mechanisms dedicated to spatial localisation. A wellknown substrate for this kind of localisation is the superior colliculus (SC; Wallace et al., 1996;Wallace & Stein, 1997;Stein & Meredith, 1993). Multisensory integration in superior colliculus develops with postnatal sensory experience, which seems to be learnt via the cortico-SC projection (Stein et al., 2009). The decisions in our study may also (or alternatively) have depended on other cortical mechanisms (Molholm et al., 2002;#CITATION_TAG et al., 2007;Noppeney et al., 2010). A study with adults directly comparing saccadic, directed manual and simple manual responses found violations of Miller""s bound in all three cases (Hughes et al., 1994). The present developmental results from a directed manual task, together with previous data from spatial orienting (Neil et al., 2006) and simple manual tasks (Barutchu et al., 2009;Barutchu et al., 2010;Brandwein et al., 2011), suggest that pooling of auditory and visual signals may develop much earlier for spatial localisation of signals than for simple detection of their onset. A follow-up study comparing simple responses and localisation directly should test this interpretation."	3	"However, it may be that the need to localise the stimulus in space on every trial tapped into multisensory decision mechanisms dedicated to spatial localisation. A wellknown substrate for this kind of localisation is the superior colliculus (SC; Wallace et al., 1996;Wallace & Stein, 1997;Stein & Meredith, 1993). Multisensory integration in superior colliculus develops with postnatal sensory experience, which seems to be learnt via the cortico-SC projection (Stein et al., 2009). The decisions in our study may also (or alternatively) have depended on other cortical mechanisms (Molholm et al., 2002;#CITATION_TAG et al., 2007;Noppeney et al., 2010). A study with adults directly comparing saccadic, directed manual and simple manual responses found violations of Miller""s bound in all three cases (Hughes et al., 1994). The present developmental results from a directed manual task, together with previous data from spatial orienting (Neil et al., 2006) and simple manual tasks (Barutchu et al., 2009;Barutchu et al., 2010;Brandwein et al., 2011), suggest that pooling of auditory and visual signals may develop much earlier for spatial localisation of signals than for simple detection of their onset. A follow-up study comparing simple responses and localisation directly should test this interpretation."	c
CC2783	Recent research has stressed the importance of network structures in understanding business exchanges (Achrol, 1997;M�_ller & Rajala, 2007). These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;Porter, 1985) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980). Business networks have also been characterised in strategic marketing as value-creating systems (Parolini, 1999) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (M�_ller & Svahn, 2006;Normann & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998;#CITATION_TAG, Nohria, & Zaheer, 2000).	0	These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;Porter, 1985) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980). Business networks have also been characterised in strategic marketing as value-creating systems (Parolini, 1999) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (M�_ller & Svahn, 2006;Normann & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998;#CITATION_TAG, Nohria, & Zaheer, 2000).	n
CC1733	"One approach may be to use a set of model traits and diseases and employ their existing mapped loci to identify a small set of the component genes by brute-force (or, luck) and use the uncovered biology to infer which other genes in their """"pathways"""" can explain the dis-ease. This approach has been highly profitable in Crohn""s disease-a common inflammatory disorder whose root causes remained cryptic until genome-wide association studies identified a large number of loci with fundamental defects in mucosal immunity (Graham and Xavier, 2013)-but not in type 2 diabetes, where the pathophysiology awaits clarification (Groop and Pociot, 2013). Although we suspect that the numbers of pathways involved are fewer than the numbers of genes involved, this is merely suspicion. Nevertheless, can we reduce the complexity of the problem by identifying all of the relevant pathways? Despite uncertainty, this approach has the advantage of leading to specific testable hypotheses. The second approach is to focus research on why the disease is complex in the first place. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (#CITATION_TAG et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the complex inheritance problem (Yosef et al., 2013). Even more importantly, this approach might, through the effect of mutations, allow us to decipher cell circuitry and understand which pathways are limiting and which are redundant. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges. One might counter that existing gene ontologies do precisely that, but, even in yeast, this appears to be highly incomplete (Dutkowski et al., 2013)."	4	Nevertheless, can we reduce the complexity of the problem by identifying all of the relevant pathways? Despite uncertainty, this approach has the advantage of leading to specific testable hypotheses. The second approach is to focus research on why the disease is complex in the first place. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (#CITATION_TAG et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the complex inheritance problem (Yosef et al., 2013). Even more importantly, this approach might, through the effect of mutations, allow us to decipher cell circuitry and understand which pathways are limiting and which are redundant. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges.	g
CC1326	"Where academic performance is measured under less stressful conditions, such as continuous assessment work, the relationship between neuroticism and academic performance is less well-defined (Chamorro-Premuzic & Furnham, 2009, p. 75). Kappe and van der Flier (2010) found neuroticism to be positively correlated with academic performance (r=0.18, p<0.05, n=133) when assessment is free from time constraints and supervision. Research is inconsistent regarding the remaining two personality dimensions of extroversion and agreeableness and their relationship with academic performance. Introverts tend to have better study habits and are less easily distracted (Entwistle & Entwistle, 1970as cited in Chamorro-Premuzic & Furnham, 2009, while extroverts tend to perform better in class participation, oral exams, seminar presentations, and multiple-choice style questions (#CITATION_TAG & Medhurst, 1995;Kappe & van der Flier, 2010). In their meta-analysis of a number of studies investigating personality as a predictor of academic performance, O""Connor and Paunonen ( 2007) concluded agreeableness is not associated with academic performance. Farsides and Woodfield (2003) found that agreeableness, while not related to academic performance, was linked to other performance indicators such as attendance record."	0	"Where academic performance is measured under less stressful conditions, such as continuous assessment work, the relationship between neuroticism and academic performance is less well-defined (Chamorro-Premuzic & Furnham, 2009, p. 75). Kappe and van der Flier (2010) found neuroticism to be positively correlated with academic performance (r=0.18, p<0.05, n=133) when assessment is free from time constraints and supervision. Research is inconsistent regarding the remaining two personality dimensions of extroversion and agreeableness and their relationship with academic performance. Introverts tend to have better study habits and are less easily distracted (Entwistle & Entwistle, 1970as cited in Chamorro-Premuzic & Furnham, 2009, while extroverts tend to perform better in class participation, oral exams, seminar presentations, and multiple-choice style questions (#CITATION_TAG & Medhurst, 1995;Kappe & van der Flier, 2010). In their meta-analysis of a number of studies investigating personality as a predictor of academic performance, O""Connor and Paunonen ( 2007) concluded agreeableness is not associated with academic performance. Farsides and Woodfield (2003) found that agreeableness, while not related to academic performance, was linked to other performance indicators such as attendance record."	r
CC1768	"For the full system (1.6), the Cauchy problem is well posed for a suitable class of -independent initial data (see details in Ginibre and Velo 1979;Tsutsumi 1987). However, the well posedness is not uniform in . In practical terms that means that the solution to (1.6) behaves in a very irregular way in some regions of the (x, t)-plane when __� 0. Such an irregular behavior begins near the points (x = x 0 , t = t 0 ) of the ""gradient catastrophe"" of the solution to the dispersionless limit (1.8). The solutions to (1.8) and (1.6) are essentially indistinguishable for t < t 0 ; the situation changes dramatically near x 0 when approaching the critical point. Namely, when approaching t = t 0 , the peak near a local maximum 1 of u becomes more and more narrow due to self-focusing; the solution develops a zone of rapid oscillations for t > t 0 . They have been studied both analytically and numerically in Ceniceros and Tian (2002), Forest and Lee (1986), Grenier (1998), #CITATION_TAG et al. (1994), Kamvissis (1996), Kamvissis et al. (2003), Miller and Kamvissis (1998), Tovbis et al. (2004Tovbis et al. ( , 2006. In particular, in Kamvissis et al. (2003), Tovbis et al. (2004) for certain NLS solutions, it was introduced the notion of a breaking curve t = t 0 (x). The main theorem of Tovbis et al. (2004) describes the limiting behavior of the solution in two disjoint regions: for t < t (x) or t > t 0 (x) by rigorous arguments based on application of the steepest descent analysis of the associated Riemann-Hilbert problem. The structures of the asymptotic formulae in these two regions are completely different. The points on the breaking curve were excluded from the rigorous analysis of Kamvissis et al. (2003) and Tovbis et al. (2004). However, no results are available so far about the behavior of the solutions to the focusing NLS at the critical point (x 0 , t 0 ) (that is, at the cusp point t 0 (x 0 ) of the breaking curve)."	0	"Such an irregular behavior begins near the points (x = x 0 , t = t 0 ) of the ""gradient catastrophe"" of the solution to the dispersionless limit (1.8). The solutions to (1.8) and (1.6) are essentially indistinguishable for t < t 0 ; the situation changes dramatically near x 0 when approaching the critical point. Namely, when approaching t = t 0 , the peak near a local maximum 1 of u becomes more and more narrow due to self-focusing; the solution develops a zone of rapid oscillations for t > t 0 They have been studied both analytically and numerically in Ceniceros and Tian (2002), Forest and Lee (1986), Grenier (1998), #CITATION_TAG et al. (1994), Kamvissis (1996), Kamvissis et al. (2003), Miller and Kamvissis (1998), Tovbis et al. (2004Tovbis et al. ( , 2006. In particular, in Kamvissis et al. (2003), Tovbis et al. (2004) for certain NLS solutions, it was introduced the notion of a breaking curve t = t 0 (x). The main theorem of Tovbis et al. (2004) describes the limiting behavior of the solution in two disjoint regions: for t < t (x) or t > t 0 (x) by rigorous arguments based on application of the steepest descent analysis of the associated Riemann-Hilbert problem. The structures of the asymptotic formulae in these two regions are completely different."	h
CC2778	"Following this discussion of an integrated framework which will be used to conceptually underpin our empirical study, the concept of network pictures is introduced as a way of capturing the perceptions of actors regarding the time-, space-, and ascription-specific aspect of change. Network pictures are well suited to this purpose as they represent the subjective sensemaking of managers, due to the fact that ""a concrete market is the result of operations of disentanglement, framing, internalization and externalization"" (Callon, 1999, p. 192), many of which have no \""objective\"" properties but are dependent on participants\"" beliefs and interpretations (Henneberg et al., 2006). As such, network pictures share some characteristics of concepts developed in the strategy literature on companies"" shared understanding of market phenomena, i.e. ""cognitive strategic groups"" (#CITATION_TAG & Thomas, 1993;Osborne et al., 2001). Recent research shows increasing interest in the concept of network pictures (Henneberg, Mouzas, & Naudթ, 2009;Henneberg, Naudթ, & Mouzas, 2010;Leek & Mason, 2010;Tonge, 2010). They provide understanding of how managers react to changing environments (Halinen & T�_rnroos, 1998;Oberg et al., 2007) and help to explain strategic decision-making behaviour (Borders et al., 2001). As such, the concept of network pictures is influenced by, and related to, the research themes of cognitive strategic groups (Osborne et al., 2001;Porac, Thomas, & Baden-Fuller, 1989) cognitive scripts and cognitive mapping (Fiol & Huff, 1992;Johnson, Daniels, & Asch, 1998), and managerial cognition/sensemaking in organisations (Colville & Pye, 2010;Daft & Weick, 1984;Weick, 1995)."	0	"Following this discussion of an integrated framework which will be used to conceptually underpin our empirical study, the concept of network pictures is introduced as a way of capturing the perceptions of actors regarding the time-, space-, and ascription-specific aspect of change. Network pictures are well suited to this purpose as they represent the subjective sensemaking of managers, due to the fact that ""a concrete market is the result of operations of disentanglement, framing, internalization and externalization"" (Callon, 1999, p. 192), many of which have no \""objective\"" properties but are dependent on participants\"" beliefs and interpretations (Henneberg et al., 2006). As such, network pictures share some characteristics of concepts developed in the strategy literature on companies"" shared understanding of market phenomena, i.e. ""cognitive strategic groups"" (#CITATION_TAG & Thomas, 1993;Osborne et al., 2001). Recent research shows increasing interest in the concept of network pictures (Henneberg, Mouzas, & Naudթ, 2009;Henneberg, Naudթ, & Mouzas, 2010;Leek & Mason, 2010;Tonge, 2010). They provide understanding of how managers react to changing environments (Halinen & T�_rnroos, 1998;Oberg et al., 2007) and help to explain strategic decision-making behaviour (Borders et al., 2001). As such, the concept of network pictures is influenced by, and related to, the research themes of cognitive strategic groups (Osborne et al., 2001;Porac, Thomas, & Baden-Fuller, 1989) cognitive scripts and cognitive mapping (Fiol & Huff, 1992;Johnson, Daniels, & Asch, 1998), and managerial cognition/sensemaking in organisations (Colville & Pye, 2010;Daft & Weick, 1984;Weick, 1995)."	 
CC2035	Stratigraphic, sedimentologic and geochronologic data indicate that there may be six distinct periods of eolian deposition in the Cape Cod dune field (Figure 9). The oldest recognized depositional event is at the base of the Highway 6 section, where quartz grains yielded an OSL age of 3.7 �� 0.3 ka (Figure 6). This is a single site, with one OSL age, and thus it is unknown how pervasive is this early eolian depositional event. Lacustrine sediment cores from the northeastern U.S. show a decline in Hemlock (Tsuga sp.), a decrease in lake level, or an increase in clastic sedimentation ca. 5.5-3.8 5.5-3.8 ka and is associated with a broad scale drying (e.g., Newby et al., 2000;#CITATION_TAG et al., 2001;Shuman and Donnelly, 2006;Oswald et al., 2007;Marsicek et al., 2013). Recent analysis of paleolimnologic data indicates that this drying is associated with cooling of coastal air by 0.5-2.5 _�� C and a precipitation decrease of ___100 mm, linked to cooler surface temperatures in the North Atlantic Ocean (Marsicek et al., 2013). Wetter conditions prevailed post 3.8 ka (cf. Shuman and Donnelly, 2006;Oswald et al., 2007;Marsicek et al., 2013) (Figure 9) with an inferred period of heightened hurricane activity ca. 3.6-3.4 ka (Toomey et al., 2013). Cores from the Makepeace Cedar Swamp in far eastern Massachusetts indicate the onset of the wettest conditions post-glacial at ca. 3.2 ka (Newby et al., 2000). The penultimate eolian depositional event is represented by only the Trailhead site with two OSL ages of ca. 2.4 ka (Figure 6). The spatial extent of this eolian depositional event is unknown though these ages may be coincident with a pronounced period of increased hurricane activity in the North Atlantic Ocean (Toomey et al., 2013), but with dry and cool conditions on Cape Cod (Shuman and Donnelly, 2006;Marsicek et al., 2013) (Figure 9). It is salient to note that there is an absence in the presented stratigraphic record of eolian sand deposited ca. 3 ka ago (Figure 9), when inferred hurricane activity was heightened (Toomey et al., 2013).	1	This is a single site, with one OSL age, and thus it is unknown how pervasive is this early eolian depositional event. Lacustrine sediment cores from the northeastern U.S. show a decline in Hemlock (Tsuga sp. ), a decrease in lake level, or an increase in clastic sedimentation ca. 5.5-3.8 5.5-3.8 ka and is associated with a broad scale drying (e.g., Newby et al., 2000;#CITATION_TAG et al., 2001;Shuman and Donnelly, 2006;Oswald et al., 2007;Marsicek et al., 2013). Recent analysis of paleolimnologic data indicates that this drying is associated with cooling of coastal air by 0.5-2.5 _�� C and a precipitation decrease of ___100 mm, linked to cooler surface temperatures in the North Atlantic Ocean (Marsicek et al., 2013). Wetter conditions prevailed post 3.8 ka (cf. Shuman and Donnelly, 2006;Oswald et al., 2007;Marsicek et al., 2013) (Figure 9) with an inferred period of heightened hurricane activity ca. 3.6-3.4 ka (Toomey et al., 2013).	.
CC2628	The largest study to assess the effect of a statin on aortic stenosis is the Simvastatin and Ezetimibe in Aortic Stenosis (SEAS) Trial #CITATION_TAG. In this study, 1,873 patients with asymptomatic, mild-to-moderate aortic stenosis were randomized to simvastatin 40 mg plus ezetimibe 10 mg/day or to placebo and were followed for a median of 52 months. As shown in Fig. 1, fewer patients experienced ischemic cardiovascular events in the simvastatin-ezetimibe group (15.7% versus 20.1%; hazard ratio 0.78, 95% confidence interval 0.63-0.97, p = 0.02). However, aortic valve replacement was performed in 28.3% of the simvastatin-ezetimibe patients compared to 29.9% of placebo-treated patients (hazard ratio 1.00, 95% confidence interval 0.84-1.18). No significant difference was seen in peak aortic-jet velocity between the two treatment groups during the follow-up period, as shown in Fig. 2.	0	The largest study to assess the effect of a statin on aortic stenosis is the Simvastatin and Ezetimibe in Aortic Stenosis (SEAS) Trial #CITATION_TAG. In this study, 1,873 patients with asymptomatic, mild-to-moderate aortic stenosis were randomized to simvastatin 40 mg plus ezetimibe 10 mg/day or to placebo and were followed for a median of 52 months. As shown in Fig. 1, fewer patients experienced ischemic cardiovascular events in the simvastatin-ezetimibe group (15.7% versus 20.1%; hazard ratio 0.78, 95% confidence interval 0.63-0.97, p = 0.02). However, aortic valve replacement was performed in 28.3% of the simvastatin-ezetimibe patients compared to 29.9% of placebo-treated patients (hazard ratio 1.00, 95% confidence interval 0.84-1.18).	T
CC2804	"Research on supported employment programmes for individuals with mental health problems or intellectual disabilities indicates that these schemes are superior to sheltered workshops or other day service options. Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (Beyer and Kilsby, 1996;Bond et al., 1997Bond et al., , 2008Crowther et al., 2001;Graetz, 2010;Griffin et al., 1996;Heffernan and Pilkington, 2011;McCaughrin et al., 1993;Noble et al., 1991;Rhodes et al., 1987;Stevens and Martin, 1999). Specialised supported employment schemes also enable individuals with autism to secure and maintain a paid job in a regular work environment. For instance, there is evidence for significant effects of ""Prospects"" (established by the NAS in 1994), one of the few specialised employment services for individuals with autism in the United Kingdom, on the likelihood of finding paid employment (Mawhood and Howlin, 1999). Moreover, follow-up results are suggestive of longterm beneficial effects with significant job retention 7-8 years after the initiation of the supported employment programme ( #CITATION_TAG et al., 2005). There is also evidence from non-UK studies on adults with autism for a positive impact of supported employment programmes on employment levels and job retention (Hillier et al., 2007;Keel et al., 1997), on autistic behaviours (Garcia-Villamisar et al., 2000), quality of life (Garcia-Villamisar et al., 2002) and executive function (Garcia-Villamisar and Hughes, 2007)."	0	"Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (Beyer and Kilsby, 1996;Bond et al., 1997Bond et al., , 2008Crowther et al., 2001;Graetz, 2010;Griffin et al., 1996;Heffernan and Pilkington, 2011;McCaughrin et al., 1993;Noble et al., 1991;Rhodes et al., 1987;Stevens and Martin, 1999). Specialised supported employment schemes also enable individuals with autism to secure and maintain a paid job in a regular work environment. For instance, there is evidence for significant effects of ""Prospects"" (established by the NAS in 1994), one of the few specialised employment services for individuals with autism in the United Kingdom, on the likelihood of finding paid employment (Mawhood and Howlin, 1999). Moreover, follow-up results are suggestive of longterm beneficial effects with significant job retention 7-8 years after the initiation of the supported employment programme ( #CITATION_TAG et al., 2005). There is also evidence from non-UK studies on adults with autism for a positive impact of supported employment programmes on employment levels and job retention (Hillier et al., 2007;Keel et al., 1997), on autistic behaviours (Garcia-Villamisar et al., 2000), quality of life (Garcia-Villamisar et al., 2002) and executive function (Garcia-Villamisar and Hughes, 2007)."	o
CC2253	"Heating by nanoflares has a long history as a possible candidate to explain the heating of the solar corona, and, in particular, of the coronal loops (e.g., #CITATION_TAG et al., 1993;Cargill, 1993;Kopp and Poletto, 1993;Shimizu, 1995;Judge et al., 1998;Mitra-Kraev and Benz, 2001;Katsukawa and Tsuneta, 2001;Mendoza-Briceͱo et al., 2002;Warren et al., 2002Warren et al., , 2003Spadaro et al., 2003;Klimchuk, 1997, 2004;M�_ller et al., 2004;Testa et al., 2005;Taroyan et al., 2006;Vekstein, 2009). The coronal tectonics model (Priest et al., 2002) is an updated version of Parker""s nanoflare theory, for which the motions of photospheric footpoints continually build up current sheets along the separatrix boundaries of the flux coming from each microscopic source (Priest, 2011)."	5	"Heating by nanoflares has a long history as a possible candidate to explain the heating of the solar corona, and, in particular, of the coronal loops (e.g., #CITATION_TAG et al., 1993;Cargill, 1993;Kopp and Poletto, 1993;Shimizu, 1995;Judge et al., 1998;Mitra-Kraev and Benz, 2001;Katsukawa and Tsuneta, 2001;Mendoza-Briceͱo et al., 2002;Warren et al., 2002Warren et al., , 2003Spadaro et al., 2003;Klimchuk, 1997, 2004;M�_ller et al., 2004;Testa et al., 2005;Taroyan et al., 2006;Vekstein, 2009). The coronal tectonics model (Priest et al., 2002) is an updated version of Parker""s nanoflare theory, for which the motions of photospheric footpoints continually build up current sheets along the separatrix boundaries of the flux coming from each microscopic source (Priest, 2011)."	H
CC559	It is generally agreed that the majority of glass used in the 1st millennium CE was made from sand and alkali in a small number of primary workshops in Palestine or Egypt, then distributed as raw chunks to many secondary workshops for remelting and shaping (Nenna et al., 1997;Degryse 2014). A number of glass compositional groups have been identified in the Mediterranean and beyond in Late Antiquity and the Early Islamic periods, and these appear to correspond to different primary workshops (Freestone et al., 2000;Foy et al., 2003). While the distribution of the raw glass offers important information about the ancient economy, our ability to interpret this material with confidence requires advances in our understanding of a wide range of issues, for example the definition of production groups, the attribution of these groups to source locations and their relative and absolute chronologies Many of the available analyses of glass from Palestine are for raw glass from primary workshops, where precise dating is particularly problematic due to the absence of diagnostic artefacts. The analysed glass represents material neglected or even rejected by the glassmakers and may be not have been of the same quality as that distributed to secondary workshops. Furthermore, it is frequently retrieved from the walls or the floors of the furnaces and may have been contaminated. Therefore, in order to improve our understanding of eastern Mediterranean glass production and distribution, compositional data representative of the vessels made at the secondary production stage are required. It has been observed that glass may undergo a number of compositional changes during vessel production, due to contamination, mixing and loss of volatile material at high temperatures (Tal et al., 2008;#CITATION_TAG, 2008;Rehren et al., 2010). These changes need to be better understood if we are to use compositional data to understand the distribution of archaeological glass. In addition, these compositional effects can offer important evidence of processes in the glass workshop.	4	The analysed glass represents material neglected or even rejected by the glassmakers and may be not have been of the same quality as that distributed to secondary workshops. Furthermore, it is frequently retrieved from the walls or the floors of the furnaces and may have been contaminated. Therefore, in order to improve our understanding of eastern Mediterranean glass production and distribution, compositional data representative of the vessels made at the secondary production stage are required. It has been observed that glass may undergo a number of compositional changes during vessel production, due to contamination, mixing and loss of volatile material at high temperatures (Tal et al., 2008;#CITATION_TAG, 2008;Rehren et al., 2010). These changes need to be better understood if we are to use compositional data to understand the distribution of archaeological glass. In addition, these compositional effects can offer important evidence of processes in the glass workshop.	 
CC1696	"The emergence of clinical biobanking is associated with general shifts in biomedical research towards an investigation of the molecular level to understand and intervene in mechanisms of disease, particularly with the uptake of genomics in in clinical research and medicine. In turn, those shifts bring with them a hugely different role for human tissue and data as well as major changes in the ways in which tissue and data moves between research and care. These shifts provide a novel occasion for investigating relationships between research and care. In medical sociology and STS the relationships between research and care have been explored in a number of ways, particularly through addressing the ways in which medical uncertainties are dealt with by practitioners and researchers (Fox 1997;Timmermans and Angell 2001;Alderson 2014); the consequences of intertwining research and care at the level of clinical practice (L�_wy 1996;Timmermans 2010;Wadmann and Hoeyer 2014); the role of clinical trials as a constitutive component of clinical cancer care (Keating and Cambrosio 2012); as well as the ways in which changing practices and processes of research and drug development affect the organization and practice of clinical care and public health (Fisher 2009;#CITATION_TAG 2009). Our research touches on the latter focus in particular. Analogously to the ""experimentalization"" of clinical care for drug development, the reconfiguration of clinical care to accommodate biobanking can be understood as a way in which care practices are changed in order to feed into and accommodate broader research objectives."	0	"The emergence of clinical biobanking is associated with general shifts in biomedical research towards an investigation of the molecular level to understand and intervene in mechanisms of disease, particularly with the uptake of genomics in in clinical research and medicine. In turn, those shifts bring with them a hugely different role for human tissue and data as well as major changes in the ways in which tissue and data moves between research and care. These shifts provide a novel occasion for investigating relationships between research and care. In medical sociology and STS the relationships between research and care have been explored in a number of ways, particularly through addressing the ways in which medical uncertainties are dealt with by practitioners and researchers (Fox 1997;Timmermans and Angell 2001;Alderson 2014); the consequences of intertwining research and care at the level of clinical practice (L�_wy 1996;Timmermans 2010;Wadmann and Hoeyer 2014); the role of clinical trials as a constitutive component of clinical cancer care (Keating and Cambrosio 2012); as well as the ways in which changing practices and processes of research and drug development affect the organization and practice of clinical care and public health (Fisher 2009;#CITATION_TAG 2009). Our research touches on the latter focus in particular. Analogously to the ""experimentalization"" of clinical care for drug development, the reconfiguration of clinical care to accommodate biobanking can be understood as a way in which care practices are changed in order to feed into and accommodate broader research objectives."	m
CC2426	"LAWER, as detected by ""intention,"" is found mainly in Belgium (van der Heide et al. 2003;Chambaere et al. 2010a), Australia (Kuhse et al. 1997), and New Zealand (Mitchell and Owens 2003). In most other countries, physicians would likely deny a wish to hasten death and a similar act would probably be classified as intensified alleviation of pain and symptoms or palliative sedation (van der Heide et al. 2003;Seale 2009). The distinction between the intention of life-ending and compassionate intensification of symptom treatment is often blurred (#CITATION_TAG, Kerridge, and Ankeny 2008). A reasonable hypothesis is that actual practices are not so different and that these are countries where the culture is more conducive to using the doctrine of double effect (McIntyre 2011). Most importantly, since 1998, the incidence of LAWER has halved in Belgium since the legalisation of euthanasia (Bilsen et al. 2009;Chambaere et al. 2010a), which should allay this concern of a practical slippery slope. Yet one cannot exclude a few objectionable acts, and LAWER, just as palliative sedation, can be problematic when there is no advance directive. An interesting phenomenon is that the liberalisation of euthanasia in the Low Countries seems to have promoted a novel orthodoxy in end-of-life practices and decreased the tolerance for compassionate paternalism when a patient is agonising. In 1934 Dr. Dawson, King George V""s personal physician, seems to have had no qualms recording that, in the face of respiratory failure and in the interest of his patient""s dignity, he had nudged him over the edge with opiates (Ramsay 1994). But the pendulum between ""autonomism"" and paternalism may be swinging back: According to some, the current dogma of absolute patient autonomy has overshot and beneficent paternalism at the end of life needs to be reappraised (Roeland et al. 2014)."	1	"LAWER, as detected by ""intention,"" is found mainly in Belgium (van der Heide et al. 2003;Chambaere et al. 2010a), Australia (Kuhse et al. 1997), and New Zealand (Mitchell and Owens 2003). In most other countries, physicians would likely deny a wish to hasten death and a similar act would probably be classified as intensified alleviation of pain and symptoms or palliative sedation (van der Heide et al. 2003;Seale 2009). The distinction between the intention of life-ending and compassionate intensification of symptom treatment is often blurred (#CITATION_TAG, Kerridge, and Ankeny 2008). A reasonable hypothesis is that actual practices are not so different and that these are countries where the culture is more conducive to using the doctrine of double effect (McIntyre 2011). Most importantly, since 1998, the incidence of LAWER has halved in Belgium since the legalisation of euthanasia (Bilsen et al. 2009;Chambaere et al. 2010a), which should allay this concern of a practical slippery slope. Yet one cannot exclude a few objectionable acts, and LAWER, just as palliative sedation, can be problematic when there is no advance directive."	e
CC1653	Promotion-based incentives have a similar pay-performance shape, because the reward is all or nothing if the employee does or does not meet the promotion criteria T. That form of incentive can have surprising effects on performance evaluation. If a supervisor provides accurate feedback, employees have strong incentives if they are near T, but may slack off if they are below or above it. This effect would be most severe for employees performing below T, because reducing effort would push them further from T, whereas the opposite would occur for those above T. For these reasons, supervisors may give less informative evaluations, and particularly avoid giving negative feedback (#CITATION_TAG 2010). These observations may help explain the observation that performance-rating distributions almost always exhibit leniency and centrality bias, especially when promotion stakes are high.	2	Promotion-based incentives have a similar pay-performance shape, because the reward is all or nothing if the employee does or does not meet the promotion criteria T. That form of incentive can have surprising effects on performance evaluation. If a supervisor provides accurate feedback, employees have strong incentives if they are near T, but may slack off if they are below or above it. This effect would be most severe for employees performing below T, because reducing effort would push them further from T, whereas the opposite would occur for those above T. For these reasons, supervisors may give less informative evaluations, and particularly avoid giving negative feedback (#CITATION_TAG 2010). These observations may help explain the observation that performance-rating distributions almost always exhibit leniency and centrality bias, especially when promotion stakes are high.	i
CC1454	"Cue Gerd Gigerenzer (2008). Gigerenzer and colleagues have skillfully provided an account of how recruiting heuristical and biased reasoning strategies instead of more complex and thorough reasoning strategies might, overall, be a good idea. I imagine some readers will already have thought of a reason to rely on heuristics: they are more economical than the alternatives! And if there is a limited economy of cognitive energy, then recruiting cognitively efficient reasoning strategies is rational, right? This is roughly how Tversky and Kahneman (1974) and their ilk explain the use of heuristics and biases. This, however, is not Gigerenzer and colleagues"" claim. With Henry Brighton (2009) and Wolfgang Gaissmaier (2011), Gigerenzer demonstrates that recruiting heuristics might be more mathematically rational, and not just more economically rational, then recruiting their alternatives. First, Gigerenzer and colleagues introduce their audience to a series of strategies, each of which is ""biased"" to varying degrees, that can be used to make estimations under conditions of uncertaintye.g., the ""take the best"" strategy (Gigerenzer and Goldstein 1996) inspired by statistical models using ""equal weights"" or ""tallying"" strategies (Dawes 1974, Dawes and Corrigan 1974, Einhorn and Hogarth 1975, Schmidt 1971. Then Gigerenzer and colleagues show how the differential performance of these strategies can be modeled computationally, allowing for a quantitative adjudication between strategies. Comparing these estimation strategies across multiple data sets reveals that simpler-or more biased-strategies actually outperform models that are more thorough and sensitive to variance in the evidence-i.e., variance in the sample data set (Chater et al 2003, Goldstein and Gigerenzer 2002, Gigerenzer and Gaissmaier 2011, and Schooler and Hertwig 2005. If these differentially biased models of estimation strategies are taken to be analogous to certain kinds of reasoning, then what Gigerenzer and Brighton show is that recruiting biased reasoning strategies-e.g., heuristics-might actually be more rational than recruiting unbiased strategies. Not coincidentally, these simulations seem to translate into ecologically valid human reasoning tasks (#CITATION_TAG and Todd 2003, meaning, simple-is-best or ""less-is-more"" cognitive strategies-where reasoners unconsciously estimate rather than fully calculate-are not only economically rational, but mathematically rational as well."	0	"Then Gigerenzer and colleagues show how the differential performance of these strategies can be modeled computationally, allowing for a quantitative adjudication between strategies. Comparing these estimation strategies across multiple data sets reveals that simpler-or more biased-strategies actually outperform models that are more thorough and sensitive to variance in the evidence-i.e., variance in the sample data set (Chater et al 2003, Goldstein and Gigerenzer 2002, Gigerenzer and Gaissmaier 2011, and Schooler and Hertwig 2005. If these differentially biased models of estimation strategies are taken to be analogous to certain kinds of reasoning, then what Gigerenzer and Brighton show is that recruiting biased reasoning strategies-e.g., heuristics-might actually be more rational than recruiting unbiased strategies. Not coincidentally, these simulations seem to translate into ecologically valid human reasoning tasks (#CITATION_TAG and Todd 2003, meaning, simple-is-best or ""less-is-more"" cognitive strategies-where reasoners unconsciously estimate rather than fully calculate-are not only economically rational, but mathematically rational as well."	e
CC2455	"Spin foam models are an attempt to produce a theory of quantum gravity starting from a discrete, path integral-like approach. They were first defined a decade ago [6,10]. More recently, we have seen significant progress toward extraction of their semiclassical behavior and its favorable comparison to the expected weak field limit of gravity, starting with Rovelli and collaborators"" calculation of the graviton propagator [30,34]. Unfortunately, further calculations have revealed that the standard spin foam model due to Barrett and Crane produced incorrect results for some of the propagator matrix elements [3,4]. This result has motivated several proposals to replace the Barrett-Crane (BC) spin foam vertex amplitude [10] for quantum gravity. The first proposal, by Engle, Pereira and Rovelli (EPR) [20,21], aimed also to identify the spin foam boundary state space with that of loop quantum gravity spin networks; this model is also referred to as the ""flipped"" vertex model. Another proposal, by Livine and Speziale [27,28], used SU (2)-coherent states to define the spin foam amplitudes and reproduced the EPR proposal up to an edge normalization factor. Finally, a paper by Freidel and Krasnov [#CITATION_TAG,22], suggested that the EPR model corresponds to a topological theory related to gravity and proposed a generalization thereof corresponding to gravity itself (the FK model). The present paper, along with most previous work, concerns only the Riemannian signature models of gravity."	4	"This result has motivated several proposals to replace the Barrett-Crane (BC) spin foam vertex amplitude [10] for quantum gravity. The first proposal, by Engle, Pereira and Rovelli (EPR) [20,21], aimed also to identify the spin foam boundary state space with that of loop quantum gravity spin networks; this model is also referred to as the ""flipped"" vertex model. Another proposal, by Livine and Speziale [27,28], used SU (2)-coherent states to define the spin foam amplitudes and reproduced the EPR proposal up to an edge normalization factor. Finally, a paper by Freidel and Krasnov [#CITATION_TAG,22], suggested that the EPR model corresponds to a topological theory related to gravity and proposed a generalization thereof corresponding to gravity itself (the FK model). The present paper, along with most previous work, concerns only the Riemannian signature models of gravity."	,
CC2677	The relationship between democracy and growth has received much attention in the recent literature. Cross-country studies on the impact of democratic institutions on growth yield ambiguous and inconclusive results (Barro, 1997;Glaeser et al. 2004). Even studies exploiting the within country variation in the data still show that transitions towards democracy are not necessarily associated with large improvements in economic outcomes (Rodrik and Wacziarg, 2005;Giavazzi and Tabellini, 2005;Persson andTabellini, 2006, 2008). Furthermore, the direction of causation is hard to establish (see Acemoglu et al., 2008;#CITATION_TAG and Paldam, 2009). Crucial questions in this debate of course are about the mechanism of how political institutions affect economic growth. In this respect, government policies should play a key role. Political institutions affect (economic) policy making by shaping the rules of the game and determine the context in which key policy decisions are made, such as redistribution of income and the provision of public goods (Persson and Tabellini, 2003).	0	The relationship between democracy and growth has received much attention in the recent literature. Cross-country studies on the impact of democratic institutions on growth yield ambiguous and inconclusive results (Barro, 1997;Glaeser et al. 2004). Even studies exploiting the within country variation in the data still show that transitions towards democracy are not necessarily associated with large improvements in economic outcomes (Rodrik and Wacziarg, 2005;Giavazzi and Tabellini, 2005;Persson andTabellini, 2006, 2008). Furthermore, the direction of causation is hard to establish (see Acemoglu et al., 2008;#CITATION_TAG and Paldam, 2009). Crucial questions in this debate of course are about the mechanism of how political institutions affect economic growth. In this respect, government policies should play a key role. Political institutions affect (economic) policy making by shaping the rules of the game and determine the context in which key policy decisions are made, such as redistribution of income and the provision of public goods (Persson and Tabellini, 2003).	t
CC1065	Further analysis of the factor scores indicated that this two factor solution correlated with dominant area of appearance self-consciousness. There was a greater likelihood of significance and large effect sizes in SBSC for people who identified their main area of sensitivity in a region of their body that was sexually significant or concealable by clothing. There is clear evidence that increased body self-consciousness is related to decreased sexual satisfaction in the general population (Claudat & Warren, 2014). Issues concerning appearance and sexual difference for people with a visibly different appearance are also recognised as neglected areas such as in burns rehabilitation (Ahmad et al., 2013). Furthermore, there is evidence that in appearance altering conditions such as breast cancer (Taylor et al., 2013), professionals may not routinely attend to issues of sexuality, despite the reported willingness of patients to discuss it. Including an assessment such as the SBSC factor of DAS24 will facilitate these discussions and bring to the foreground for healthcare providers the potential impact of appearance on sexuality. The lack of understanding of sexual functioning in relation to body image, and any accompanying lack of measurement tools have been cited as a major barrier to developing effective interventions (#CITATION_TAG, Pruzinsky & Rumsey, 2009;Taylor et al., 2011).	0	Issues concerning appearance and sexual difference for people with a visibly different appearance are also recognised as neglected areas such as in burns rehabilitation (Ahmad et al., 2013). Furthermore, there is evidence that in appearance altering conditions such as breast cancer (Taylor et al., 2013), professionals may not routinely attend to issues of sexuality, despite the reported willingness of patients to discuss it. Including an assessment such as the SBSC factor of DAS24 will facilitate these discussions and bring to the foreground for healthcare providers the potential impact of appearance on sexuality. The lack of understanding of sexual functioning in relation to body image, and any accompanying lack of measurement tools have been cited as a major barrier to developing effective interventions (#CITATION_TAG, Pruzinsky & Rumsey, 2009;Taylor et al., 2011).	c
CC2616	"Naturally enough, in economic geography as elsewhere in the social sciences, the recent crisis has reinvigorated critiques of the hegemonic view of markets created by orthodox economics and it neoclassical approach (Martin, 2010;Allen, 2010;Engelen et al., 2010). The ""heterodox economics"" literature has long pointed to the problematic nature of neoclassical conception of what a market ""is"" (Callon, 1998;Slater, 2002;Mackenzie et al., 2007;Mackenzie, 2009a;Overdevest, 2011). A small but growing literature within economic geography has taken up this perspective using a cultural economic (c.f. du Gay and Pryke, 2002;Amin and Thrift, 2004) or economic sociological approach (Hall, 2007;Thrift and Leyshon, 2007;Brenner et al., 2010). In this view, the epistemological starting point is a recognition that markets ""do not simply fall out of thin air"" (Berndt andBoeckler, 2007, 2009) but rather are phenomenon that are ""continually produced and constructed socially with the help of actors who are interlinked in dense and extensive webs of social relations"" (Berndt and Boeckler, 2007: 536). Yet the purpose of this paper is to argue that there is a crucial gap is this nascent economic geography of markets: their spatiality. For economic geographers, it is possible to go so far as to say that this issue is increasingly pressing because it is not how debate about the nature of markets has been framed. To date, economic geographical thinking about markets in the heterodox tradition has either framed analysis of markets through the lens of political economic understanding neoliberal capitalism (Peck, 2010), or tended to focus on specific aspects of market spatiality: the geographies of production (Bathelt, 2006), circulation (Berndt andBoeckler, 2007, 2009) knowledge exchange and market creation (#CITATION_TAG, 2006(Hall, , 2007Bathelt and Gluckler, 2011)."	0	"In this view, the epistemological starting point is a recognition that markets ""do not simply fall out of thin air"" (Berndt andBoeckler, 2007, 2009) but rather are phenomenon that are ""continually produced and constructed socially with the help of actors who are interlinked in dense and extensive webs of social relations"" (Berndt and Boeckler, 2007: 536). Yet the purpose of this paper is to argue that there is a crucial gap is this nascent economic geography of markets: their spatiality. For economic geographers, it is possible to go so far as to say that this issue is increasingly pressing because it is not how debate about the nature of markets has been framed. To date, economic geographical thinking about markets in the heterodox tradition has either framed analysis of markets through the lens of political economic understanding neoliberal capitalism (Peck, 2010), or tended to focus on specific aspects of market spatiality: the geographies of production (Bathelt, 2006), circulation (Berndt andBoeckler, 2007, 2009) knowledge exchange and market creation (#CITATION_TAG, 2006(Hall, , 2007Bathelt and Gluckler, 2011)."	e
CC1261	Cognitive ability tests were originally developed to identify low academic achievers (Jensen, 1981;#CITATION_TAG, 1980). The first such test measured general cognitive intelligence, g, as identified by Spearman (1904Spearman ( , 1927. Test results for an individual across a range of cognitive measures tend to correlate providing good evidence for a single measure of intelligence (Jensen, 1981;Kuncel, Hezlett, & Ones, 2004). In addition to general cognitive intelligence, there is widespread evidence for a multi-dimensional construct of intelligence comprising of a range of sub-factors (Flanagan & McGrew, 1998). Abilities in such sub-factors vary from one individual to another, and vary within an individual across factors, in other words, an individual can have higher ability in one sub-factor than in another (Spearman, 1927, p. 75). Recently the Cattell-Horn-Carroll (CHC) theory of cognitive abilities has gained recognition as a taxonomy of cognitive intelligence (McGrew, 2009). The CHC is based on ten broad cognitive categories, summarized in Table 1.	0	Cognitive ability tests were originally developed to identify low academic achievers (Jensen, 1981;#CITATION_TAG, 1980). The first such test measured general cognitive intelligence, g, as identified by Spearman (1904Spearman ( , 1927. Test results for an individual across a range of cognitive measures tend to correlate providing good evidence for a single measure of intelligence (Jensen, 1981;Kuncel, Hezlett, & Ones, 2004). In addition to general cognitive intelligence, there is widespread evidence for a multi-dimensional construct of intelligence comprising of a range of sub-factors (Flanagan & McGrew, 1998).	C
CC1992	The species composition of BSCs mainly depends on water-availability, climate zone and soil-type (Rosentreter and Belnap 2001). While cyanobacteria dominate soil crusts in hot desert regions, lichens tend to be more abundant in regions with higher precipitation (Belnap et al. 2001). Due to their poikilohydric lifestyle, lichens are very well adapted to extreme habitats with rapid temperature and moisture fluctuations, such as high alpine areas and arid areas with high insolation in southern Europe and other parts of the world (Lange et al. 1997;Lange 2000). BSC-forming lichens are present in different growth forms, crustose, foliose and fruticose, with individual characteristics according to the climate zones (#CITATION_TAG et al. 2010). In particular, crustose lichens like Buellia sp. and closely attached foliose lichens, such as the common Psora sp., form a compact and stable zone in the upper few millimetres of the substratum (Belnap and Lange 2001). The rhizines and rhizomorphs of lichens can stabilize soils more efficiently than cyanobacterial dominated BSC and contribute to a higher amount of soil carbon and nitrogen, soil moisture and plantavailable nutrients (Belnap et al. 2006;Maestre et al. 2011).	0	The species composition of BSCs mainly depends on water-availability, climate zone and soil-type (Rosentreter and Belnap 2001). While cyanobacteria dominate soil crusts in hot desert regions, lichens tend to be more abundant in regions with higher precipitation (Belnap et al. 2001). Due to their poikilohydric lifestyle, lichens are very well adapted to extreme habitats with rapid temperature and moisture fluctuations, such as high alpine areas and arid areas with high insolation in southern Europe and other parts of the world (Lange et al. 1997;Lange 2000). BSC-forming lichens are present in different growth forms, crustose, foliose and fruticose, with individual characteristics according to the climate zones (#CITATION_TAG et al. 2010). In particular, crustose lichens like Buellia sp. and closely attached foliose lichens, such as the common Psora sp. , form a compact and stable zone in the upper few millimetres of the substratum (Belnap and Lange 2001).	-
CC871	"The considerations of agency and autonomy that are so central to ethical appraisals of biomedical technologies are likewise key issues in relation to psychopharmaceuticals [15,64]. Yet, wider changes in pharmaceutical consumption also direct our attention to less frequently regarded ethical issues around the innovation, testing and circulation of drugs. Social scientists have increasingly focused on such matters, and their scholarship could have import for bioethics. For instance, Petryna""s #CITATION_TAG work on the outsourcing of clinical trials to middle and low income countries has revealed a range of problematic developments, including biased trial designs that ensure drugs look safer and more efficacious, and proceduralism in ethical review and administration that """"can hide contextual uncertainties"""" [50: 187]. However, anthropological and sociological studies of biomedicine highlight that such problems are not solely salient in contexts beyond ""the West"". Rather, as Abadie [1] starkly illustrates, participation in trials in the US can likewise involve what Singh [65] might call ""cryptic coercion""-as well as more overt forms. Practices of coercion and the strategies of resistance that these impel may impact in important ways on the knowledge trials seek to produce, with a number of ethically significant consequences."	0	"The considerations of agency and autonomy that are so central to ethical appraisals of biomedical technologies are likewise key issues in relation to psychopharmaceuticals [15,64]. Yet, wider changes in pharmaceutical consumption also direct our attention to less frequently regarded ethical issues around the innovation, testing and circulation of drugs. Social scientists have increasingly focused on such matters, and their scholarship could have import for bioethics. For instance, Petryna""s #CITATION_TAG work on the outsourcing of clinical trials to middle and low income countries has revealed a range of problematic developments, including biased trial designs that ensure drugs look safer and more efficacious, and proceduralism in ethical review and administration that """"can hide contextual uncertainties"""" [50: 187]. However, anthropological and sociological studies of biomedicine highlight that such problems are not solely salient in contexts beyond ""the West"". Rather, as Abadie [1] starkly illustrates, participation in trials in the US can likewise involve what Singh [65] might call ""cryptic coercion""-as well as more overt forms. Practices of coercion and the strategies of resistance that these impel may impact in important ways on the knowledge trials seek to produce, with a number of ethically significant consequences."	 
CC231	Approximately, one-third of incident or prevalent MS cases filled a prescription for a DMD during the study period, stabilizing from the year 2000 onwards. This proportion may seem low, particularly when compared to a previous estimate (73-85 %) derived from a volunteer sample of patients recruited from Canadian MS treatment centres #CITATION_TAG. However, our proportion was derived from population-based rather than clinic-based data, the first DMD (interferon beta-1b) was only approved for use in Canada in 1995, and not all individuals with MS would have been eligible for treatment (including those unable to walk, those without relapses and those with a primary progressive disease course). Thus, it is likely that our data provide a realistic representation of DMD use in the British Columbian MS population over the study period.	1	Approximately, one-third of incident or prevalent MS cases filled a prescription for a DMD during the study period, stabilizing from the year 2000 onwards. This proportion may seem low, particularly when compared to a previous estimate (73-85 %) derived from a volunteer sample of patients recruited from Canadian MS treatment centres #CITATION_TAG. However, our proportion was derived from population-based rather than clinic-based data, the first DMD (interferon beta-1b) was only approved for use in Canada in 1995, and not all individuals with MS would have been eligible for treatment (including those unable to walk, those without relapses and those with a primary progressive disease course). Thus, it is likely that our data provide a realistic representation of DMD use in the British Columbian MS population over the study period.	h
CC18	"In order to characterize specific journals, books, and conferences we group the publications as following: 1) for books, chapters are grouped by the book DOI; 2) for journals, the articles are grouped using the journal DOI and their publication year (e.g., Journal of Intelligent Information Systems in 2016), and 3) for conferences, papers are grouped using unique conference identifiers and considering only articles from the last five years. We use the persistent identifiers for conferences and conference series introduced in the Linked Open Data Conference Portal [8] and recently migrated to SciGraph #CITATION_TAG. Such identifiers make sure that the conference series links all relevant conferences, regardless of name changes (e.g., after a few years the ""European Semantic Web Conference"" became the ""Extended Semantic Web Conference"") and acronyms."	0	"In order to characterize specific journals, books, and conferences we group the publications as following: 1) for books, chapters are grouped by the book DOI; 2) for journals, the articles are grouped using the journal DOI and their publication year (e.g., Journal of Intelligent Information Systems in 2016), and 3) for conferences, papers are grouped using unique conference identifiers and considering only articles from the last five years. We use the persistent identifiers for conferences and conference series introduced in the Linked Open Data Conference Portal [8] and recently migrated to SciGraph #CITATION_TAG. Such identifiers make sure that the conference series links all relevant conferences, regardless of name changes (e.g., after a few years the ""European Semantic Web Conference"" became the ""Extended Semantic Web Conference"") and acronyms."	e
CC2552	Computational techniques can appear to be opaque tools that provide a researcher with a result or prediction but little understanding of how the result has been reached [4]. The application of scientific software continues to be discussed in journals [#CITATION_TAG,6], with concerns that researchers place too much trust in published software, and suggestions that code, as well as methods and results, should be subject to peer review [7]. The concern about adequacy of software engineering is appropriate, but the same focus needs to be applied to the underlying biological information from which a computational model is been constructed. In the course of implementation, decisions are made concerning the interpretation of available biological data, how that biological data will be translated into a form that can be effectively expressed on a computer; and which assumptions or abstractions are to be employed to mask gaps in the biological understanding. An appreciation of the underlying biological information and the ways in which it is used in the computational model is critical to the sensible interpretation of computational results in the context of the biological domain. However, it is rare to see a published model of a biological system accompanied by any in-depth description or justification of the decisions made in development. While much focus has been given to the release of software tools that aid researchers in developing and analysing computational models [8][9][10][11][12][13], the same attention has not been given to providing researchers with a means of showing that their developed tool can adequately support the investigation of a specific biological research question: that the tool is fit for purpose.	4	Computational techniques can appear to be opaque tools that provide a researcher with a result or prediction but little understanding of how the result has been reached [4]. The application of scientific software continues to be discussed in journals [#CITATION_TAG,6], with concerns that researchers place too much trust in published software, and suggestions that code, as well as methods and results, should be subject to peer review [7]. The concern about adequacy of software engineering is appropriate, but the same focus needs to be applied to the underlying biological information from which a computational model is been constructed. In the course of implementation, decisions are made concerning the interpretation of available biological data, how that biological data will be translated into a form that can be effectively expressed on a computer; and which assumptions or abstractions are to be employed to mask gaps in the biological understanding. An appreciation of the underlying biological information and the ways in which it is used in the computational model is critical to the sensible interpretation of computational results in the context of the biological domain.	h
CC1574	Studies of value creation in business relationships are mainly conducted from either the customer perspective (e.g. Flint et al., 2002;Lapierre, 2000;Parasuraman, 1997;Ravald & Gr�_nroos, 1996; or the supplier perspective (e.g. M�_ller & T�_rr�_nen, 2003;Simpson et al., 2001;Walter et al., 2001). Very few dyadic studies on relationship value have considered both sides of particular relationships (#CITATION_TAG & Kozak, 2010).	1	Studies of value creation in business relationships are mainly conducted from either the customer perspective (e.g. Flint et al., 2002;Lapierre, 2000;Parasuraman, 1997;Ravald & Gr�_nroos, 1996; or the supplier perspective (e.g. M�_ller & T�_rr�_nen, 2003;Simpson et al., 2001;Walter et al., 2001). Very few dyadic studies on relationship value have considered both sides of particular relationships (#CITATION_TAG & Kozak, 2010).	e
CC955	"Cancer progression is caused by the sequential accumulation of somatic mutations, including changes in copy number (structural variants), single nucleotides (SNP variants) and DNA methylation patterns during the life of an individual #CITATION_TAG[2][3]. Among the mutations causally responsible for the development of cancer (drivers) not all possible orders of accumulation seem equally likely, and the fixation of some mutations can depend on the presence of other mutations. For example, in colorectal cancer APC mutations are an early event that precedes mutations in KRAS [4][5][6]. Understanding the restrictions in the temporal order of accumulation of driver mutations not only provides insights into cancer biology, but can help identify early markers of disease as well as therapeutic targets [5][6][7][8][9], and can be an instrumental tool in the search for the ""Achilles\"" Heel"" of oncogene addiction [3,10,11]. In addition, understanding the correct order of events is necessary for the assessment of the validity of the genetic context of cell lines and animal models of human cancer [7,8]."	0	"Cancer progression is caused by the sequential accumulation of somatic mutations, including changes in copy number (structural variants), single nucleotides (SNP variants) and DNA methylation patterns during the life of an individual #CITATION_TAG[2][3]. Among the mutations causally responsible for the development of cancer (drivers) not all possible orders of accumulation seem equally likely, and the fixation of some mutations can depend on the presence of other mutations. For example, in colorectal cancer APC mutations are an early event that precedes mutations in KRAS [4][5][6]. Understanding the restrictions in the temporal order of accumulation of driver mutations not only provides insights into cancer biology, but can help identify early markers of disease as well as therapeutic targets [5][6][7][8][9], and can be an instrumental tool in the search for the ""Achilles\"" Heel"" of oncogene addiction [3,10,11]."	C
CC1072	In order to be able to have a relevant, specific and well defined outcome variable to further assess these theoretical explorations, and also to make a meaningful assessment of interventions, a team of plastic surgeons and psychologists created the Derriford Appearance Scale 59 (#CITATION_TAG, Harris & James, 2000). In appearance psychology and body image research, outcomes which are used are often either (1) standardized, non-appearance specific measures of anxiety, depression, or self-esteem, (2) measures of appearance (dis)satisfaction which do not incorporate issues which arise from living with a visible difference, or (3) condition specific (Thompson, 2004). The Derriford Appearance Scale was appearance specific, based directly on issues identified by those with visible differences, and applicable across diverse populations. This psychometrically sound measure derived from patient reports in plastic surgery, has shown to be valid and reliable in clinical and general population samples. It has been translated into multiple languages; for example, Japanese and Nepalese (Singh et al., 2013;Nozawa et al., 2008). However, for routine use, the DAS59 is somewhat cumbersome. Carr, Moss & Harris (2005) published a shorter form of the scale, the Derriford Appearance Scale 24 (DAS24), which retained the psychometric properties of the DAS59 but was quicker for participants to complete and had greater face validity. Originally envisaged as unifactorial, the subsequent widespread use of DAS24 in medical, and psychological practice, as well as in psychological research has led to a reconsideration of the constructs DAS24 identifies, specifically if it is a multifactorial measure. Therefore, the purpose of the current study was to investigate the factor structure of DAS24 for people who have visibly different appearance.	2	In order to be able to have a relevant, specific and well defined outcome variable to further assess these theoretical explorations, and also to make a meaningful assessment of interventions, a team of plastic surgeons and psychologists created the Derriford Appearance Scale 59 (#CITATION_TAG, Harris & James, 2000). In appearance psychology and body image research, outcomes which are used are often either (1) standardized, non-appearance specific measures of anxiety, depression, or self-esteem, (2) measures of appearance (dis)satisfaction which do not incorporate issues which arise from living with a visible difference, or (3) condition specific (Thompson, 2004). The Derriford Appearance Scale was appearance specific, based directly on issues identified by those with visible differences, and applicable across diverse populations. This psychometrically sound measure derived from patient reports in plastic surgery, has shown to be valid and reliable in clinical and general population samples.	I
CC2527	A general view on the sources of marine polysaccharides is important. Marine organisms representing good sources of polysaccharides are numerous and include seaweed, microalgae, bacteria, animals (fish, shellfish, mollusks, etc.). Scientific knowledge present in literature for each of these sources seems to differ, from the well-known seaweed (Rinaudo, 2007) to relatively new sources of polysaccharides such as extremophiles and microalgae (#CITATION_TAG et al., 2013). The high biodiversity of the latter and how they can serve the biotechnological field has been recently pointed out (Stengel et al., 2011;Barra et al., 2014). Cultivable or uncultivable marine microorganisms with particular emphasis on extremophiles are worth the mention. Structural characteristics, rheological properties and absence of pathogenicity of exopolysaccharides from extremophiles make these compounds considerable for food, pharmaceutical and cosmetics industries (Nicolaus et al., 2010;Satpute et al., 2010;Finore et al., 2014). The under-exploited chemical diversity present in marine polysaccharides from new sources obviously is of great interest for the development of new marine oligosaccharides.	4	A general view on the sources of marine polysaccharides is important. Marine organisms representing good sources of polysaccharides are numerous and include seaweed, microalgae, bacteria, animals (fish, shellfish, mollusks, etc.). Scientific knowledge present in literature for each of these sources seems to differ, from the well-known seaweed (Rinaudo, 2007) to relatively new sources of polysaccharides such as extremophiles and microalgae (#CITATION_TAG et al., 2013). The high biodiversity of the latter and how they can serve the biotechnological field has been recently pointed out (Stengel et al., 2011;Barra et al., 2014). Cultivable or uncultivable marine microorganisms with particular emphasis on extremophiles are worth the mention. Structural characteristics, rheological properties and absence of pathogenicity of exopolysaccharides from extremophiles make these compounds considerable for food, pharmaceutical and cosmetics industries (Nicolaus et al., 2010;Satpute et al., 2010;Finore et al., 2014).	i
CC305	Implementing personalised electronic government services is far from trivial and the literature reports many obstacles. These include socio-political problems of information sharing across traditional organisational boundaries (Mulgan, 2005;Homburg, 2008), the existence of legacy systems (Pieterson et al., 2007) and concerns related to privacy issues (#CITATION_TAG et al., 2004).	4	Implementing personalised electronic government services is far from trivial and the literature reports many obstacles. These include socio-political problems of information sharing across traditional organisational boundaries (Mulgan, 2005;Homburg, 2008), the existence of legacy systems (Pieterson et al., 2007) and concerns related to privacy issues (#CITATION_TAG et al., 2004).	h
CC1702	"A third process of bio-objectification relates to the patients participating in these clinical biobanking endeavors and the roles they are expected to take up vis-�_-vis the tissue and data procured from them. Through large-scale forms of resource provision embedded into practical routines and infrastructures for healthcare, patients are turned into regular contributors to the clinical research enterprise. This is reflected in terminology involved to describe their role. Instead of the use of language such as ""research subjects"" , contributing human tissue and data is now often framed as an act of ""donating"" , which a term previously reserved for more tangible donations dedicated to others"" well-being such as through blood donations (Tutton 2002). A case in point is that in 2011 Dutch professional guidelines for responsible use of human tissue in biomedical research routinely speak of ""donors"" and ""donations""; yet, in 2001 the terminology used was ""betrokkene"" (i.e. someone who""s involved) (Federatie van Medisch-Wetenschappelijke Verenigingen (FEDERA) 2001; Federatie van Medisch-Wetenschappelijke Verenigingen (Federa) ( 2011)). Some scholars have referred to this process as involving new forms of ""clinical"" and ""immaterial labor"". At the same time, the labour performed by most donors is also minimized and made invisible by integrating it into routine aspects of care (#CITATION_TAG and Waldby 2010;Mitchell 2012). In order to achieve high rates of donation, the success of clinical biobanking is considered to depend on its unobtrusiveness and on not being seen to overburden patients in their donations. This is reflected in concerted efforts in PSI to minimize the work and time expended on biobanking for patients, research nurses and clinicians by integrating tissue and data procurement as efficiently as possible in day-to-day clinical care. These adjustments in clinical routines, which also involve mundane aspects such as training of research nurses and timing of clinical appointments, are forms of bio-objectification that allow for patients\"" data and tissue to be swiftly transformed into ""workable epistemic objects"" (Eriksson and Webster 2015)."	2	"Instead of the use of language such as ""research subjects"" , contributing human tissue and data is now often framed as an act of ""donating"" , which a term previously reserved for more tangible donations dedicated to others"" well-being such as through blood donations (Tutton 2002). A case in point is that in 2011 Dutch professional guidelines for responsible use of human tissue in biomedical research routinely speak of ""donors"" and ""donations""; yet, in 2001 the terminology used was ""betrokkene"" (i.e. someone who""s involved) (Federatie van Medisch-Wetenschappelijke Verenigingen (FEDERA) 2001; Federatie van Medisch-Wetenschappelijke Verenigingen (Federa) ( 2011)). Some scholars have referred to this process as involving new forms of ""clinical"" and ""immaterial labor"". At the same time, the labour performed by most donors is also minimized and made invisible by integrating it into routine aspects of care (#CITATION_TAG and Waldby 2010;Mitchell 2012). In order to achieve high rates of donation, the success of clinical biobanking is considered to depend on its unobtrusiveness and on not being seen to overburden patients in their donations. This is reflected in concerted efforts in PSI to minimize the work and time expended on biobanking for patients, research nurses and clinicians by integrating tissue and data procurement as efficiently as possible in day-to-day clinical care. These adjustments in clinical routines, which also involve mundane aspects such as training of research nurses and timing of clinical appointments, are forms of bio-objectification that allow for patients\"" data and tissue to be swiftly transformed into ""workable epistemic objects"" (Eriksson and Webster 2015)."	 
CC791	"Historically, sedentary behavior (SED) was conceptualized as the lower end of the PA spectrum, as opposed to moderate-to vigorous PA (MVPA), but is now increasingly being viewed as a behavior distinct from PA, defined as waking behavior characterized by an energy expenditure 1.5 metabolic equivalents (METs) [21]. Although somewhat arbitrary, studies using Actigraph instruments mainly operationalize SED as the time spent <100 cpm. This cutoff differs substantially from that used to define ""inactivity"" (<500 cpm) applied by Matthews et al [6]. Thus, besides the study of reliability of SED in older adults [8], to the best of our knowledge, reliability of SED obtained by accelerometry have not been investigated in adults, and should be prioritized #CITATION_TAG. Furthermore, no studies have determined the intra-individual week-by-week agreement of accelerometer outcomes using absolute measures of reliability (i.e., standard error of the measurement and limits of agreement). Most evidence suggest that a reliability of !0.70-0.80 are achieved for PA with 3-7 days of monitoring by estimation of the number of days needed based on the Spearman Brown prophecy formula, when measurements are conducted over a single 7-day period [5-7, 2, 8]. However, such study designs have received critique for possibly leaving to optimistic results and should be interpreted with caution [23][24][25]. First, the results are in principle only generalizable to the included days, as inclusion of additional days, weeks or seasons will add variability. Secondly, the assumption of compound symmetry (i.e., similar variances and co-variances) across days of measurement might not be fulfilled. Additionally, ICC is the variance partitioning of subjects to the total variance, thus ICC is a relative and context-specific estimate that depends on the heterogeneity of the sample [26][27][28]. Thus, research targeting agreement of SED and PA measurements by means of absolute measures of reliability, which allow for a direct quantification of how much outcomes vary over time independent of the variability of observations should be given priority."	0	"Historically, sedentary behavior (SED) was conceptualized as the lower end of the PA spectrum, as opposed to moderate-to vigorous PA (MVPA), but is now increasingly being viewed as a behavior distinct from PA, defined as waking behavior characterized by an energy expenditure 1.5 metabolic equivalents (METs) [21]. Although somewhat arbitrary, studies using Actigraph instruments mainly operationalize SED as the time spent <100 cpm. This cutoff differs substantially from that used to define ""inactivity"" (<500 cpm) applied by Matthews et al [6]. Thus, besides the study of reliability of SED in older adults [8], to the best of our knowledge, reliability of SED obtained by accelerometry have not been investigated in adults, and should be prioritized #CITATION_TAG. Furthermore, no studies have determined the intra-individual week-by-week agreement of accelerometer outcomes using absolute measures of reliability (i.e., standard error of the measurement and limits of agreement). Most evidence suggest that a reliability of ! 0.70-0.80 are achieved for PA with 3-7 days of monitoring by estimation of the number of days needed based on the Spearman Brown prophecy formula, when measurements are conducted over a single 7-day period [5-7, 2, 8]."	s
CC2610	"A common mistake [which] causes messages to self-destruct (Goldstein et al, 2007, p.18) is where injunctive and descriptive norms conict, and the`wrong"" norm dominates. For example, #CITATION_TAG (2003) and colleagues investigated the phenomenon of visitors taking pieces of petried wood from Arizona""s Petried Forest National Park, and the park authority""s attempts to enjoin visitors not to do this: New arrivals quickly learn of the past thievery from prominently placed signage:`Your heritage is being vandalized every day by theft losses of petried wood of 14 tons a year, mostly a small piece at a time. Although it is understandable that park ocials would want to instigate corrective action by describing the dismaying size of the problem. . . it would be better to design park signage to focus visitors on the social disapproval (rather than the harmful prevalence) of environmental theft."	0	"A common mistake [which] causes messages to self-destruct (Goldstein et al, 2007, p.18) is where injunctive and descriptive norms conict, and the`wrong"" norm dominates. For example, #CITATION_TAG (2003) and colleagues investigated the phenomenon of visitors taking pieces of petried wood from Arizona""s Petried Forest National Park, and the park authority""s attempts to enjoin visitors not to do this: New arrivals quickly learn of the past thievery from prominently placed signage:`Your heritage is being vandalized every day by theft losses of petried wood of 14 tons a year, mostly a small piece at a time. Although it is understandable that park ocials would want to instigate corrective action by describing the dismaying size of the problem. . . it would be better to design park signage to focus visitors on the social disapproval (rather than the harmful prevalence) of environmental theft."	o
CC1929	Regular and irregular forms are present in the Italian verb system i.e. past-participle and present-tense. Studies have examined the production of these two verb aspects in braindamaged participants (#CITATION_TAG and Clahsen, 2002;Walenski, Katiuscia, Cappa and Ullman, 2009). Walenski et al. (2009) explored regular and irregular production in Italian verb tenses (past A D P asked to read sentences aloud filling in the appropriate form of the verb. All the participants showed impaired irregular production, while the production of regular forms was spared. These findings replicate findings from studies in English (Miozzo, 2003;Pinker and Ullman, 2002;Ullman et al., 1997). Walenski et al. (2009) interpreted their findings within the dual mechanism account.	1	Regular and irregular forms are present in the Italian verb system i.e. past-participle and present-tense. Studies have examined the production of these two verb aspects in braindamaged participants (#CITATION_TAG and Clahsen, 2002;Walenski, Katiuscia, Cappa and Ullman, 2009). Walenski et al. (2009) explored regular and irregular production in Italian verb tenses (past A D P asked to read sentences aloud filling in the appropriate form of the verb. All the participants showed impaired irregular production, while the production of regular forms was spared. These findings replicate findings from studies in English (Miozzo, 2003;Pinker and Ullman, 2002;Ullman et al., 1997).	t
CC1256	Self-regulated learning is recognized as a complex concept that overlaps with a number of other concepts including temperament, learning approach, and motivation, specifically self-efficacy and goal setting (Bidjerano & Dai, 2007;Boekaerts, 1996). While many students may set goals, the ability to selfregulate learning can be the difference between achieving, or not achieving, the goals set (Covington, 2000). Self-regulated learners take responsibility for setting and achieving their own learning goals by planning their learning, having effective time management, using appropriate learning strategies, continually monitoring and evaluating the quality of their own learning, and altering their learning strategies when required (Schunk, 2005;#CITATION_TAG, 1990). Such learners regard learning as a process they can control, but their motivation factors can vary (Pintrich & DeGroot, 1990). To be motivated to self-regulate, a learner must be confident in setting goals and organizing study, and also be confident that study efforts will result in good marks (high self-efficacy). Such learners must also accept delayed gratification as self-regulation requires students to focus on long-term gains for their effort (Bembenutty, 2009;Komarraju & Nadler, 2013;Zimmerman, 1990;Zimmerman & Kitsantas, 2005). Volet (1996) argues that self-regulated learning is more significant in the tertiary level than earlier levels of education because of the shift from a teacher-controlled environment to one of self-regulated study.	0	Self-regulated learning is recognized as a complex concept that overlaps with a number of other concepts including temperament, learning approach, and motivation, specifically self-efficacy and goal setting (Bidjerano & Dai, 2007;Boekaerts, 1996). While many students may set goals, the ability to selfregulate learning can be the difference between achieving, or not achieving, the goals set (Covington, 2000). Self-regulated learners take responsibility for setting and achieving their own learning goals by planning their learning, having effective time management, using appropriate learning strategies, continually monitoring and evaluating the quality of their own learning, and altering their learning strategies when required (Schunk, 2005;#CITATION_TAG, 1990). Such learners regard learning as a process they can control, but their motivation factors can vary (Pintrich & DeGroot, 1990). To be motivated to self-regulate, a learner must be confident in setting goals and organizing study, and also be confident that study efforts will result in good marks (high self-efficacy). Such learners must also accept delayed gratification as self-regulation requires students to focus on long-term gains for their effort (Bembenutty, 2009;Komarraju & Nadler, 2013;Zimmerman, 1990;Zimmerman & Kitsantas, 2005).	l
CC2779	"Network pictures are managers"" network theories (Mattsson, 1987), representing what they subjectively perceive to be important and what the pertaining logic for actions and consequences of managerial activities are. As such, they are the ""theories-in-use"" helping managers not only to make sense of their complex environment, but also to guide their decision-making and influence their managerial behaviour (Welch & Wilkinson, 2002). Conceptually, our understanding of network pictures is grounded in the theories of cognitive cycles (Neisser, 1967(Neisser, , 1976. Network pictures may or may not be explicit; however, they represent an individual""s solution to the ""framing problem i.e. knowing what knowledge or inferences may be relevant or irrelevant to a specific issue (#CITATION_TAG, McEvily, & Perrone, 1998). They are therefore posited to guide networking activities, but they are also used to ascribe meaning to events in the network, such as activities instigated by other actors (Ford et al., 2003;Smircich & Stubbart, 1985). The individual decision-maker is thus provided with a bounded field of decision possibilities within the limits of expectations shaped by the framework of his network picture."	0	"Network pictures are managers"" network theories (Mattsson, 1987), representing what they subjectively perceive to be important and what the pertaining logic for actions and consequences of managerial activities are. As such, they are the ""theories-in-use"" helping managers not only to make sense of their complex environment, but also to guide their decision-making and influence their managerial behaviour (Welch & Wilkinson, 2002). Conceptually, our understanding of network pictures is grounded in the theories of cognitive cycles (Neisser, 1967(Neisser, , 1976. Network pictures may or may not be explicit; however, they represent an individual""s solution to the ""framing problem i.e. knowing what knowledge or inferences may be relevant or irrelevant to a specific issue (#CITATION_TAG, McEvily, & Perrone, 1998). They are therefore posited to guide networking activities, but they are also used to ascribe meaning to events in the network, such as activities instigated by other actors (Ford et al., 2003;Smircich & Stubbart, 1985). The individual decision-maker is thus provided with a bounded field of decision possibilities within the limits of expectations shaped by the framework of his network picture."	w
CC665	"_� Novel DBS indications planning: It is likely that DBS will become a bearer of hope for many psychiatric disorders -in particular for depression, OCD and Tourette""s syndromefor which known therapies have failed (e.g., recent studies estimate that more than 50% of patients suffering from depression may be treatment-resistant; #CITATION_TAG et al. 2013)."	3	"_� Novel DBS indications planning: It is likely that DBS will become a bearer of hope for many psychiatric disorders -in particular for depression, OCD and Tourette""s syndromefor which known therapies have failed (e.g., recent studies estimate that more than 50% of patients suffering from depression may be treatment-resistant; #CITATION_TAG et al. 2013)."	_
CC219	We aimed to estimate the incidence and prevalence of MS in BC, Canada using previously validated case definitions of MS [10,#CITATION_TAG] based on health administrative data. Also, we described the demographics of the incident and prevalent cases and temporal changes in their characteristics including the sex ratio.	5	We aimed to estimate the incidence and prevalence of MS in BC, Canada using previously validated case definitions of MS [10,#CITATION_TAG] based on health administrative data. Also, we described the demographics of the incident and prevalent cases and temporal changes in their characteristics including the sex ratio.	W
CC188	The enduring popularity of unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy (NRT) and the stop-smoking medications, bupropion and varenicline, are readily available and widely promoted. [9,10] Yet little appears to be known about this population or this self-guided route to cessation success. In contrast, the phenomenon of self-change (also known as natural recovery) is comparatively well documented in the fields of drug and alcohol addiction, [12,13] and health behaviour change (for example, eating disorders, obesity and gambling). [14] We recently published a systematic review of unassisted cessation in Australia. [9] We, like others, [15] established that the majority of contemporary cessation research is quantitative and intervention focused. [16] While completing that review we determined that the available qualitative research was concerned primarily with evaluating smoker and ex-smoker perceptions of mass-reach interventions such as marketing or retail regulations, tax increases, graphic health warnings, smoke-free legislation or intervention acceptability from the perspective of the GP, current smoker, or third parties likely to be impacted by mass-reach interventions. Australian smoking cessation research provided few insights into quitting from the perspective of the smoker who quits unassisted. However our systematic review highlighted that 54% to 69% of ex-smokers quit unassisted and 41% to 58% of current smokers had attempted to quit unassisted. [9] We consequently became interested in what the qualitative cessation literature had to say about smokers who quit unassisted. Qualitative approaches offer an opportunity to explain unexpected or anomalous findings from quantitative research and to clarify relationships identified in these studies. [17,18] By integrating individual qualitative research studies into a qualitative synthesis, new insights and understandings can be generated and a cumulative body of empirical work produced. #CITATION_TAG Such syntheses have proven useful to health policy and practice. [20,21] By focusing our review on the views of smokers (i.e. on the people to whom the interventions are directed), we might start to better understand why many smokers continue to quit unassisted instead of using the assistance available to them. Such an understanding might help us to decide whether we should be developing better approaches to unassisted cessation or focusing our attention on directing more smokers to use the efficacious pharmacological and professional behavioural support that already exists.	0	[9] We consequently became interested in what the qualitative cessation literature had to say about smokers who quit unassisted. Qualitative approaches offer an opportunity to explain unexpected or anomalous findings from quantitative research and to clarify relationships identified in these studies. [17,18] By integrating individual qualitative research studies into a qualitative synthesis, new insights and understandings can be generated and a cumulative body of empirical work produced. #CITATION_TAG Such syntheses have proven useful to health policy and practice. [20,21] By focusing our review on the views of smokers (i.e. on the people to whom the interventions are directed), we might start to better understand why many smokers continue to quit unassisted instead of using the assistance available to them. Such an understanding might help us to decide whether we should be developing better approaches to unassisted cessation or focusing our attention on directing more smokers to use the efficacious pharmacological and professional behavioural support that already exists.	A
CC2961	"Micro-electrode recordings suggest that the hippocampus may play a similar role in rats. Place cells in the rat hippocampus, which encode specific locations in a structured environment, such as a maze, also fire when the animal is outside that environment, sometimes when the animal is asleep (#CITATION_TAG and McNaughton, 1994) and sometimes when it is awake but immobile (Karlsson and Frank, 2009). Recordings show that this firing occurs in what have been termed sharp-wave ripples, sweeping out trajectories corresponding to earlier locations in the environment. These ripples are accompanied by widespread activation in the cerebral cortex, along with inhibition of activity in the diencephalon, limbic system, and brain stem, suggesting an interaction between hippocampus and cortex in the consolidation of acquired awake experience (Logothetis et al., 2012). It might also be interpreted as representing the experiencing of trajectories, either previously experienced or planned (Corballis, 2013)-in other words, mental time travel. This is further suggested by evidence that the trajectories need not correspond to actual trajectories that the animal took while it was in the environment. Sometimes they correspond to a previously taken path in a maze, but sometimes to the reverse of such paths, or even to paths through regions the rat did not actually visit (Gupta et al., 2010). This might be taken as evidence for mental time travel along not only past trajectories, but also along imagined future ones. More direct evidence that hippocampal activity signals future behavior comes from rats trained to alternate left and right turns at a particular location in a maze. Between trials, they were introduced to a running wheel, and while they were running differential activity in the hippocampus signaled which turn they would take next. Based on this and other findings, the authors concluded that self-organized activity in the hippocampus, ""having evolved for the computation of distances, can also support the episodic recall of events and the planning of action sequences and goals"" (Pastalkova et al., 2008(Pastalkova et al., , p. 1327."	2	Micro-electrode recordings suggest that the hippocampus may play a similar role in rats. Place cells in the rat hippocampus, which encode specific locations in a structured environment, such as a maze, also fire when the animal is outside that environment, sometimes when the animal is asleep (#CITATION_TAG and McNaughton, 1994) and sometimes when it is awake but immobile (Karlsson and Frank, 2009). Recordings show that this firing occurs in what have been termed sharp-wave ripples, sweeping out trajectories corresponding to earlier locations in the environment. These ripples are accompanied by widespread activation in the cerebral cortex, along with inhibition of activity in the diencephalon, limbic system, and brain stem, suggesting an interaction between hippocampus and cortex in the consolidation of acquired awake experience (Logothetis et al., 2012). It might also be interpreted as representing the experiencing of trajectories, either previously experienced or planned (Corballis, 2013)-in other words, mental time travel.	l
CC2990	The idea that language evolved from manual gestures has a long history, going back at least to Condillac (1746Condillac ( /1971 in the 18th century, and restated in modern form by Hewes (1973). The gestural theory was boosted with the discovery in monkeys of mirror neurons, so called because they respond both when the monkey makes a grasping movement and when it observed the same movement performed by another individual (Rizzolatti et al., 1988). Mirror neurons are now considered part of a more extensive mirror system, involving regions in the ventral prefrontal cortex, parietal cortex, and superior temporal sulcus (Rizzolatti and Sinigaglia, 2010), and in fact overlapping extensively with the default network. The idea that mirror neurons may underlie the evolution of language has been elaborated by a number of authors (e.g., Corballis, 2002;#CITATION_TAG, 2005;Rizzolatti and Sinigaglia, 2008). The gestural theory was also boosted by the realization that the signed languages of the deaf are true languages, with full syntactic and semantic properties, albeit based entirely on visible movements of the hands and face (Armstrong et al., 1995;Armstrong, 1999).	2	The idea that language evolved from manual gestures has a long history, going back at least to Condillac (1746Condillac ( /1971 in the 18th century, and restated in modern form by Hewes (1973). The gestural theory was boosted with the discovery in monkeys of mirror neurons, so called because they respond both when the monkey makes a grasping movement and when it observed the same movement performed by another individual (Rizzolatti et al., 1988). Mirror neurons are now considered part of a more extensive mirror system, involving regions in the ventral prefrontal cortex, parietal cortex, and superior temporal sulcus (Rizzolatti and Sinigaglia, 2010), and in fact overlapping extensively with the default network. The idea that mirror neurons may underlie the evolution of language has been elaborated by a number of authors (e.g., Corballis, 2002;#CITATION_TAG, 2005;Rizzolatti and Sinigaglia, 2008). The gestural theory was also boosted by the realization that the signed languages of the deaf are true languages, with full syntactic and semantic properties, albeit based entirely on visible movements of the hands and face (Armstrong et al., 1995;Armstrong, 1999).	 
CC2066	"There is also evidence for eolian deposition burying soil surfaces in the past 250 years concomitant with dense historic settlement of Cape Cod (Cogbill et al., 2002;Motzkin et al., 2002;Eberhardt et al., 2003;Forman et al., 2008). The Pilgrims, North American pioneers, arrived on Cape Cod on November 11, 1620, though they moved to Plymouth within a month, the Cape was an early area of European land use (Kittredge, 1968, p. 20). Timber harvesting soon commenced around Provincetown and prompted the earliest known conservation law in AD 1626 to ensure that the locals had a ready timber supply (Geller, 1974, p. 27). Intensive land use on Cape Cod began ca. AD 1630 with planting of crops and rampant tree felling which resulted in exposed soils for wind erosion (#CITATION_TAG, 1983;Holmes et al., 1998, p. 56). Unregulated livestock grazing was common post-AD 1630 with cattle and sheep requiring no confined pasturage; swine were also allowed to roam freely and root up soil with feeding (Kittredge, 1968, pp. 71-72). Extensive corn planting during this time depleted soils of nutrients and in AD 1637 one farmer wrote that, ""after five or six years, [the soil] grows barren beyond belief"" (Cronon, 1983, p. 125). Town fathers by the AD 1670s expressed concern about the limited supply of timber (Kittredge, 1968, p. 86). Blowing sand emerged as a persistent problem to Cape Cod residents in the early AD 1700\""s with large areas around Provincetown described as ""deserts"" in AD 1725-1730 (Rubertone, 1985, pp. 90-92;Holmes et al., 1998, p. 57). A Massachusetts Senate Document from AD 1714 indicates the scale of blowing sand around Provincetown was such that the harbor at Cape Cod was infilling jeopardizing docking of ships . Active dune migration persisted through the 18th to 21st centuries as a legacy of past landscape disturbance with initial European settlement (Forman et al., 2008)."	0	"The Pilgrims, North American pioneers, arrived on Cape Cod on November 11, 1620, though they moved to Plymouth within a month, the Cape was an early area of European land use (Kittredge, 1968, p. 20). Timber harvesting soon commenced around Provincetown and prompted the earliest known conservation law in AD 1626 to ensure that the locals had a ready timber supply (Geller, 1974, p. 27). Intensive land use on Cape Cod began ca. AD 1630 with planting of crops and rampant tree felling which resulted in exposed soils for wind erosion (#CITATION_TAG, 1983;Holmes et al., 1998, p. 56). Unregulated livestock grazing was common post-AD 1630 with cattle and sheep requiring no confined pasturage; swine were also allowed to roam freely and root up soil with feeding (Kittredge, 1968, pp. 71-72). Extensive corn planting during this time depleted soils of nutrients and in AD 1637 one farmer wrote that, ""after five or six years, [the soil] grows barren beyond belief"" (Cronon, 1983, p. 125)."	6
CC1932	The selection of these tests was constrained by the materials available for assessment in Jordanian Arabic in clinics in Jordan at the time of assessment. These subtests were taken from two sources: translated subtests of the Comprehensive Aphasia Test (CAT) (#CITATION_TAG, Porter & Howard, 2004) and subtests that have been developed in speech and language clinics in Jordan. The CAT subtests had been translated by clinicians and were in use in Jordan. They were modified to suit local cultural and linguistic criteria. In addition, a connected speech sample was recorded from each participant. The sample served as a measure of lexical retrieval and grammatical construction in connected speech. Transcription, coding and analysis of the output was conducted in line with recommendations suggested by Herbert, Best, Hickin, Howard and Osborne (2008;p. 200-202).	5	The selection of these tests was constrained by the materials available for assessment in Jordanian Arabic in clinics in Jordan at the time of assessment. These subtests were taken from two sources: translated subtests of the Comprehensive Aphasia Test (CAT) (#CITATION_TAG, Porter & Howard, 2004) and subtests that have been developed in speech and language clinics in Jordan. The CAT subtests had been translated by clinicians and were in use in Jordan. They were modified to suit local cultural and linguistic criteria. In addition, a connected speech sample was recorded from each participant.	h
CC1538	The canonical view of fluent and grammatically correct speech in SD is only partially supported by our results. Whilst there was no evidence of gross syntactic violations, there was definitely an increased rate of errors on free and bound closed class items, and syntactic anomalies occurred when lexical retrieval went awry. We end with the twin questions of why the syntax of SD speech might (a) be relatively preserved and (b) appear, on casual observation, to be even more normal than it actually is. Syntax is abstracted information and it is likely to be redundantly coded (Bates & Wulfeck, 1989) as it applies across all members of a given class and across variations in other semantic content. Given that the semantic deficit in SD is so strongly modulated by redundant coding -i.e. what the patients still know is information that generalises to many different concepts (Patterson, 2007;Rogers & Patterson, 2007) -it seems plausible that lexico-syntactic information may be precisely the kind of abstracted, general, highly frequent information that should be preserved in SD. Syntactic structures can be seen as highly routinised and automatically produced (Bates & Wulfeck, 1989;#CITATION_TAG, Curtiss, & Jackson, 1987) and the processes that underpin their use in production have been constructively compared to implicit learning (Bock & Griffin, 2000;Chang, Dell, & Bock, 2006;Ferreira & Bock, 2006).	4	We end with the twin questions of why the syntax of SD speech might (a) be relatively preserved and (b) appear, on casual observation, to be even more normal than it actually is. Syntax is abstracted information and it is likely to be redundantly coded (Bates & Wulfeck, 1989) as it applies across all members of a given class and across variations in other semantic content. Given that the semantic deficit in SD is so strongly modulated by redundant coding -i.e. what the patients still know is information that generalises to many different concepts (Patterson, 2007;Rogers & Patterson, 2007) -it seems plausible that lexico-syntactic information may be precisely the kind of abstracted, general, highly frequent information that should be preserved in SD. Syntactic structures can be seen as highly routinised and automatically produced (Bates & Wulfeck, 1989;#CITATION_TAG, Curtiss, & Jackson, 1987) and the processes that underpin their use in production have been constructively compared to implicit learning (Bock & Griffin, 2000;Chang, Dell, & Bock, 2006;Ferreira & Bock, 2006).	c
CC601	Akhiezer considered a generalization of the Chebyshev polynomials to the pair of intervals [__�1, �]___[�_, 1], and investigated their properties by conformal mapping. #CITATION_TAG and Lawrence [8] used the theory of elliptic functions to investigate these polynomials. In this section we obtain the differential equation in the where S is two intervals, and obtain a differential equation for the endpoints that is related to the one from [8].	0	Akhiezer considered a generalization of the Chebyshev polynomials to the pair of intervals [__�1, �]___[�_, 1], and investigated their properties by conformal mapping. #CITATION_TAG and Lawrence [8] used the theory of elliptic functions to investigate these polynomials. In this section we obtain the differential equation in the where S is two intervals, and obtain a differential equation for the endpoints that is related to the one from [8].	C
CC2617	"The key issue is that neither the heterodox social science literature concerned with markets, nor its more recent economic geographical variant, have directly engaged with how markets exist in space, and the difference that this makes to economic practice or outcome. The ""market"" in much of the economic geographical literature is set in the background rather than the foreground, with the issue of what space, place or context that markets occupy rarely addressed, and often only implicitly. Conceptualising the difference that the spatiality of markets -as well as their geographies -make to economic outcomes is thus an important challenge that geographical thinking needs to engage with. Such a contention echoes #CITATION_TAG""s (2007) argument that there is a need for the socio-technical literature on markets to not only address ""the question of spatiality"" but produce ""nuanced answers"" (#CITATION_TAG, 2007: 372)."	5	"The key issue is that neither the heterodox social science literature concerned with markets, nor its more recent economic geographical variant, have directly engaged with how markets exist in space, and the difference that this makes to economic practice or outcome. The ""market"" in much of the economic geographical literature is set in the background rather than the foreground, with the issue of what space, place or context that markets occupy rarely addressed, and often only implicitly. Conceptualising the difference that the spatiality of markets -as well as their geographies -make to economic outcomes is thus an important challenge that geographical thinking needs to engage with. Such a contention echoes #CITATION_TAG""s (2007) argument that there is a need for the socio-technical literature on markets to not only address ""the question of spatiality"" but produce ""nuanced answers"" (#CITATION_TAG, 2007: 372)."	h
CC930	The two common cyanolichens we surveyed, N. arcticum and P. aphthosa, appear to contribute as much as the forest mosses to the total ecosystem input of atmospheric nitrogen into birch-heath woodland. Cyanolichens are virtually absent from the other two habitats studied, fens and bogs, where mosses fix substantial N 2 on a landscape area basis. Thus we claim that mosses probably play a key role in the process of nitrogen fixation there. A similar pattern emerges from older studies done in the region on total yearly nitrogen input. The two cyanolichens S. paschale and N. arcticum fix 0.38 g N m __�2 yr __�1 in similar Fennoscandian woodland (Longton 1997) and N. arcticum alone fixes 0.1 g N m __�2 yr __�1 in Fennoscandian tundra (Sonesson et al. 1975), whereas the two mosses S. riparium and Drepanocladus exannulus together fix as much as 0.3-4.3 g N m __�2 yr __�1 in the Stordalen fen, Abisko (Basilier 1979). In comparison, atmospheric deposition levels of nitrate (NO y ) and ammonia (NH x ), as reported in a survey by the Swedish Environmental Research Institute, are less than 0.4 g N m __�2 yr __�1 for the region of North Sweden (Binkley and Hogberg 1997). These data together ascertain the significance of biological N fixation to the N-limited vegetation in the Subarctic (#CITATION_TAG and Kershaw 1978).	1	A similar pattern emerges from older studies done in the region on total yearly nitrogen input. The two cyanolichens S. paschale and N. arcticum fix 0.38 g N m __�2 yr __�1 in similar Fennoscandian woodland (Longton 1997) and N. arcticum alone fixes 0.1 g N m __�2 yr __�1 in Fennoscandian tundra (Sonesson et al. 1975), whereas the two mosses S. riparium and Drepanocladus exannulus together fix as much as 0.3-4.3 g N m __�2 yr __�1 in the Stordalen fen, Abisko (Basilier 1979). In comparison, atmospheric deposition levels of nitrate (NO y ) and ammonia (NH x ), as reported in a survey by the Swedish Environmental Research Institute, are less than 0.4 g N m __�2 yr __�1 for the region of North Sweden (Binkley and Hogberg 1997). These data together ascertain the significance of biological N fixation to the N-limited vegetation in the Subarctic (#CITATION_TAG and Kershaw 1978).	d
CC2210	The limitations of this method are substantial. In particular, one filter ratio value provides one temperature value for each pixel; this is a reliable measurement, within experimental errors, as long as the assumption of isothermal plasma approximately holds for the plasma column in the pixel along the line of sight. If the plasma is considerably multithermal, the temperature value is an average weighted for the instrumental response. Since the response is a highly nonlinear function of the emitting plasma temperature, it is not trivial to interpret the related maps correctly. In addition, it is fundamental to know the instrument response with high precision, in order to avoid systematic errors, which propagate dangerously when filter ratios are evaluated. In this respect, broadband filters provide robust thermal diagnostics, because they are weakly dependent on the details of the atomic physics models, e.g., on the presence of unknown or not well-known spectral lines, on the choice of element abundances. Narrowband filters can show non-unique dependencies of filter ratio values on temperature (e.g., Patsourakos and Klimchuk, 2007), due to the presence of several important spectral lines in the bands, but a more general problem can be the bias to detect narrow ranges of temperatures forced by the specific instrument characteristics (#CITATION_TAG et al., 2005). This problem can be important especially when the distribution of the emission measure along the line of sight is not simple and highly nonlinear (e.g., Reale et al., 2009a). New methods for thermal diagnostics with narrow band instruments have been proposed (Dudok de Wit et al., 2013).	5	Since the response is a highly nonlinear function of the emitting plasma temperature, it is not trivial to interpret the related maps correctly. In addition, it is fundamental to know the instrument response with high precision, in order to avoid systematic errors, which propagate dangerously when filter ratios are evaluated. In this respect, broadband filters provide robust thermal diagnostics, because they are weakly dependent on the details of the atomic physics models, e.g., on the presence of unknown or not well-known spectral lines, on the choice of element abundances. Narrowband filters can show non-unique dependencies of filter ratio values on temperature (e.g., Patsourakos and Klimchuk, 2007), due to the presence of several important spectral lines in the bands, but a more general problem can be the bias to detect narrow ranges of temperatures forced by the specific instrument characteristics (#CITATION_TAG et al., 2005). This problem can be important especially when the distribution of the emission measure along the line of sight is not simple and highly nonlinear (e.g., Reale et al., 2009a). New methods for thermal diagnostics with narrow band instruments have been proposed (Dudok de Wit et al., 2013).	b
CC1166	"What follows is shaped by two evolutionary considerations. First, any kind of activity that absorbs up to a half of conscious brain activity must have been selected for its contribution to the human species"" survival. Indeed, it appears that the brain\""s default-mode network provides the substrate for mind-wandering (e.g., Mason et al., 2007;Christoff et al., 2009;Andrews-Hanna et al., 2010a;Stawarczyk et al., 2011b), a network of several ""hubs"" and ""subsystems"" (Andrews-Hanna, 2012) that probably constitutes a majority of the brain\""s energy consumption (Raichle, 2009). It must serve important functions. These include a variety of mental processes, including retrieval of past experiences and imagining future scenarios (Buckner et al., 2008), which are essential for planning and are also stock components of mind-wandering sequences. Raichle (2009) argues that the default-mode network is not coextensive with conscious mind-wandering, citing the continuation of the network""s activity into lighter states of anesthesia and Stages 1 and 2 of sleep (#CITATION_TAG et al., 2008), when dreaming is most frequent, and the demonstration (Christoff et al., 2009) of executive-network elements during resting states. However, if one accepts that there is continuity between mind-wandering states and dreaming, and if one recognizes that non-conscious processes (meaning-complexes; Klinger, 1971Klinger, , 2011 underlie both the thought and dream segments, there is no reason to doubt the close relationship of mind-wandering and its variants with the default-mode network."	0	"Indeed, it appears that the brain\""s default-mode network provides the substrate for mind-wandering (e.g., Mason et al., 2007;Christoff et al., 2009;Andrews-Hanna et al., 2010a;Stawarczyk et al., 2011b), a network of several ""hubs"" and ""subsystems"" (Andrews-Hanna, 2012) that probably constitutes a majority of the brain\""s energy consumption (Raichle, 2009). It must serve important functions. These include a variety of mental processes, including retrieval of past experiences and imagining future scenarios (Buckner et al., 2008), which are essential for planning and are also stock components of mind-wandering sequences. Raichle (2009) argues that the default-mode network is not coextensive with conscious mind-wandering, citing the continuation of the network""s activity into lighter states of anesthesia and Stages 1 and 2 of sleep (#CITATION_TAG et al., 2008), when dreaming is most frequent, and the demonstration (Christoff et al., 2009) of executive-network elements during resting states. However, if one accepts that there is continuity between mind-wandering states and dreaming, and if one recognizes that non-conscious processes (meaning-complexes; Klinger, 1971Klinger, , 2011 underlie both the thought and dream segments, there is no reason to doubt the close relationship of mind-wandering and its variants with the default-mode network."	l
CC2768	"Managers often want to change distribution structures or their company""s position in a network (#CITATION_TAG & Bell, 2005). At the same time there are those in the network who resist such changes (HՂkansson & Ford, 2002). Management decision-making in this context is affected by managers"" interpretations of the available information at a particular point in time (Ford, Gadde, HՂkansson, & Snehota, 2003) also referred to as sensemaking (Weick, 1979(Weick, , 1995. However, changes, and managers"" perceptions of such changes, can vary across a business network as change perceptions are idiosyncratic to the individual actor. A systematic way of comparing and contrasting managers"" perceptions of their surrounding network, will therefore aid our understanding of their decision-making behaviour."	0	"Managers often want to change distribution structures or their company""s position in a network (#CITATION_TAG & Bell, 2005). At the same time there are those in the network who resist such changes (HՂkansson & Ford, 2002). Management decision-making in this context is affected by managers"" interpretations of the available information at a particular point in time (Ford, Gadde, HՂkansson, & Snehota, 2003) also referred to as sensemaking (Weick, 1979(Weick, , 1995. However, changes, and managers"" perceptions of such changes, can vary across a business network as change perceptions are idiosyncratic to the individual actor."	M
CC2357	Overall, and despite no detectable trends in the total amount of precipitation during the wet seasons (Fig. 4b) nor any other trend, the high inter-annual variability of (1) the timing of the onset of the agricultural year (as determined by the first pronounced precipitation event) and (2) dry spells during the wet season, especially during the very sensible early phase of plant growing, kept rain-fed farming constantly challenging and likely favored perceptions of water scarcity (#CITATION_TAG et al., 2013).	1	Overall, and despite no detectable trends in the total amount of precipitation during the wet seasons (Fig. 4b) nor any other trend, the high inter-annual variability of (1) the timing of the onset of the agricultural year (as determined by the first pronounced precipitation event) and (2) dry spells during the wet season, especially during the very sensible early phase of plant growing, kept rain-fed farming constantly challenging and likely favored perceptions of water scarcity (#CITATION_TAG et al., 2013).	O
CC522	"To fight corruption, it is necessary to have institutions in place that disseminate relevant information, and voters who act on that information to hold politicians accountable. Costas et al. (2011), Freille et al. (2007, Lederman et al. (2005), Adsera et al. (2003), Brunetti and Weder (2003), and Besley and Burgess (2002) provide evidence on the importance of a free press in reducing corruption. However, as Johnson et al. (2011), Chang et al. (2010), and Golden (2006) point out, corruption exists even in advanced democracies. Worse yet, Costas et al. (2011), Fern��ndez-V��zquez andRivero (2010), Chang et al. (2010), and Reed (2005) show, through Spanish, Italian, and Japanese examples, that such corruption usually makes very little difference in the reelection fortunes of politicians even when it is public knowledge. Welch and Hibbing (1997), Dimock and Jacobson (1995), and Peters and Welch (1980) provide similar evidence in the case of U.S. Chang et al. (2010), Fern��ndez-V��zquez andRivero (2010), Manzetti and Wilson (2007) and Golden (2006) offer some explanations as to why this is so. They argue that voters may doubt the information or dismiss it as partisan, especially if the accusations are leveled predominantly against the members of one party. The pool of candidates from which citizens can choose may be seriously restricted in terms of their quality. Also, the voters may take corruptness of an incumbent into account in casting their ballots, but only as one of his/her many attributes. Especially if they believe that the honest challengers will not be able to deliver the same results in terms of economic development and increases in their well-being, many of them may still vote for the 1 It should be noted that some researchers, such as Nye (1989) and Leff (1964) argue that optimal level of corruption may be non-zero. They assert that bribing can be viewed as greasing the wheels of the government, enabling firms to sidestep burdensome government controls. However others dispute the existence of a stable growth-enhancing equilibrium level of corruption. Rose-Ackerman (1997) for example argues that corruption will always escalate to ever higher levels, and Kaufman and Wei (2000) find that in economies where corruption is high and more bribes have to be paid, managers end up allocating more time to public officials and less time to productive work. A more recent and more rigorous study by Swaleheen (2011) shows that the ""grease-in-the-wheel"" argument applies only in the cases of very high-corruption countries, and by  #CITATION_TAG and Verdon (2010) only in the cases of low-freedom countries . Since such countries do not have fair elections and voter response to speak of, they do not fall under the subject matter of our paper. corrupt incumbent. Consequently, a politician can offset, at least partially, the negative impact of his/her corrupt behavior by transferring government benefits to his/her constituents, supporting economic policies with which they agree and/or governing competently otherwise. Corrupt incumbents may lose votes but not enough to deny them re-election, as long as they keep the level of corruption in check and do not allow it to damage overall economic performance significantly. This would explain why Pellegrini and Gerlagh (2008) and Lederman et al. (2005) find the level of corruption to be lower, and Drury et al. (2006), its harm on economic growth to be less, in democratic countries."	1	"They assert that bribing can be viewed as greasing the wheels of the government, enabling firms to sidestep burdensome government controls. However others dispute the existence of a stable growth-enhancing equilibrium level of corruption. Rose-Ackerman (1997) for example argues that corruption will always escalate to ever higher levels, and Kaufman and Wei (2000) find that in economies where corruption is high and more bribes have to be paid, managers end up allocating more time to public officials and less time to productive work. A more recent and more rigorous study by Swaleheen (2011) shows that the ""grease-in-the-wheel"" argument applies only in the cases of very high-corruption countries, and by  #CITATION_TAG and Verdon (2010) only in the cases of low-freedom countries . Since such countries do not have fair elections and voter response to speak of, they do not fall under the subject matter of our paper. corrupt incumbent. Consequently, a politician can offset, at least partially, the negative impact of his/her corrupt behavior by transferring government benefits to his/her constituents, supporting economic policies with which they agree and/or governing competently otherwise. Corrupt incumbents may lose votes but not enough to deny them re-election, as long as they keep the level of corruption in check and do not allow it to damage overall economic performance significantly."	t
CC304	"In many western countries, it is especially local governments that are developing onestop shops that serve as a point-of-entry to the whole range of government (#CITATION_TAG, 2002;Ho, 2002). Some contributions in the literature present high hopes of ""transformation"" of the public sector (Weerakkoddy & Reddick, 2013). The actual implementation and take-up of the e-government phenomenon by public sector organizations, however, has lagged behind policy ambitions and rhetoric of transformation, reform and re-engineering (Moon, 2002;Homburg & Dijkshoorn, 2011). In trying to explain the actual diffusion of e-government, various empirical studies have identified city size, citizen demand, organizational structure, geographic location and capacity (see Table 1 for details of the literature reviewed) as the most important determinants of egovernment adoption by public sector organizations. Implicit in the explanations that are featured by these authors seems to be the idea that public sector organizations -as organizations in general -are rational, utility-maximizing entities. In order for these organizations to survive, they may adopt innovative ideas and technologies, but the empirical evidence suggests they are sometimes hampered by the identified determinants. Such a rational explanation suffers from two weaknesses."	0	"In many western countries, it is especially local governments that are developing onestop shops that serve as a point-of-entry to the whole range of government (#CITATION_TAG, 2002;Ho, 2002). Some contributions in the literature present high hopes of ""transformation"" of the public sector (Weerakkoddy & Reddick, 2013). The actual implementation and take-up of the e-government phenomenon by public sector organizations, however, has lagged behind policy ambitions and rhetoric of transformation, reform and re-engineering (Moon, 2002;Homburg & Dijkshoorn, 2011). In trying to explain the actual diffusion of e-government, various empirical studies have identified city size, citizen demand, organizational structure, geographic location and capacity (see Table 1 for details of the literature reviewed) as the most important determinants of egovernment adoption by public sector organizations."	I
CC2067	The most pervasive eolian landforms on outermost Cape Cod are the kilometer-scale parabolic dunes. The orientation these dunes indicates migration from the west to northwest, consistent with the wind drift potential during winter (Figure 2). Eolian transport may dominate during the late fall to early winter with the common exceedance of threshold wind velocities (>5 m/s) for eolian entrainment (Forman et al., 2008). Also, a low vegetation cover and a drier moisture status of the dune landscape in late fall and early winter may enhance further eolian transport (cf. Ollerhead et al., 2013). Often the presence of snow and interstitial ice binds eolian particles to surfaces necessitating higher threshold shear velocities for transport (Barchyn and Hugenholtz, 2012;Ollerhead et al., 2013), though the sublimation rate may be an important factor in grain release (Vandijk and Law, 1995). The presence of snow ramparts and interstitial layers mostly composed of snow can enhance eolian accretion and dune movement (Koster and Dijkmans, 1988;#CITATION_TAG and Mucher, 1989;Ruz and Allard, 1995). Lastly, eolian transport is accelerated during hurricane passage with wind speeds of >50 m/s. Most storm tracks that impact Cape Cod in the 20th century have a trajectory from the southwest to the northeast, with winds shifting from the northwest and west with westward passage of a cyclonic disturbance (Foster and Boose, 1995, p. 309;Motzkin et al., 2002). Local winds associated with the passage of storms may be topographically funneled by the preexisting tall dune forms.	0	Eolian transport may dominate during the late fall to early winter with the common exceedance of threshold wind velocities (>5 m/s) for eolian entrainment (Forman et al., 2008). Also, a low vegetation cover and a drier moisture status of the dune landscape in late fall and early winter may enhance further eolian transport (cf. Ollerhead et al., 2013). Often the presence of snow and interstitial ice binds eolian particles to surfaces necessitating higher threshold shear velocities for transport (Barchyn and Hugenholtz, 2012;Ollerhead et al., 2013), though the sublimation rate may be an important factor in grain release (Vandijk and Law, 1995). The presence of snow ramparts and interstitial layers mostly composed of snow can enhance eolian accretion and dune movement (Koster and Dijkmans, 1988;#CITATION_TAG and Mucher, 1989;Ruz and Allard, 1995). Lastly, eolian transport is accelerated during hurricane passage with wind speeds of >50 m/s. Most storm tracks that impact Cape Cod in the 20th century have a trajectory from the southwest to the northeast, with winds shifting from the northwest and west with westward passage of a cyclonic disturbance (Foster and Boose, 1995, p. 309;Motzkin et al., 2002). Local winds associated with the passage of storms may be topographically funneled by the preexisting tall dune forms.	r
CC758	Another striking result applies when firms use non-linear prices and are allowed to charge different prices for on-net and off-net calls (as many operators in fact do). In this case, variable prices will be set equal to perceived marginal cost, so that equilibrium profits accrue from the collection of fixed fees and from the provision of termination services. Laffont, Rey and Tirole (1998b) show that the total profit of firms is strictly decreasing in termination charge. Building on this result, Gans and King (2001) show that firms strictly prefer below cost termination charges. The intuition behind this result is that when there is a price differential between on-and off-net calls, there are so-called tariff-mediated network externalities: consumers care about the size of each network. In particular, when termination charge is above cost, off-net calls will be more expensive than on-net calls so that consumers will then prefer to belong to the larger network. As a result, lowering the fixed fee will become a more effective competitive tool to increase market share and price competition is intensified. 12 Clearly, firms prefer instead to soften competition and this can be attained by having termination charge below cost, which comes at the expense of reduced social welfare and consumer surplus. This result has been shown to be very robust. It holds for any number of networks (Calzada and Valletti, 2008), when call externalities are taken into account (#CITATION_TAG, 2005) and when networks are asymmetric (L�_pez and Rey, 2009). Hurkens and Jeon (2009) show that the result also holds when there are both direct network externalities (i.e., elastic subscription demand as in Dessein, 2003) and tariff-mediated network externalities (i.e. onand off-net price differentiation as in Gans and King, 2001).	0	As a result, lowering the fixed fee will become a more effective competitive tool to increase market share and price competition is intensified. 12 Clearly, firms prefer instead to soften competition and this can be attained by having termination charge below cost, which comes at the expense of reduced social welfare and consumer surplus. This result has been shown to be very robust. It holds for any number of networks (Calzada and Valletti, 2008), when call externalities are taken into account (#CITATION_TAG, 2005) and when networks are asymmetric (L�_pez and Rey, 2009). Hurkens and Jeon (2009) show that the result also holds when there are both direct network externalities (i.e., elastic subscription demand as in Dessein, 2003) and tariff-mediated network externalities (i.e. onand off-net price differentiation as in Gans and King, 2001).	f
CC2170	"The capacity to discriminate between different rewards is important for selecting the most valuable reward during decision making. Reward discrimination is limited by two processes, namely stimulus generalization, which is due to physical similarity between stimuli, and pseudoconditioning, which occurs via context conditioning by primary reinforcers (Mackintosh, 1974;Sheafor, 1975). The activating dopamine responses to stimuli consist of two components. The initial component is prone to generalization and thus discriminates poorly, whereas the second component distinguishes well between differently rewarded stimuli (Figure 2B). Generalization in the first component of dopamine responses occurs with neutral stimuli (Schultz and Romo, 1990;Waelti et al., 2001), aversive stimuli (Mirenowicz and Schultz, 1996;Joshua et al., 2008;#CITATION_TAG and Hikosaka, 2009), explicit nonreward predicting stimuli (conditioned inhibitors; Tobler et al., 2003), and delay-predicting stimuli (Kobayashi and Schultz, 2008). Substantial fractions of dopamine neurons are activated by physically salient stimuli (Ljungberg et al., 1992), although these responses seem to be largely due to pseudoconditioning (Kobayashi and Schultz, 2010). The initial, """"false,"""" generalized or pseudoconditioned activation is often followed by a depressant response that may not entirely cancel the effects of the activation. Thus stimulus generalization may lead to net striatal dopamine release with neutral stimuli (Day et al., 2007 (B) Generalization of phasic activating population response from reward predicting stimulus (gray) to explicit no-reward predicting stimulus (black; conditioned inhibitor). The shorter and smaller activations to the conditioned inhibitor are partly offset by depressions. Similar generalizing activations are seen with unrewarded stimuli in up to 50% of dopamine neurons (Waelti et al., 2001;Tobler et al., 2003). Data from Tobler et al. (2003), with permission by Society for Neuroscience."	1	"Reward discrimination is limited by two processes, namely stimulus generalization, which is due to physical similarity between stimuli, and pseudoconditioning, which occurs via context conditioning by primary reinforcers (Mackintosh, 1974;Sheafor, 1975). The activating dopamine responses to stimuli consist of two components. The initial component is prone to generalization and thus discriminates poorly, whereas the second component distinguishes well between differently rewarded stimuli (Figure 2B). Generalization in the first component of dopamine responses occurs with neutral stimuli (Schultz and Romo, 1990;Waelti et al., 2001), aversive stimuli (Mirenowicz and Schultz, 1996;Joshua et al., 2008;#CITATION_TAG and Hikosaka, 2009), explicit nonreward predicting stimuli (conditioned inhibitors; Tobler et al., 2003), and delay-predicting stimuli (Kobayashi and Schultz, 2008). Substantial fractions of dopamine neurons are activated by physically salient stimuli (Ljungberg et al., 1992), although these responses seem to be largely due to pseudoconditioning (Kobayashi and Schultz, 2010). The initial, """"false,"""" generalized or pseudoconditioned activation is often followed by a depressant response that may not entirely cancel the effects of the activation. Thus stimulus generalization may lead to net striatal dopamine release with neutral stimuli (Day et al., 2007 (B) Generalization of phasic activating population response from reward predicting stimulus (gray) to explicit no-reward predicting stimulus (black; conditioned inhibitor)."	r
CC261	Only by explicitly simulating climates including all possible weather states consistent with the climate forcing conditions for the period of interest can the uncertainties be addressed associated with the question: what fraction of the event probability is attributable to the anthropogenic drivers? The climate forcing conditions are simulated using observed greenhouse gas and aerosol forcings and observed SSTs. Thus not all possible climate states are simulated but possible weather given the historical SSTs. The large initial condition ensemble used addresses the uncertainty in the simulated weather to a high degree and under the assumption of an unchanging relationship between hazard and resulting damage, event probability can be seen as a proxy for risk (Pall et al. 2011). The return times of the 5-day means in Figs. 3 and 4 are almost straight, parallel, lines comparable to the return times of river runoff (Pall et al. 2011) which is in contrast to, e.g., monthly means or shorter timescales (Fowler et al. 2010). The straight parallel lines corroborate the finding of, e.g., Stott et al. (2004) and #CITATION_TAG et al. (2007) that for the fraction of attributable risk (FAR) calculation (Allen 2003), the actual value of the threshold does not matter. Therefore return times of precipitation based on 5-day means are a good measure for event attribution studies as a bias in magnitudes does not matter if the fractional change in return time is independent of the threshold. Our choice to concentrate on 5-day means is therefore not only adequate in terms of flood risk assessment but also for the independence of the threshold for risk attribution.	0	The large initial condition ensemble used addresses the uncertainty in the simulated weather to a high degree and under the assumption of an unchanging relationship between hazard and resulting damage, event probability can be seen as a proxy for risk (Pall et al. 2011). The return times of the 5-day means in Figs. 3 and 4 are almost straight, parallel, lines comparable to the return times of river runoff (Pall et al. 2011) which is in contrast to, e.g., monthly means or shorter timescales (Fowler et al. 2010). The straight parallel lines corroborate the finding of, e.g., Stott et al. (2004) and #CITATION_TAG et al. (2007) that for the fraction of attributable risk (FAR) calculation (Allen 2003), the actual value of the threshold does not matter. Therefore return times of precipitation based on 5-day means are a good measure for event attribution studies as a bias in magnitudes does not matter if the fractional change in return time is independent of the threshold. Our choice to concentrate on 5-day means is therefore not only adequate in terms of flood risk assessment but also for the independence of the threshold for risk attribution.	r
CC2299	Expression of the T cell stimulatory molecules, HLA-DR and CD86, as well as the inhibitory PDL1 protein was induced on influenza-infected macrophages. HLA-DR is a component of the Major Histocompatibility Class (MHC) II complex involved in antigen presentation to CD4+ T cells [25], whilst CD86 is classically associated with activation of the CD28 signalling pathway #CITATION_TAG. Why the infected macrophage should upregulate both stimulatory and inhibitory signals at the same time and the functional significance of this is open for speculation. Previous investigators have suggested that there is equilibrium between expression of these markers and the activation state, with inhibitory signalling being predominant when CD80/CD86 expression is low [27]. Selenko-Gebauer et al (2003) hypothesised that PDL1 expression may function as an immunological rheostat to set a T cell activation threshold [27]. One further possibility is suggested by the fact that both CD80 and CD86 can induce signalling by the inhibitory CTLA-4 receptor on T cells as well as activating the CD28 pathway [28], therefore an inhibitory milieu may be propagated via the infected macrophage. This may be especially important in the lung as unconstrained inflammation can lead to lung function impairment and death [6].	0	Expression of the T cell stimulatory molecules, HLA-DR and CD86, as well as the inhibitory PDL1 protein was induced on influenza-infected macrophages. HLA-DR is a component of the Major Histocompatibility Class (MHC) II complex involved in antigen presentation to CD4+ T cells [25], whilst CD86 is classically associated with activation of the CD28 signalling pathway #CITATION_TAG. Why the infected macrophage should upregulate both stimulatory and inhibitory signals at the same time and the functional significance of this is open for speculation. Previous investigators have suggested that there is equilibrium between expression of these markers and the activation state, with inhibitory signalling being predominant when CD80/CD86 expression is low [27]. Selenko-Gebauer et al (2003) hypothesised that PDL1 expression may function as an immunological rheostat to set a T cell activation threshold [27].	L
CC1952	"In the past, much progress in the understanding of properties of ferroic ABO 3 perovskites, such as ferroelasticity or ferroelectricity, and their related phase transitions has been achieved through temperature-, or chemical composition-dependent investigations. The use of pressure has been much rarer and was mostly limited to pressures below 10 GPa due to experimental difficulties which have now been overcome for a number of years by the use of diamond-anvil cells. One of the attracting properties of the external parameter high-pressure is its character of a ""cleaner"" variable, since it acts only on interatomic distances [1]. Furthermore, external pressure leads to otherwise unachievable reductions of volume and chemical bond lengths. Despite the now accessible investigation of phase transitions into the very high pressure regime, experimental investigations of prototype ferroic perovskites above 50 GPa remain still scarce, with some notable exceptions among ferroelectrics (e.g. KNbO [2], PbTiO 3 [3]), relaxor ferroelectrics (PbZn 1/3 Nb 2/3 O 3 [4]), ferroelastics (SrTiO 3 [5]). Explorations of pressure-temperature or pressure-substitution phase diagrams remain even rarer #CITATION_TAG."	0	"One of the attracting properties of the external parameter high-pressure is its character of a ""cleaner"" variable, since it acts only on interatomic distances [1]. Furthermore, external pressure leads to otherwise unachievable reductions of volume and chemical bond lengths. Despite the now accessible investigation of phase transitions into the very high pressure regime, experimental investigations of prototype ferroic perovskites above 50 GPa remain still scarce, with some notable exceptions among ferroelectrics (e.g. KNbO [2], PbTiO 3 [3]), relaxor ferroelectrics (PbZn 1/3 Nb 2/3 O 3 [4]), ferroelastics (SrTiO 3 [5]). Explorations of pressure-temperature or pressure-substitution phase diagrams remain even rarer #CITATION_TAG."	r
CC2497	ODT was preceded the Linear Eddy Model (LEM) #CITATION_TAG, in which mapping frequency is a specified function of map size, much as �� has a specified dependence on sub-tree level when the flow simulation is not used to determine �� locally. The HiPS formulation and analysis in Sects. 2 through 4 in many ways reflects analogous considerations pertinent to LEM.	0	ODT was preceded the Linear Eddy Model (LEM) #CITATION_TAG, in which mapping frequency is a specified function of map size, much as �� has a specified dependence on sub-tree level when the flow simulation is not used to determine �� locally. The HiPS formulation and analysis in Sects. 2 through 4 in many ways reflects analogous considerations pertinent to LEM.	O
CC368	An analogous challenge has arisen in fMRI studies of visual perception. Early attempts to localize specific perceptions to specific brain areas worked only for grossly different stimuli-visualizing navigating a house compared to imagining playing tennis (Owen et al., 2006)-or for simple stimuli in early, highly specialized, visual areas (Kay et al., 2008). More latterly, however, multivoxel pattern analysis (MVPA), in which patterns of activity across wide areas of the brain are analyzed, has produced a significant increase in cognitive resolution. Fairly similar stimuli, for example chairs and shoes (Norman et al., 2006;deCharms, 2008;Poldrack, 2011), or over-riding categories of images such as living or non-living (Naselaris et al., 2012) can now be recognized from pattern information fMRI (Formisano and Kriegeskorte, 2012) data. Not only perceptions, but also images and memories (Chadwick et al., 2012;Rissman and Wagner, 2012) are starting to be distinguished. Beginnings are starting to be made in reproducing data across as well as within subjects (Accamma and Suma, 2012;#CITATION_TAG and Connolly, 2012). Significantly, cognitive resolution appears to increase as the focus of the analysis is widened to include more brain areas.	0	More latterly, however, multivoxel pattern analysis (MVPA), in which patterns of activity across wide areas of the brain are analyzed, has produced a significant increase in cognitive resolution. Fairly similar stimuli, for example chairs and shoes (Norman et al., 2006;deCharms, 2008;Poldrack, 2011), or over-riding categories of images such as living or non-living (Naselaris et al., 2012) can now be recognized from pattern information fMRI (Formisano and Kriegeskorte, 2012) data. Not only perceptions, but also images and memories (Chadwick et al., 2012;Rissman and Wagner, 2012) are starting to be distinguished. Beginnings are starting to be made in reproducing data across as well as within subjects (Accamma and Suma, 2012;#CITATION_TAG and Connolly, 2012). Significantly, cognitive resolution appears to increase as the focus of the analysis is widened to include more brain areas.	n
CC2071	A Massachusetts Senate Document from AD 1714 indicates the scale of blowing sand around Provincetown was such that the harbor at Cape Cod was infilling jeopardizing docking of ships (#CITATION_TAG, 1979)	0	A Massachusetts Senate Document from AD 1714 indicates the scale of blowing sand around Provincetown was such that the harbor at Cape Cod was infilling jeopardizing docking of ships (#CITATION_TAG, 1979)	A
CC1643	"Compensation systems have several roles beyond creating incentives (e.g., #CITATION_TAG, Neely & Kennerly 1999;Ittner & Larcker 2002). This fact is most evident in performance evaluation. Firms may use an evaluation to monitor and reward effort. However, performance also depends on factors other than effort, such as the employee""s abilities, training, information, and working relationship with colleagues."	0	"Compensation systems have several roles beyond creating incentives (e.g., #CITATION_TAG, Neely & Kennerly 1999;Ittner & Larcker 2002). This fact is most evident in performance evaluation. Firms may use an evaluation to monitor and reward effort. However, performance also depends on factors other than effort, such as the employee""s abilities, training, information, and working relationship with colleagues."	C
CC1679	"A similar but distinct approach to unsupervised extraction is distant supervision. Similarly as unsupervised extraction methods, distant supervision methods don""t require any labeled data, but make use of weakly labeled data, such as data extracted from a knowledge base. Distant supervision has been applied to relation extraction (Liu et al., 2014), extraction of gene interactions (#CITATION_TAG et al., 2015), PPI extraction , and identification of PICO elements (Wallace et al., 2016). The advantage of our approach compared to the distantly supervised methods is that it does not require any underlying knowledge base or a similar source of data."	0	"A similar but distinct approach to unsupervised extraction is distant supervision. Similarly as unsupervised extraction methods, distant supervision methods don""t require any labeled data, but make use of weakly labeled data, such as data extracted from a knowledge base. Distant supervision has been applied to relation extraction (Liu et al., 2014), extraction of gene interactions (#CITATION_TAG et al., 2015), PPI extraction , and identification of PICO elements (Wallace et al., 2016). The advantage of our approach compared to the distantly supervised methods is that it does not require any underlying knowledge base or a similar source of data."	s
CC2189	While we were able to identify a substantial body of papers evaluating interventions using high-technology AAC, there is currently a lack of high-quality evidence of effect. This is due to a lack of good-quality studies rather than there being evidence of a lack of effect. It is important to note the predominance of case series or case study designs in the field representing only level IV evidence #CITATION_TAG . There is currently a dearth of studies with comparator arms, which while presenting challenges must be a future priority if the evidence base is to be strengthened. While considered to be the design most subject to bias, case studies are commonly used and reported in the healthcare literature. It has been argued [12] that they can be a helpful source of information about adverse events, can generate hypotheses, provide more participants, longer follow-up and are more generalisable than controlled trials. However, they have significant limitations in terms of providing conclusive evidence of effectiveness. The evidence from these case studies should be used to underpin stronger designs in future research.	5	While we were able to identify a substantial body of papers evaluating interventions using high-technology AAC, there is currently a lack of high-quality evidence of effect. This is due to a lack of good-quality studies rather than there being evidence of a lack of effect. It is important to note the predominance of case series or case study designs in the field representing only level IV evidence #CITATION_TAG . There is currently a dearth of studies with comparator arms, which while presenting challenges must be a future priority if the evidence base is to be strengthened. While considered to be the design most subject to bias, case studies are commonly used and reported in the healthcare literature. It has been argued [12] that they can be a helpful source of information about adverse events, can generate hypotheses, provide more participants, longer follow-up and are more generalisable than controlled trials. However, they have significant limitations in terms of providing conclusive evidence of effectiveness.	 
CC988	"Experience sampling has a long and sometimes controversial history in psychology, but has recently seen a resurgence of popularity. Experience Sampling Methodology is a family of empirical methods that allow researchers to obtain measurements of an individual""s account of their internal mental events outside artificial laboratory settings and within the context of their normal everyday settings. In general, these methods involve interrupting an individual while they are going about an activity in its normal setting and asking them to make brief subjective reports about their current subjective state, via brief notes or rating scales. By probing immediate self-report of inner experience, this method enables researchers to measure a person""s momentary thoughts, feelings and action-tendencies than by asking through more retrospective recall methods (for more detail see Feldman, Barrett & Barrett, 2001;Myin-Germeys , 2009;Smyth et al., 2001;#CITATION_TAG & Shiffman, 1994). This approach was employed in this experiment to assess momentary internal experience of each of the imagery components of interest. The objective of our first experiment was to explore what contributed to dancers"" thought patterns while creating movement in response to tasks set them by McGregor, and how changes in the nature of the task and the mode of creation affected these patterns of thought. We used two forms of task instruction, based on characteristic tasks that we had observed McGregor use previously. One task condition was based upon spatial-praxic imagery: one of the actual tasks used in this experiment was ""Imagine an object. Reduce it to a line drawing. Visualise an element of it. Describe what is visible"". The other condition was based upon emotional instructions: one example from this experiment being ""Think of a familiar song or piece of music. Focus on the memories, feelings or sensation it evokes, in you or someone else. Translate it into 3d and draw the meaning."	5	"Experience sampling has a long and sometimes controversial history in psychology, but has recently seen a resurgence of popularity. Experience Sampling Methodology is a family of empirical methods that allow researchers to obtain measurements of an individual""s account of their internal mental events outside artificial laboratory settings and within the context of their normal everyday settings. In general, these methods involve interrupting an individual while they are going about an activity in its normal setting and asking them to make brief subjective reports about their current subjective state, via brief notes or rating scales. By probing immediate self-report of inner experience, this method enables researchers to measure a person""s momentary thoughts, feelings and action-tendencies than by asking through more retrospective recall methods (for more detail see Feldman, Barrett & Barrett, 2001;Myin-Germeys , 2009;Smyth et al., 2001;#CITATION_TAG & Shiffman, 1994). This approach was employed in this experiment to assess momentary internal experience of each of the imagery components of interest. The objective of our first experiment was to explore what contributed to dancers"" thought patterns while creating movement in response to tasks set them by McGregor, and how changes in the nature of the task and the mode of creation affected these patterns of thought. We used two forms of task instruction, based on characteristic tasks that we had observed McGregor use previously."	p
CC2427	"However, this pluralism is more than institutional: It has also been internalised at an individual level by much of the population. Irrespective of personal worldview, a great many citizens embrace pluralism and adhere to the ethical modernity of secular humanism. (Secular humanism is understood here to be as that suggested by writers such as H.T. Engelhardt, a physician and a philosopher: an ethics of separation between the personal and the public domain, a core ethics originating in the Enlightenment, positing a common language between ""moral foreigners"" [Engelhardt 1991].) Other indications of this abound. Many Catholic intellectuals (e.g., Lenaers 2001) and health care workers (as well as much of the general public) hold individual autonomy and responsibility in the highest esteem. Catholic physicians (most of whom prefer to call themselves ""Christian"" when given the choice) abbreviate lives no less than their free-thinking colleagues when it is upon patients\"" request, and they tend to do less (compassionate) life abbreviation without explicit request (Deliens et al. 2000;Bernheim 2002). It is probable that this Catholic modernity is connected with the enthusiasm for the concept of ""personalism"" as proposed by Catholic thinkers such as Mounier (1949), Ladri��re (2004), Janssens (1957, #CITATION_TAG (1974), andSchillebeeckx (1982) at the Dutch-and French-language Catholic universities of Leuven/Louvain, who together have trained about half of Belgian physicians. As a result of such progressive attitudes, respect by ""free-thinkers"" for modern religious thought has increased (e.g., Bernheim 2002). The philosopher Leo Apostel\""s ""atheistic religiosity"" was seminal in this respect (Apostel 1998). In May 2013, Christian de Duve, an emeritus professor of the Universitͩ Catholique de Louvain and Nobel laureate of medicine, ailing but not moribund, chose ""rational"" death by euthanasia at the age of 95 after giving a valedictory interview announcing and justifying his decision."	0	") Other indications of this abound. Many Catholic intellectuals (e.g., Lenaers 2001) and health care workers (as well as much of the general public) hold individual autonomy and responsibility in the highest esteem. Catholic physicians (most of whom prefer to call themselves ""Christian"" when given the choice) abbreviate lives no less than their free-thinking colleagues when it is upon patients\"" request, and they tend to do less (compassionate) life abbreviation without explicit request (Deliens et al. 2000;Bernheim 2002). It is probable that this Catholic modernity is connected with the enthusiasm for the concept of ""personalism"" as proposed by Catholic thinkers such as Mounier (1949), Ladri��re (2004), Janssens (1957, #CITATION_TAG (1974), andSchillebeeckx (1982) at the Dutch-and French-language Catholic universities of Leuven/Louvain, who together have trained about half of Belgian physicians. As a result of such progressive attitudes, respect by ""free-thinkers"" for modern religious thought has increased (e.g., Bernheim 2002). The philosopher Leo Apostel\""s ""atheistic religiosity"" was seminal in this respect (Apostel 1998). In May 2013, Christian de Duve, an emeritus professor of the Universitͩ Catholique de Louvain and Nobel laureate of medicine, ailing but not moribund, chose ""rational"" death by euthanasia at the age of 95 after giving a valedictory interview announcing and justifying his decision."	p
CC1081	Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). The work was almost entirely performed in protoplast with large use of transient transformation but clearly showed that fusogenic and nonfusogenic functions can be ascribed to the same SNARE and be dependent on protein localization. When anchored to the TGN membrane, AtSYP51, and AtSYP52 behaved as t-SNARE, with a fusogenic role, but when they were sorted to the tonoplast their role become non-fusogenic. Despite a certain level of functional specificity, they both seemed to play a structural role in the tonoplast formation by influencing the arrival of new membrane from prevacuolar compartments. Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from the central vacuole and already observed with other vacuolar markers (#CITATION_TAG et al., 2008).	0	The work was almost entirely performed in protoplast with large use of transient transformation but clearly showed that fusogenic and nonfusogenic functions can be ascribed to the same SNARE and be dependent on protein localization. When anchored to the TGN membrane, AtSYP51, and AtSYP52 behaved as t-SNARE, with a fusogenic role, but when they were sorted to the tonoplast their role become non-fusogenic. Despite a certain level of functional specificity, they both seemed to play a structural role in the tonoplast formation by influencing the arrival of new membrane from prevacuolar compartments. Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from the central vacuole and already observed with other vacuolar markers (#CITATION_TAG et al., 2008).	 
CC910	"Through the quantification of interspecific variation in plant functional traits, comparative plant ecology has gained momentum in explaining species distributions, tradeoffs in biochemistry and physiology, as well as whole-ecosystem processes (Westoby and Wright 2006). Lavorel et al. (2007) considered vascular plant functional traits as predictors of key vegetation responses to environmental variation and vegetation effects on ecosystem functions at diverse spatial and temporal scales. In contrast to the effort put into developing extensive international methodological protocols and databases for quantitative analysis of vascular plant traits (Cornelissen et al. 2003), non-vascular plants such as cryptogams have remained mostly out of the scientists"" trait research scope to date, even though the application of bryophyte and lichen traits as drivers of large-scale biogeochemistry holds much promise. Indeed, bryophytes and lichens are of particular importance in cold biomes such as the Arctic tundra (#CITATION_TAG 1997), where they are key contributors to green biomass and control soil hydrology, temperatures and chemistry (Cornelissen et al. 2007). One aspect of the latter is their ability to form symbiotic relationships with cyanobacteria (Dalton and Chatfield 1985;During and Van Tooren 1990;Henriksson et al. 1987). Most bryophytes have a high water retention capacity and thus provide a stable and favourable habitat for cyanobacterial growth and N fixation activity (Dickson 2000). In lichens, cyanobacteria are a symbiont which provide their fungal partner with nitrogen from fixed atmospheric N in exchange for physical protection and-in the case of tripartite lichens-from carbohydrates in the fungi (Nash 1996). Since nitrogen is a principal limiting environmental factor for plant growth and soil organic matter turnover in polar regions (Longton 1988) and biological nitrogen fixation there occurs mostly in cyanobacteria (Solheim et al. 1996), the interaction between cryptogams and cyanobacteria forms an important field for ecological investigations."	0	"Through the quantification of interspecific variation in plant functional traits, comparative plant ecology has gained momentum in explaining species distributions, tradeoffs in biochemistry and physiology, as well as whole-ecosystem processes (Westoby and Wright 2006). Lavorel et al. (2007) considered vascular plant functional traits as predictors of key vegetation responses to environmental variation and vegetation effects on ecosystem functions at diverse spatial and temporal scales. In contrast to the effort put into developing extensive international methodological protocols and databases for quantitative analysis of vascular plant traits (Cornelissen et al. 2003), non-vascular plants such as cryptogams have remained mostly out of the scientists"" trait research scope to date, even though the application of bryophyte and lichen traits as drivers of large-scale biogeochemistry holds much promise. Indeed, bryophytes and lichens are of particular importance in cold biomes such as the Arctic tundra (#CITATION_TAG 1997), where they are key contributors to green biomass and control soil hydrology, temperatures and chemistry (Cornelissen et al. 2007). One aspect of the latter is their ability to form symbiotic relationships with cyanobacteria (Dalton and Chatfield 1985;During and Van Tooren 1990;Henriksson et al. 1987). Most bryophytes have a high water retention capacity and thus provide a stable and favourable habitat for cyanobacterial growth and N fixation activity (Dickson 2000). In lichens, cyanobacteria are a symbiont which provide their fungal partner with nitrogen from fixed atmospheric N in exchange for physical protection and-in the case of tripartite lichens-from carbohydrates in the fungi (Nash 1996)."	e
CC1164	"The processing priority for goal-related cues has been shown using a variety of other cognitive methods. One is the use of quasi-Stroop procedures. In the classic Stroop procedure, under instructions to name the color of the font of words displayed one at a time as quickly as possible, participants typically respond more slowly when the meaning of the word conflicts with the color, such as green font for the word RED. Similarly, reaction times (RTs) in reporting the font color of goal-related words are typically on average longer than they are to non-goal-related words (#CITATION_TAG et al., 1994;Riemann and McNally, 1995;Gilboa-Schechtman et al., 2000;Fadardi and Cox, 2008) or images. Presumably, the owngoal-relatedness of the word""s meaning grabs processing priority over identification of font color, thereby slowing reporting of font color. This processing priority could readily account for the tendency of conscious mental content in mind-wandering to gravitate toward material related to the individual""s own goals. The stimuli for such shifts in content are presumably internal ones in the individual""s own ongoing stream of thought."	0	"The processing priority for goal-related cues has been shown using a variety of other cognitive methods. One is the use of quasi-Stroop procedures. In the classic Stroop procedure, under instructions to name the color of the font of words displayed one at a time as quickly as possible, participants typically respond more slowly when the meaning of the word conflicts with the color, such as green font for the word RED. Similarly, reaction times (RTs) in reporting the font color of goal-related words are typically on average longer than they are to non-goal-related words (#CITATION_TAG et al., 1994;Riemann and McNally, 1995;Gilboa-Schechtman et al., 2000;Fadardi and Cox, 2008) or images. Presumably, the owngoal-relatedness of the word""s meaning grabs processing priority over identification of font color, thereby slowing reporting of font color. This processing priority could readily account for the tendency of conscious mental content in mind-wandering to gravitate toward material related to the individual""s own goals. The stimuli for such shifts in content are presumably internal ones in the individual""s own ongoing stream of thought."	i
CC2033	Optical dating is eminently suitable for deciphering the Holocene record of eolian deposition on Cape Cod, Massachusetts. The luminescence emission is a measure of the time between the last light exposure during grain transport and deposition, and the burial period, when shielded from any further light exposure (cf. Aitken, 1992). The development of single aliquot regeneration (SAR) protocols provides improved accuracy and precision for dating the burial time of eolian quartz grains (e.g., #CITATION_TAG and Wintle, 2003;Wintle and Murray, 2006) and needed resolution for dating young sediments, <500 years old (Madsen and Murray, 2009). Most recently SAR protocols have yielded ages consistent with independent chronologic control for the past ca. 600 year on sand beds deposited within marsh sediments, associated with beach wash over during hurricane events on southern Cape Cod . The eolian strata was sampled for luminescence dating only after there is a full understanding of the sedimentology, the extent of soil development and associated lateral changes. Often, at least two samples were extracted for luminescence dating from each eolian stratigraphic unit avoiding horizons with pedogenesis, and favoring primary eolian depositional strata. Sampling for OSL dating used light tight 5-cm-diameter and 15-cm-long sections of black ABS pipe, which were easily hammered into the desired sampling level.	5	Optical dating is eminently suitable for deciphering the Holocene record of eolian deposition on Cape Cod, Massachusetts. The luminescence emission is a measure of the time between the last light exposure during grain transport and deposition, and the burial period, when shielded from any further light exposure (cf. Aitken, 1992). The development of single aliquot regeneration (SAR) protocols provides improved accuracy and precision for dating the burial time of eolian quartz grains (e.g., #CITATION_TAG and Wintle, 2003;Wintle and Murray, 2006) and needed resolution for dating young sediments, <500 years old (Madsen and Murray, 2009). Most recently SAR protocols have yielded ages consistent with independent chronologic control for the past ca. 600 year on sand beds deposited within marsh sediments, associated with beach wash over during hurricane events on southern Cape Cod . The eolian strata was sampled for luminescence dating only after there is a full understanding of the sedimentology, the extent of soil development and associated lateral changes. Often, at least two samples were extracted for luminescence dating from each eolian stratigraphic unit avoiding horizons with pedogenesis, and favoring primary eolian depositional strata.	e
CC1302	While many studies cite correlations between academic performance and various measures of motivation, particularly self-efficacy, learning goals, and intrinsic motivation, evidence supporting causal relationships between motivation and academic performance are less consistent, and are influenced to some extent by the selection of factors included in any specific study. For example, #CITATION_TAG and Furnham (2003) and Breiman (2001) found motivation was a mediator between conscientiousness and performance, while Komarraju et al. (2009) found conscientiousness mediated between intrinsic motivation and performance. Komarraju et al. (2009) also noted that motivation did not account for any additional variance on academic performance beyond what was already explained by the Big Five. Brown et al. (2008) on the other hand, in a study not including personality factors, found that selfefficacy had a causal relationship with academic performance. In a meta-analysis covering a range of psychosocial and study skills impacting on academic performance at the tertiary level, excluding personality factors, Robbins et al. (2004) found self-efficacy and achievement motivation to be the best predictors of GPA attained by learners. A number of studies investigating both personality and motivation argue that personality-based factors are a better predictor of academic performance than motivation (De Feyter et al., 2012;Komarraju et al., 2009). However Zuffian�_ et al. (2013) found that self-efficacy significantly contributed to the explained variance in academic performance over and above ability and personality. It also has a more practical value in that self-efficacy beliefs are more easily changed than ability or personality. This would suggest that while correlations exist between factors of personality and motivation, factors of personality, particularly conscientiousness, and factors of motivation, particularly self-efficacy and achievement goals, each have value, and are worth further consideration in models of student learning.	0	While many studies cite correlations between academic performance and various measures of motivation, particularly self-efficacy, learning goals, and intrinsic motivation, evidence supporting causal relationships between motivation and academic performance are less consistent, and are influenced to some extent by the selection of factors included in any specific study. For example, #CITATION_TAG and Furnham (2003) and Breiman (2001) found motivation was a mediator between conscientiousness and performance, while Komarraju et al. (2009) found conscientiousness mediated between intrinsic motivation and performance. Komarraju et al. (2009) also noted that motivation did not account for any additional variance on academic performance beyond what was already explained by the Big Five. Brown et al. (2008) on the other hand, in a study not including personality factors, found that selfefficacy had a causal relationship with academic performance. In a meta-analysis covering a range of psychosocial and study skills impacting on academic performance at the tertiary level, excluding personality factors, Robbins et al. (2004) found self-efficacy and achievement motivation to be the best predictors of GPA attained by learners.	o
CC383	Similarly, though there is evidence that different modalities of therapy may have different levels of effectiveness (see Tolin, 2010 for a meta-analytic comparison of CBT with other therapies), where this does occur this appears to be more a quantitative than a qualitative difference. The outcomes of psychodynamic, personcentred, and behavioral psychotherapy are broadly equivalent despite their varieties of approaches and targets for therapeutic change (Stiles et al., 2008;#CITATION_TAG and Hughes, 2009) perhaps because they work via common final paths (Mansell, 2011).	0	Similarly, though there is evidence that different modalities of therapy may have different levels of effectiveness (see Tolin, 2010 for a meta-analytic comparison of CBT with other therapies), where this does occur this appears to be more a quantitative than a qualitative difference. The outcomes of psychodynamic, personcentred, and behavioral psychotherapy are broadly equivalent despite their varieties of approaches and targets for therapeutic change (Stiles et al., 2008;#CITATION_TAG and Hughes, 2009) perhaps because they work via common final paths (Mansell, 2011).	h
CC1769	This is a system of elliptic type because of the condition u > 0. Indeed, the eigenvalues of the coefficient matrix v u __�1 v are complex conjugate, �_ = v �� i ___ u. So, the Cauchy problem for the system (1.8) is ill-posed in the Hadamard sense (cf. Mͩtivier 2006;Carles 2007). Even for analytic initial data the life span of a typical solution is finite, t < t 0 . The x-and t-derivatives explode at some point x = x 0 when the time approaches t 0 . This phenomenon is similar to the gradient catastrophe of solutions to nonlinear hyperbolic PDEs (#CITATION_TAG 1995).	2	This is a system of elliptic type because of the condition u > 0. Indeed, the eigenvalues of the coefficient matrix v u __�1 v are complex conjugate, �_ = v �� i ___ u. So, the Cauchy problem for the system (1.8) is ill-posed in the Hadamard sense (cf. Mͩtivier 2006;Carles 2007). Even for analytic initial data the life span of a typical solution is finite, t < t 0 . The x-and t-derivatives explode at some point x = x 0 when the time approaches t 0 This phenomenon is similar to the gradient catastrophe of solutions to nonlinear hyperbolic PDEs (#CITATION_TAG 1995).	s
CC611	"As noted by Boltzmann [28] and Schroedinger [29], thermodynamics must apply to the living organisms as well. Life feeds on low entropy ""food"" and rejects high entropy ""waste"" products and heat, thereby fueling its metabolism and sustaining its structure. Living organisms, just as any other dissipative process, maintain their state away from thermodynamic equilibrium by an overall net export of entropy to the surroundings. This commonality of living organisms and purely physical dissipative processes led Lovelock and Margulis [30] to use the metaphor of describing Earth as a superorganism. To characterize the organization of the steady state that describes the sum of all living organisms, in an ecosystem at the small scale, or in the biosphere at the global scale, the same thermodynamic maximization principles have been proposed, first by Lotka [31,32], and further explored by others [33][34][35][36]#CITATION_TAG[38][39]."	0	"Life feeds on low entropy ""food"" and rejects high entropy ""waste"" products and heat, thereby fueling its metabolism and sustaining its structure. Living organisms, just as any other dissipative process, maintain their state away from thermodynamic equilibrium by an overall net export of entropy to the surroundings. This commonality of living organisms and purely physical dissipative processes led Lovelock and Margulis [30] to use the metaphor of describing Earth as a superorganism. To characterize the organization of the steady state that describes the sum of all living organisms, in an ecosystem at the small scale, or in the biosphere at the global scale, the same thermodynamic maximization principles have been proposed, first by Lotka [31,32], and further explored by others [33][34][35][36]#CITATION_TAG[38][39]."	h
CC1381	"This theorem on the one hand entails that any positive FPT result obtainable for the model in question essentially implies that there is a ""downward reduction"" for the underlying problem to some sort of smaller or less-complex instance of the same problem, which can then be solved -whilst on the other hand (assuming W[1] 4 = FPT) any negative result implies that there is no such downward reduction. This (formal) correspondence between complex instances and simpler manifestations of a problem seems to match well with an observation from problem-solving and reasoning experiments with human participants: Different forms of reductions from complex to simpler (but still solution-equivalent) problems are reported to be pervasive and crucial in many human problem solving scenarios (see, e.g.,  #CITATION_TAG)."	0	"This theorem on the one hand entails that any positive FPT result obtainable for the model in question essentially implies that there is a ""downward reduction"" for the underlying problem to some sort of smaller or less-complex instance of the same problem, which can then be solved -whilst on the other hand (assuming W[1] 4 = FPT) any negative result implies that there is no such downward reduction. This (formal) correspondence between complex instances and simpler manifestations of a problem seems to match well with an observation from problem-solving and reasoning experiments with human participants: Different forms of reductions from complex to simpler (but still solution-equivalent) problems are reported to be pervasive and crucial in many human problem solving scenarios (see, e.g.,  #CITATION_TAG)."	h
CC2281	There were some early attempts to study the structure along the single strands in TRACE observations (#CITATION_TAG et al., 2002). Later, it was shown that in many cases, hot loop structures observed in active regions with the Yohkoh/SXT ( > 3 MK) are not exactly co-spatial with warm structures ( ___ 1 -2 MK) observed in the EUV bands (195��) of the SoHO/EIT, nor they cool down to become visible with EIT (Nagata et al., 2003;Schmieder et al., 2004). In another study, hot monolithic loops visible with the Yohkoh/SXT were instead resolved as stranded cooler structures with TRACE at later times (Winebarger and Warren, 2005), although the large time delay (1 to 3 hours) is hardly compatible with the cooling time from SXT to TRACE sensitivity. More systematic studies of TRACE images proposed that about 10% of the positions across loops can be fitted with an isothermal model, indicating coherent loop components about 2000 km wide (Aschwanden and Nightingale, 2005;Aschwanden et al., 2007).	5	There were some early attempts to study the structure along the single strands in TRACE observations (#CITATION_TAG et al., 2002). Later, it was shown that in many cases, hot loop structures observed in active regions with the Yohkoh/SXT ( > 3 MK) are not exactly co-spatial with warm structures ( ___ 1 -2 MK) observed in the EUV bands (195��) of the SoHO/EIT, nor they cool down to become visible with EIT (Nagata et al., 2003;Schmieder et al., 2004). In another study, hot monolithic loops visible with the Yohkoh/SXT were instead resolved as stranded cooler structures with TRACE at later times (Winebarger and Warren, 2005), although the large time delay (1 to 3 hours) is hardly compatible with the cooling time from SXT to TRACE sensitivity. More systematic studies of TRACE images proposed that about 10% of the positions across loops can be fitted with an isothermal model, indicating coherent loop components about 2000 km wide (Aschwanden and Nightingale, 2005;Aschwanden et al., 2007).	T
CC2107	"Recently alternative schemes to ""simulate"" artificial gauge fields for neutral atoms have been explored using two-dimensional (2D) optical lattices [8,9,10,11,12,13,#CITATION_TAG]. As they do not involve any mechanical rotation of the system, they should be less sensitive to the imperfection of the trapping potential and thus easier to implement. The guideline for these proposals is the celebrated Harper model [15,16], defined by the two-dimensional (2D) single-particle Hamiltonian,"	1	"Recently alternative schemes to ""simulate"" artificial gauge fields for neutral atoms have been explored using two-dimensional (2D) optical lattices [8,9,10,11,12,13,#CITATION_TAG]. As they do not involve any mechanical rotation of the system, they should be less sensitive to the imperfection of the trapping potential and thus easier to implement. The guideline for these proposals is the celebrated Harper model [15,16], defined by the two-dimensional (2D) single-particle Hamiltonian,"	R
CC1172	"The strong evidence for perceptual decoupling raises a further question regarding the processes involved in protecting the integrity of segments of behavior -that is, of relatively integrated response sequences that lead from the decision to act (or the start of a thought train) to the intended endpoint of the sequence (see also #CITATION_TAG et al., 2013;. That launching such a sequence instates an inhibition of interruptive factors seems clear. External interruptions of on-going behavior before some logical endpoint or pause ""leads to visceral arousal"" and emotional upset (Mandler, 1964, p. 163). The entire literature on emotional accompaniments of extinction of operant behaviors attests to this (e.g., Klinger, 1975Klinger, , 1977. The expectation that accustomed sequences of behavior will end as usual seems ingrained in people from an early age. EEG evidence with eventrelated potentials indicates that infants as young as nine months react with N400 deflections (negative deflections after 400 ms poststimulus) when sequences they observe end unexpectedly (Reid et al., 2009). Perceptual decoupling may, accordingly, be part of a more extensive process that protects ongoing behavior (see also Klinger, 1971Klinger, , 2011, in regard to a meaning-complex theory of response organization; also behavioral chunking, e.g., Perlman et al., 2010)."	0	"The strong evidence for perceptual decoupling raises a further question regarding the processes involved in protecting the integrity of segments of behavior -that is, of relatively integrated response sequences that lead from the decision to act (or the start of a thought train) to the intended endpoint of the sequence (see also #CITATION_TAG et al., 2013;. That launching such a sequence instates an inhibition of interruptive factors seems clear. External interruptions of on-going behavior before some logical endpoint or pause ""leads to visceral arousal"" and emotional upset (Mandler, 1964, p. 163). The entire literature on emotional accompaniments of extinction of operant behaviors attests to this (e.g., Klinger, 1975Klinger, , 1977."	T
CC2998	Besides bridges, several other man-made features proved to be useful for georeferencing. One example is the Prater Main Avenue in the imperial hunting ground Prater. The originally 5.6 km long boulevard was constructed as a straight line in 1537/38 on a large island close to the city (Fig. 6). It functioned as a main landmark in historical riverscape cartography (Slezak 1980). Borders of land properties, hunting grounds or administrative borders also proved to be very useful; in particular, the so-called Burgfriedsgrenze, the jurisdiction border of the Viennese magistrate established in the Late Middle Ages that stretched far north into the floodplain (#CITATION_TAG 1986). It was marked with numerous stone boundary markers, the oldest going back to the 1540s, from which we know when they were set up (Opll et al. 1984). We assumed solid floodplain terrain at their locations at least for the time of their erection. Even if property borders were not directly marked in the maps they can be used as landmarks, because they are often indicated by different forms of land use.	0	One example is the Prater Main Avenue in the imperial hunting ground Prater. The originally 5.6 km long boulevard was constructed as a straight line in 1537/38 on a large island close to the city (Fig. 6). It functioned as a main landmark in historical riverscape cartography (Slezak 1980). Borders of land properties, hunting grounds or administrative borders also proved to be very useful; in particular, the so-called Burgfriedsgrenze, the jurisdiction border of the Viennese magistrate established in the Late Middle Ages that stretched far north into the floodplain (#CITATION_TAG 1986). It was marked with numerous stone boundary markers, the oldest going back to the 1540s, from which we know when they were set up (Opll et al. 1984). We assumed solid floodplain terrain at their locations at least for the time of their erection. Even if property borders were not directly marked in the maps they can be used as landmarks, because they are often indicated by different forms of land use.	e
CC82	"Acute toxicity and adverse effects were reported using the NCI common toxicity criteria v2.0 and RTOG/EORTC recommendations for classifying late toxic effects of radiotherapy [#CITATION_TAG,25]. Perioperative complications were graded by Dindo""s classification [26]."	5	"Acute toxicity and adverse effects were reported using the NCI common toxicity criteria v2.0 and RTOG/EORTC recommendations for classifying late toxic effects of radiotherapy [#CITATION_TAG,25]. Perioperative complications were graded by Dindo""s classification [26]."	A
CC152	"Willpower. The concept of willpower was clearly important to smokers and often used by researchers to account for smokers"" success or failure, but rarely examined or unpacked. Willpower was reported to be a method of quitting, a strategy to counteract cravings or urges (much as NRT or counselling is regarded as a method of quitting or a way of dealing with an urge to smoke) [#CITATION_TAG,32,35] or a personal quality or trait fundamental to quitting success. [28,32,33,36] For example, although Ogden and Hill (2008) classified their participants according to whether they had ""stopped smoking through willpower or a smoking course they gave no definition or explanation of what willpower was. Similarly, Thompson (1995) reported many participants used ""sheer willpower to overcome the strong urges to smoke and Abdullah and Ho (2006) reported relapsed smokers cited ""willpower and determination"" as key factors for quitting success, but did not elaborate on what was meant by willpower. Stewart""s 1999 sociological study of smokers who quit unassisted [32] attempted to understand willpower from the smokers"" perspective, yet despite directly questioning smokers about willpower, Stewart could find no agreement among smokers as to what willpower was. In summing up, Stewart concluded: ""it is difficult to connect a successful cessation attempt with the use of willpower without creating a tautology: one is successful if one has willpower, and one has willpower if one is successful,"" capturing what is arguably still an issue in contemporary smoking cessation research."	4	"Willpower. The concept of willpower was clearly important to smokers and often used by researchers to account for smokers"" success or failure, but rarely examined or unpacked. Willpower was reported to be a method of quitting, a strategy to counteract cravings or urges (much as NRT or counselling is regarded as a method of quitting or a way of dealing with an urge to smoke) [#CITATION_TAG,32,35] or a personal quality or trait fundamental to quitting success. [28,32,33,36] For example, although Ogden and Hill (2008) classified their participants according to whether they had ""stopped smoking through willpower or a smoking course they gave no definition or explanation of what willpower was. Similarly, Thompson (1995) reported many participants used ""sheer willpower to overcome the strong urges to smoke and Abdullah and Ho (2006) reported relapsed smokers cited ""willpower and determination"" as key factors for quitting success, but did not elaborate on what was meant by willpower. Stewart""s 1999 sociological study of smokers who quit unassisted [32] attempted to understand willpower from the smokers"" perspective, yet despite directly questioning smokers about willpower, Stewart could find no agreement among smokers as to what willpower was."	l
CC2429	"A main condition for euthanasia in the Benelux laws is ""unbearable"" suffering. This is actually a slightly unfortunate misnomer. Unbearable"" is an objective notion, while it is the subjective assessment by the patient that matters most. The objective measurement of suffering is still in its infancy (Deschepper et al. 2013) and no one is in a position to call someone else""s suffering bearable or not (#CITATION_TAG et al. 2010). The public, most physicians, and the Control and Evaluation Commission of Euthanasia interpret this term in the euthanasia legislation to mean suffering that is no longer tolerated by the patient."	1	"A main condition for euthanasia in the Benelux laws is ""unbearable"" suffering. This is actually a slightly unfortunate misnomer. Unbearable"" is an objective notion, while it is the subjective assessment by the patient that matters most. The objective measurement of suffering is still in its infancy (Deschepper et al. 2013) and no one is in a position to call someone else""s suffering bearable or not (#CITATION_TAG et al. 2010). The public, most physicians, and the Control and Evaluation Commission of Euthanasia interpret this term in the euthanasia legislation to mean suffering that is no longer tolerated by the patient."	 
CC962	"To identify the best Method (or Method by Filtering combination), I have used the procedure of ""multiple comparisons with the best"" (MCB) #CITATION_TAG where, in our case, best is ""smaller"" for all four performance measures. Briefly, MCB procedures compare each Method (or Filtering by Method combination) against the best of the other methods and can return a confidence set, such that methods that are not contained in the confidence set can be rejected as methods that are not the best method [65,66]. An MCB procedure for block designs that uses Wilcoxon signed ranks has been described in [65]. In our case, for each combination of True Graph, Model, sh, S.Size, S.Time and S.Type, each data set constitutes a block. Thus, separately for each measure and for each of the 864 among-data set combinations I have used a method based in [65]. All results reported have a minimal coverage of 0.90. Full details are provided in Additional file 1 section ""Multiple comparisons with the best (MCB)"". The best methods (or method by filtering combinations) for each of the 864 among-data set combinations are shown in Additional file 4. From these, we can then find the frequency of the different confidence sets (or best subsets), for selected combinations of factors as shown, for example, in Table 4."	5	"To identify the best Method (or Method by Filtering combination), I have used the procedure of ""multiple comparisons with the best"" (MCB) #CITATION_TAG where, in our case, best is ""smaller"" for all four performance measures. Briefly, MCB procedures compare each Method (or Filtering by Method combination) against the best of the other methods and can return a confidence set, such that methods that are not contained in the confidence set can be rejected as methods that are not the best method [65,66]. An MCB procedure for block designs that uses Wilcoxon signed ranks has been described in [65]. In our case, for each combination of True Graph, Model, sh, S.Size, S.Time and S.Type, each data set constitutes a block."	T
CC2785	"Recent research proposes that network pictures exist on two levels: narrow or broad (#CITATION_TAG et al., 2010). Henneberg et al. (2006) suggest that network pictures collected from managers can provide an insight into the individual actor""s frame of mind (i.e. narrow network pictures), thereby provide an understanding of what they believe to be relevant and important. Thus, they are defined as managers\"" ""subjective, idiosyncratic sensemaking with regard to the main constituting characteristics of the network in which their company is operating"" (Henneberg et al., 2006, p. 409). Mouzas, Henneberg, and Naudթ (2008) argue that these individual network pictures represent not merely managers"" or companies"" views, but rather the interactions between managers, i.e. it is the clash of different network pictures that guides managerial actions. Interactions therefore cause a shared and inter-subjective understanding of the environment (Daft & Weick, 1984;Weick & Roberts, 1993)."	4	"Recent research proposes that network pictures exist on two levels: narrow or broad (#CITATION_TAG et al., 2010). Henneberg et al. (2006) suggest that network pictures collected from managers can provide an insight into the individual actor""s frame of mind (i.e. narrow network pictures), thereby provide an understanding of what they believe to be relevant and important. Thus, they are defined as managers\"" ""subjective, idiosyncratic sensemaking with regard to the main constituting characteristics of the network in which their company is operating"" (Henneberg et al., 2006, p. 409). Mouzas, Henneberg, and Naudթ (2008) argue that these individual network pictures represent not merely managers"" or companies"" views, but rather the interactions between managers, i.e. it is the clash of different network pictures that guides managerial actions."	R
CC202	Hou et al. (2011) #CITATION_TAG first suggest the idea of using an internal citation count based on the full text of a research paper rather than just the bibliography to determine influence. They demonstrate a positive correlation between the number of times a citation occurs and its overall influence on the citing paper. Zhu et al. [10] combine these earlier approaches and suggest a range of 40 classification features including both semantic and metric features to determine influence. Most recently, Valenzuela et al. (2015) [1] made significant e__�orts to construct a reference set which was publicly released and which this study relies heavily on. They suggest a range of 12 features, many of which show similarity with those of [10].	5	Hou et al. (2011) #CITATION_TAG first suggest the idea of using an internal citation count based on the full text of a research paper rather than just the bibliography to determine influence. They demonstrate a positive correlation between the number of times a citation occurs and its overall influence on the citing paper. Zhu et al. [10] combine these earlier approaches and suggest a range of 40 classification features including both semantic and metric features to determine influence. Most recently, Valenzuela et al. (2015) [1] made significant e__�orts to construct a reference set which was publicly released and which this study relies heavily on.	H
CC1306	"A Decision Tree (DT) algorithm identifies patterns in a dataset as conditions, represented visually as a decision tree (Quinlan, 1986). For example, the following two conditions depict a branch of depth two that capture characteristics of instances in a class ""grade=good"": ""if Conscientiousness > 5.6 and Self-Efficacy > 6.3 then Grade = Good. The size of the tree (rule depth) is configurable, influencing the specificity of the resulting model (Quinlan, 1986). Simpler implementations (e.g., C5.0) limit each branch to value ranges from a single attribute, making this a linear classifier with a further restriction that each condition is an axis-parallel hyperplane (Tan et al., 2006). Less restrictive implementations can incorporate a greater range of patterns (e.g., CART, #CITATION_TAG et al., 1984). Model interpretability makes decision trees a popular choice (Han & Kamber, 2006)."	0	"For example, the following two conditions depict a branch of depth two that capture characteristics of instances in a class ""grade=good"": ""if Conscientiousness > 5.6 and Self-Efficacy > 6.3 then Grade = Good. The size of the tree (rule depth) is configurable, influencing the specificity of the resulting model (Quinlan, 1986). Simpler implementations (e.g., C5.0) limit each branch to value ranges from a single attribute, making this a linear classifier with a further restriction that each condition is an axis-parallel hyperplane (Tan et al., 2006). Less restrictive implementations can incorporate a greater range of patterns (e.g., CART, #CITATION_TAG et al., 1984). Model interpretability makes decision trees a popular choice (Han & Kamber, 2006)."	 
CC672	At the same time, gerontological studies have revealed a gradual reduction in the water content of aging tissue, including brain tissue [13][14]#CITATION_TAG[16], which is equivalent to an increase in the total macromolecular concentration in the aqueous compartments of these tissues.	0	At the same time, gerontological studies have revealed a gradual reduction in the water content of aging tissue, including brain tissue [13][14]#CITATION_TAG[16], which is equivalent to an increase in the total macromolecular concentration in the aqueous compartments of these tissues.	A
CC2023	The other computational fluid dynamical method commonly employed in numerical astrophysics is the standard Eulerian grid based approach in which the fluid is evolved on a discretized mesh (Stone & Norman 1992;Ryu et al. 1993;Norman & Bryan 1999b;Fryxell et al. 2000;Teyssier 2002). The spatial resolution of an Eulerian scheme based on a fixed Cartesian grid is often insufficient however to adequately resolve the large dynamic range frequently encountered in many astrophysical problems, such as galaxy formation. This has motivated the development of adaptative mesh refinement (AMR) methods, in which the spatial resolution of the grid is locally refined according to some selection criterion (Berger & Colella 1989;Kravtsov, Klypin & Khokhlov 1997;#CITATION_TAG 2005). Additionally, the order of the numerical scheme has been improved by adopting the parabolic piecewise method (PPM) of Colella & Woodward (1984), such as in the AMR Eulerian codes ENZO (Norman & Bryan 1999b) and FLASH (Fryxell et al. 2000).	0	The other computational fluid dynamical method commonly employed in numerical astrophysics is the standard Eulerian grid based approach in which the fluid is evolved on a discretized mesh (Stone & Norman 1992;Ryu et al. 1993;Norman & Bryan 1999b;Fryxell et al. 2000;Teyssier 2002). The spatial resolution of an Eulerian scheme based on a fixed Cartesian grid is often insufficient however to adequately resolve the large dynamic range frequently encountered in many astrophysical problems, such as galaxy formation. This has motivated the development of adaptative mesh refinement (AMR) methods, in which the spatial resolution of the grid is locally refined according to some selection criterion (Berger & Colella 1989;Kravtsov, Klypin & Khokhlov 1997;#CITATION_TAG 2005). Additionally, the order of the numerical scheme has been improved by adopting the parabolic piecewise method (PPM) of Colella & Woodward (1984), such as in the AMR Eulerian codes ENZO (Norman & Bryan 1999b) and FLASH (Fryxell et al. 2000).	i
CC55	Networks can be analyzed to look for information about the network as a whole, or to look for individuals of interest, such as which person is most central or most connected in a network. We tended to evaluate our graphs for information about average behavior of nodes as a whole, or at the network level of analysis. For example, modularity is the fraction of the connections within given groups minus the expected such fraction if connections were distributed at random (#CITATION_TAG, 2006). The modularity score given to the networks can be interpreted as describing how well a network, in our case the NACP, can be seen as a collection of sub-networks, or is a score of how well organized the network is. Since the modularity score measures the quality of the partitioning of the graph into smaller communities, the fact that the networks have close to similar scores means that they share a similar internal community structure.	5	Networks can be analyzed to look for information about the network as a whole, or to look for individuals of interest, such as which person is most central or most connected in a network. We tended to evaluate our graphs for information about average behavior of nodes as a whole, or at the network level of analysis. For example, modularity is the fraction of the connections within given groups minus the expected such fraction if connections were distributed at random (#CITATION_TAG, 2006). The modularity score given to the networks can be interpreted as describing how well a network, in our case the NACP, can be seen as a collection of sub-networks, or is a score of how well organized the network is. Since the modularity score measures the quality of the partitioning of the graph into smaller communities, the fact that the networks have close to similar scores means that they share a similar internal community structure.	r
CC439	"If prices increase due to increased demand this has an entirely different implication than if they rise because of reduced supply. Demand growth will lead to increased production, to the extent that producers are able to respond to higher prices. If prices increase due to supply contractions, it could be a reflection of unsolved problems in the production process, a substitution of production away from food to other commodities, or a decline in productivity growth. Different causes of price changes have vastly different implications for food security and welfare both for producers and consumers. The task of identifying the factors behind price changes in food commodity markets is complex. Many of these factors are difficult, if not impossible, to measure appropriately. Thus, the simple question of ""why do prices change? usually requires elaborate investigation. Trostle (2008) identifies several demand and supply factors that simultaneously contributed to a recent increase in food prices. Gilbert (2010) emphasizes the impact of common factors on the general level of agricultural food prices. He argues that investments in index-based agricultural futures markets were a major cause of the broad rise in food prices in [2007][2008]. The fundamental driver of such index investments is the belief that rapid economic growth in China will increase demand for commodities in general. The Westernization of diets in Asian economies (#CITATION_TAG 2007), along with rapid income growth, are also highlighted as important drivers (Enders and Holt 2012). Another driver is the diversion of food crops into the production of biofuels, which is generally considered to influence long-run agricultural price levels (Serra and Zilberman 2013). The recent price booms have also lead to increased interest in the Prebisch-Singer hypothesis of continuous long-term declines in primary commodity prices (Balagtas and Holt 2009;Harvey and Kellard 2010;Enders and Holt 2012). Results yield mixed evidence of the hypothesis"" efficacy: Harvey and Kellard (2010) for instance, find longrun decline in the price of only 11 of 25 major commodities."	4	"Gilbert (2010) emphasizes the impact of common factors on the general level of agricultural food prices. He argues that investments in index-based agricultural futures markets were a major cause of the broad rise in food prices in [2007][2008]. The fundamental driver of such index investments is the belief that rapid economic growth in China will increase demand for commodities in general. The Westernization of diets in Asian economies (#CITATION_TAG 2007), along with rapid income growth, are also highlighted as important drivers (Enders and Holt 2012). Another driver is the diversion of food crops into the production of biofuels, which is generally considered to influence long-run agricultural price levels (Serra and Zilberman 2013). The recent price booms have also lead to increased interest in the Prebisch-Singer hypothesis of continuous long-term declines in primary commodity prices (Balagtas and Holt 2009;Harvey and Kellard 2010;Enders and Holt 2012). Results yield mixed evidence of the hypothesis"" efficacy: Harvey and Kellard (2010) for instance, find longrun decline in the price of only 11 of 25 major commodities."	z
CC2438	"This debate can be informed by empirical data. As in the United Kingdom (Seale 2010) and other countries (B�_low et al. 2012), in Belgium practicing religious palliative care physicians are more critical of euthanasia than others (Broeckaert et al. 2009). Perhaps surprisingly, in a survey of British students there was a significant positive correlation between religious belief and a positive attitude towards euthanasia, a finding suggesting a cultural shift to #CITATION_TAG and Hulbert-Williams (2013). Europe-wide comparative research shows that personal worldview does have some influence, but that the national cultural and legal context counts for much more. In countries where euthanasia is legal (""permissive"" countries), religious physicians are much more favourable towards euthanasia than in countries where it is not, and non-believers in the latter countries tend to report higher levels of opposition (Miccinesi et al. 2005;Cohen et al. 2008). It appears, therefore, that the surrounding culture strongly influences personal views, and perhaps euthanasia legislation changes the culture. In permissive countries the main determinants of physicians"" willingness to perform euthanasia are not physician-but patient-related: The clinical condition of the patients and their wishes are foremost. The predominance of patient interests over physician views is of itself an important ethical stance (Hunt 1994)."	1	"This debate can be informed by empirical data. As in the United Kingdom (Seale 2010) and other countries (B�_low et al. 2012), in Belgium practicing religious palliative care physicians are more critical of euthanasia than others (Broeckaert et al. 2009). Perhaps surprisingly, in a survey of British students there was a significant positive correlation between religious belief and a positive attitude towards euthanasia, a finding suggesting a cultural shift to #CITATION_TAG and Hulbert-Williams (2013). Europe-wide comparative research shows that personal worldview does have some influence, but that the national cultural and legal context counts for much more. In countries where euthanasia is legal (""permissive"" countries), religious physicians are much more favourable towards euthanasia than in countries where it is not, and non-believers in the latter countries tend to report higher levels of opposition (Miccinesi et al. 2005;Cohen et al. 2008). It appears, therefore, that the surrounding culture strongly influences personal views, and perhaps euthanasia legislation changes the culture."	r
CC2391	"Question Four: Why Is There 20 Times More Euthanasia Than Physician-Assisted Suicide? MA: About 2 percent of Belgians (and Dutch) die with euthanasia, where the physician administers the lethal drugs, and less than 0.1 percent by physician-assisted suicide, where the patient ingests the drug provided by the physician (#CITATION_TAG et al. 2009). The latter modest proportion is similar to Oregon""s, where only physicianassisted suicide is legal (Oregon Public Health Division 2013). Why is physician-assisted suicide only marginal in the Benelux countries?"	2	"Question Four: Why Is There 20 Times More Euthanasia Than Physician-Assisted Suicide? MA: About 2 percent of Belgians (and Dutch) die with euthanasia, where the physician administers the lethal drugs, and less than 0.1 percent by physician-assisted suicide, where the patient ingests the drug provided by the physician (#CITATION_TAG et al. 2009). The latter modest proportion is similar to Oregon""s, where only physicianassisted suicide is legal (Oregon Public Health Division 2013). Why is physician-assisted suicide only marginal in the Benelux countries?"	A
CC1422	"One measure of performance on the CRT is a CRT score (Frederick 2005, Paxton Ungar and Greene 2012), which is the number of questions answered correctly. Since there are three questions, 3.0 is the highest possible score. As predicted, having or being a candidate for a PhD in philosophy was positively correlated with CRT score (#CITATION_TAG 1977). After controlling for previous familiarity with the CRT, those who had or were a candidate for a PhD in philosophy had a higher CRT score, on average, by 0.28; F(1, 558) = 15.41, p < 0.001, d = 0.32. Contrary to my predictions, the number of years spent studying philosophy, after accounting for whether or not one had or was a candidate for a PhD in philosophy, was not significantly related to one""s CRT score, but as predicted, teaching philosophy was not statistically related to CRT score."	5	"One measure of performance on the CRT is a CRT score (Frederick 2005, Paxton Ungar and Greene 2012), which is the number of questions answered correctly. Since there are three questions, 3.0 is the highest possible score. As predicted, having or being a candidate for a PhD in philosophy was positively correlated with CRT score (#CITATION_TAG 1977). After controlling for previous familiarity with the CRT, those who had or were a candidate for a PhD in philosophy had a higher CRT score, on average, by 0.28; F(1, 558) = 15.41, p < 0.001, d = 0.32. Contrary to my predictions, the number of years spent studying philosophy, after accounting for whether or not one had or was a candidate for a PhD in philosophy, was not significantly related to one""s CRT score, but as predicted, teaching philosophy was not statistically related to CRT score."	 
CC1537	"Each participant (apart from DV, see below) took part in a semistructured interview (approximately 30-60 min long) using the Autobiographical Memory Interview (AMI, #CITATION_TAG, Wilson, & Baddeley, 1990;Levine, Svoboda, Hay, Winocour, & Moscovitch, 2002;see McKinnon, Miller, Black, Moscovitch, & Levine, 2006;Nestor, Graham, Bozeat, Simons, & Hodges, 2002 for examples of its use with SD patients). In brief, the interviewer began by describing the purpose of the interview (to talk about events from different periods of the participant""s life) and then asked each person to recall specific events that happened on one day and were particularly striking or memorable. They were asked to recall events from four life periods: the teenage years (before 18 years old), between 18 and 30, between 30 and 50, and something that happened in the last year. Participants were encouraged to talk at length about a given event. Following the initial response by the participant, the interviewer attempted to prompt the recall of more specific information (by asking, for example, what people were wearing, what the participant felt at the time and whether there were any specific sensory memories -i.e. colours, sounds, smells and tastes -associated with that memory). Once the participant had produced a sufficient amount of detail or it became clear that he could not, the interview moved on to the next life period. Typical topics were weddings, births, birthdays, holidays and work related events (e.g. first job, redundancies or retirement). Interviews were typically conducted in one session, although two of the patients (BC and PS) had follow up interviews that reassessed information recalled in the first session; these were also transcribed and analysed. The speech sample for patient DV was opportunistically gathered from a conversation at the start of some tests, for which responses are normally recorded; this sample was approximately 8 min long. We include it in this analysis as DV is the most impaired patient (according to tests including the ACE, MMSE, word-to-picture matching and naming -see Table 1), therefore he represents an extreme point on the SD continuum. In addition, he produced several interesting speech errors during this brief conversation."	5	"Each participant (apart from DV, see below) took part in a semistructured interview (approximately 30-60 min long) using the Autobiographical Memory Interview (AMI, #CITATION_TAG, Wilson, & Baddeley, 1990;Levine, Svoboda, Hay, Winocour, & Moscovitch, 2002;see McKinnon, Miller, Black, Moscovitch, & Levine, 2006;Nestor, Graham, Bozeat, Simons, & Hodges, 2002 for examples of its use with SD patients). In brief, the interviewer began by describing the purpose of the interview (to talk about events from different periods of the participant""s life) and then asked each person to recall specific events that happened on one day and were particularly striking or memorable. They were asked to recall events from four life periods: the teenage years (before 18 years old), between 18 and 30, between 30 and 50, and something that happened in the last year. Participants were encouraged to talk at length about a given event."	E
CC166	"From the studies included in this review, it appears that-at least in smokers"" self-understanding-commitment might be more important than motivation as an explanation of successful unassisted cessation. The enthusiastic and explicit talk about being determined, committed, or serious suggests that this concept resonates more with smokers than the concept of motivation. The overlapping and at times contradictory natures of commitment and motivation have been highlighted recently by Balmford and Borland who concluded that it may be possible to quit successfully while ambivalent, as long as the smoker remains committed in the face of ebbs and flows in motivation. [56] Further complicating the relationship, some regard commitment as a component of motivation, [57] operationalizing motivation as, for example, ""determination to quit"" [58] or ""commitment to quit"". [59] The greater research interest in reasons for quitting or pros and cons of quitting (i.e., motivation) as opposed to commitment may be because motivation is simpler to measure, for example by asking people to rate or rank reasons, costs or benefits. From a policy and practice perspective, it may also be easier to draw attention to these reasons, costs and benefits, rather than engage with commitment. For example, mass media campaigns can remind smokers of why they should quit by pointing out the benefits to short-term and long-term health. However this review draws attention to the importance of commitment for sustained quitting, at least from the point of view of smokers and quitters. The UK""s annual Stoptober campaign in which smokers committed to being smoke-free for 28 days indicates that creative approaches to addressing commitment can be successful. [60] The final concept identified, willpower, was described in terms of multiple constructs (a personal quality or trait, a method of quitting, a strategy to counteract cravings or urges), suggesting smokers and researchers may use it as a convenient or shorthand heuristic when talking about or reporting on quit success. Despite this lack of clarity, the word has persisted in the qualitative and quantitative smoking cessation literature. It could be fruitful for future research to further examine the meaning of willpower, and particularly its relationship to other more tightly defined concepts such as self-efficacy, [61] self-regulation#CITATION_TAG and self-determination, [63] from the perspective of both researchers and smokers."	3	"The UK""s annual Stoptober campaign in which smokers committed to being smoke-free for 28 days indicates that creative approaches to addressing commitment can be successful. [60] The final concept identified, willpower, was described in terms of multiple constructs (a personal quality or trait, a method of quitting, a strategy to counteract cravings or urges), suggesting smokers and researchers may use it as a convenient or shorthand heuristic when talking about or reporting on quit success. Despite this lack of clarity, the word has persisted in the qualitative and quantitative smoking cessation literature. It could be fruitful for future research to further examine the meaning of willpower, and particularly its relationship to other more tightly defined concepts such as self-efficacy, [61] self-regulation#CITATION_TAG and self-determination, [63] from the perspective of both researchers and smokers."	 
CC2502	The scale-space representation that is adopted involves an acyclic graph, or tree. Tree geometry has been used for reduced modeling of turbulence intermittency [2,4,5,14,22], in some instances formulated as generalizations of the zero-dimensional shell models [#CITATION_TAG,15] in which the scale space of turbulence is parameterized by wavenumber modulus. In such formulations, the system state consists of real variables representing velocity values that reside at each node of the tree and are time advanced. In the present approach, the system state consists of thermochemical state variables (and optionally velocity; see Sect. 5.1) associated with fluid parcels that reside at tree endpoints (the nodes at the base of the tree, below which there are no further links or nodes). Variables residing at other nodes of the tree are auxiliary variables in the present context. They are used to specify operations performed on the tree structure as time advances. In this regard, the formulation is fundamentally different from previously formulated hierarchical models in which dynamical variables at all levels, representing either filtered quantities or terms in a notional mode decomposition, are time advanced.	0	The scale-space representation that is adopted involves an acyclic graph, or tree. Tree geometry has been used for reduced modeling of turbulence intermittency [2,4,5,14,22], in some instances formulated as generalizations of the zero-dimensional shell models [#CITATION_TAG,15] in which the scale space of turbulence is parameterized by wavenumber modulus. In such formulations, the system state consists of real variables representing velocity values that reside at each node of the tree and are time advanced. In the present approach, the system state consists of thermochemical state variables (and optionally velocity; see Sect. 5.1) associated with fluid parcels that reside at tree endpoints (the nodes at the base of the tree, below which there are no further links or nodes). Variables residing at other nodes of the tree are auxiliary variables in the present context.	r
CC1129	When the individual is in a situation conducive to making progress toward attaining a goal, the response to goal cues takes the form of actions or operant mental acts that advance the goal pursuit. This idea is intuitively obvious, but its elaborations become complicated. Whether a person views a situation as conducive to pursuing a particular goal depends on a decision process that takes into account the anticipated relative gains and losses arising from a particular course of action in comparison with alternative courses of action possible in that situation. An elaborate research area has grown up around this decision process, which one can generally subsume under Expectancy X Value theory in psychology and Expected Utility theory in economics. This theory, together with recent neuroscientific findings that support it, is briefly reviewed elsewhere (#CITATION_TAG and Cox, 2011).	0	This idea is intuitively obvious, but its elaborations become complicated. Whether a person views a situation as conducive to pursuing a particular goal depends on a decision process that takes into account the anticipated relative gains and losses arising from a particular course of action in comparison with alternative courses of action possible in that situation. An elaborate research area has grown up around this decision process, which one can generally subsume under Expectancy X Value theory in psychology and Expected Utility theory in economics. This theory, together with recent neuroscientific findings that support it, is briefly reviewed elsewhere (#CITATION_TAG and Cox, 2011).	 
CC1005	Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1][2][3][4]6,9,#CITATION_TAG,14,16]. As a consequence, the evaluation of such search tools is a very important topic. Indeed, search evaluation is a core element of the Semantic Evaluation At Large Scale (SEALS) EU project 1 , which is aimed at developing a new research infrastructure dedicated to the evaluation of Semantic Web technologies.	0	Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1][2][3][4]6,9,#CITATION_TAG,14,16]. As a consequence, the evaluation of such search tools is a very important topic. Indeed, search evaluation is a core element of the Semantic Evaluation At Large Scale (SEALS) EU project 1 , which is aimed at developing a new research infrastructure dedicated to the evaluation of Semantic Web technologies.	S
CC2839	"This paper suggests, as #CITATION_TAG (2002), Roth (2008), and Rahm and Ash ( 2008) have found, that people whose cultural backgrounds differ from the dominant social group are able to engage in cross-cultural learning, but require some common ground or support to do so. Moments of cross-cultural meaning making have great potential for creating more inclusive science learning opportunities and disrupting the concept of the ""ideal"" visitor that haunted the ISE institutions. It would be valuable to explore how more inclusive informal science learning opportunities could be developed and facilitated in ISE and cross-cultural meaning making could be a useful starting point for future research."	1	"This paper suggests, as #CITATION_TAG (2002), Roth (2008), and Rahm and Ash ( 2008) have found, that people whose cultural backgrounds differ from the dominant social group are able to engage in cross-cultural learning, but require some common ground or support to do so. Moments of cross-cultural meaning making have great potential for creating more inclusive science learning opportunities and disrupting the concept of the ""ideal"" visitor that haunted the ISE institutions. It would be valuable to explore how more inclusive informal science learning opportunities could be developed and facilitated in ISE and cross-cultural meaning making could be a useful starting point for future research."	T
CC2029	Eolian landscapes like actively migrating parabolic dunes on the outer coast of Cape Cod, Massachusetts and within Cape Cod National Seashore (Figure 1) are sensitive indicators of environmental change. Activation of this dune system reflects fundamental sedimentologic controls on sand availability and supply and the persistence of wind speeds that surpass a threshold (>5 m/s) for particle entrainment (Fryberger and Dean, 1979;Pye and Tsoar, 1990, pp. 127-145). The availability of sand for eolian transport is often mediated by the type and extent of vegetation cover on dunes, and may be further controlled by moisture availability, edaphic associations (cf. related to soil processes) and landscape disturbance (e.g., Sala et al., 1988;Mangan et al., 2004;Hugenholtz and Wolfe, 2005;Luna et al., 2011). Cape Cod is a mesic (e.g., moist) environment with mean annual precipitation of 106.5 cm and with mean monthly totals that are relatively consistent between 7 and 11 cm (Provincetown: 1941(Provincetown: -2000NOAA, 2002). Despite these wet conditions parabolic dunes on the outermost coast of Cape Cod are currently active and have average migration rates between 2.1 and 3.4 m/year for the 20th and 21st century (#CITATION_TAG et al., 2008). The avalanche slopes of these actively migrating dunes are burying an adjacent forest of pitch pine and oak; an observation also made by naturalist Henry David Thoreau in 1849 (Thoreau, 2004, p. 160).	4	127-145). The availability of sand for eolian transport is often mediated by the type and extent of vegetation cover on dunes, and may be further controlled by moisture availability, edaphic associations (cf. related to soil processes) and landscape disturbance (e.g., Sala et al., 1988;Mangan et al., 2004;Hugenholtz and Wolfe, 2005;Luna et al., 2011). Cape Cod is a mesic (e.g., moist) environment with mean annual precipitation of 106.5 cm and with mean monthly totals that are relatively consistent between 7 and 11 cm (Provincetown: 1941(Provincetown: -2000NOAA, 2002). Despite these wet conditions parabolic dunes on the outermost coast of Cape Cod are currently active and have average migration rates between 2.1 and 3.4 m/year for the 20th and 21st century (#CITATION_TAG et al., 2008). The avalanche slopes of these actively migrating dunes are burying an adjacent forest of pitch pine and oak; an observation also made by naturalist Henry David Thoreau in 1849 (Thoreau, 2004, p. 160).	t
CC914	Collected plant material was temporarily stored in a dark climate chamber (5��C) and any living or dead material other than the target bryophyte or lichen carefully removed by hand. Mosses were cut to a maximal shoot length of 6 cm from the apex, liverworts were left intact, and for lichens three to four whole thallus lobes were used. Samples were saturated with water and placed in 135 ml glass vials. Right before incubation all samples were acclimated for 36 h outdoors in the experimental common environment, in a partly shaded spot just outside Abisko Research Station. Then the vials were sealed with rubber septa and in half of them 15 ml of headspace were replaced by 15 ml 15 N 2 (98% enriched, Cambridge Isotope Laboratories, Inc., U.S.A.), resulting in headspace enrichments of 10-19% depending on species [see Supplementary Materials S1 for note on calculation of final headspace enrichment]. The other half of the samples, the controls, was treated in the same manner except for the headspace substitution. All vials were placed randomly on three trays, representing statistical blocks (#CITATION_TAG and Keough 2002), which corresponded to the three valleys the samples had been collected from. Incubation took 24 h during which the trays were rotated every 4 h to avoid possible unintended gradients in the common environment. Sunscreens were put up at noon to avoid direct sun illumination of the glass vials, in order to avoid any greenhouse effects (Zielke et al. 2002); this kept PAR at average levels of ca. 75 �_Einsteins m __�2 s __�1 (ranging from 10 to �_Einsteins m __�2 s __�1 ). Additionally, temperature was recorded on Tinytag loggers coupled to probes inside and outside the vials and the data showed no significant difference within either of the incubation rounds (T-Test, for June P=0.90; for July P=0.64). June temperature averages inside and outside the vials were 18.0��C and 18.2��C respectively (13.5-22.7��C range). For July these averages were 10.5��C and 10.3��C (6.9-14.5��C range).	5	Right before incubation all samples were acclimated for 36 h outdoors in the experimental common environment, in a partly shaded spot just outside Abisko Research Station. Then the vials were sealed with rubber septa and in half of them 15 ml of headspace were replaced by 15 ml 15 N 2 (98% enriched, Cambridge Isotope Laboratories, Inc., U.S.A.), resulting in headspace enrichments of 10-19% depending on species [see Supplementary Materials S1 for note on calculation of final headspace enrichment]. The other half of the samples, the controls, was treated in the same manner except for the headspace substitution. All vials were placed randomly on three trays, representing statistical blocks (#CITATION_TAG and Keough 2002), which corresponded to the three valleys the samples had been collected from. Incubation took 24 h during which the trays were rotated every 4 h to avoid possible unintended gradients in the common environment. Sunscreens were put up at noon to avoid direct sun illumination of the glass vials, in order to avoid any greenhouse effects (Zielke et al. 2002); this kept PAR at average levels of ca. 75 �_Einsteins m __�2 s __�1 (ranging from 10 to �_Einsteins m __�2 s __�1 ).	a
CC482	"WR""s comprehension profile showed a dissociation which is the reverse of the predominantly reported pattern of good performance on actives and poor performance on passives, and poses a substantial challenge to conventional explanations for syntactic disorder (Druks & Marshall, 1995. English passives are ""harder"" with regard to a number of variables (Caplan & Waters, 1999;#CITATION_TAG & Grodzinsky, 2006;Druks, 2002;Grodzinsky, 2000;Mauner, Fromkin, & Cornell, 1993) as they contain more words, more functional morphemes, have a non-canonical word order and, in some theories, involve a transformation from canonical order (or ""movement"" of constituents) ."	0	"WR""s comprehension profile showed a dissociation which is the reverse of the predominantly reported pattern of good performance on actives and poor performance on passives, and poses a substantial challenge to conventional explanations for syntactic disorder (Druks & Marshall, 1995. English passives are ""harder"" with regard to a number of variables (Caplan & Waters, 1999;#CITATION_TAG & Grodzinsky, 2006;Druks, 2002;Grodzinsky, 2000;Mauner, Fromkin, & Cornell, 1993) as they contain more words, more functional morphemes, have a non-canonical word order and, in some theories, involve a transformation from canonical order (or ""movement"" of constituents) ."	n
CC500	"Classic race model Miller""s (1982) classic formulation of the race model has been widely used to analyse response times in previous multisensory studies with adults (e.g. Schroger & Widmann, 1998;Hughes et al., 1994) and children (Neil et al., 2006;Barutchu et al., 2009;Barutchu et al., 2010;Brandwein et al., 2011;Barutchu et al., 2011); Fig. 1a. Model predictions for combined-cue condition AV are calculated from cumulative distributions of A and V response times. The model prediction (""Miller\""s bound""; see Supplemental Methods for full details) is given simply by the sum of the cumulative A and V distributions. Importantly, the model does not aim to capture performance, but rather to state the upper limit on the speed gains theoretically possible via a race between channels. To do so, the model assumes (1) negative correlation between channels (see Fig. 1a), an extreme case predicting the greatest RT benefits (see Supplemental Methods), and (2) ""context invariance"" (#CITATION_TAG, 1986): that the system\""s processing of one signal (e.g. A) is the same when A is the only signal present and when a second signal is also present (e.g. AV). If no further assumptions are made, then response times faster than this bound cannot be explained by parallel processing. Times faster than this prediction have traditionally been interpreted as showing evidence for pooling of signals. In the present study, we used this approach to compare AV reaction times with Miller""s bound prediction."	5	"Model predictions for combined-cue condition AV are calculated from cumulative distributions of A and V response times. The model prediction (""Miller\""s bound""; see Supplemental Methods for full details) is given simply by the sum of the cumulative A and V distributions. Importantly, the model does not aim to capture performance, but rather to state the upper limit on the speed gains theoretically possible via a race between channels. To do so, the model assumes (1) negative correlation between channels (see Fig. 1a), an extreme case predicting the greatest RT benefits (see Supplemental Methods), and (2) ""context invariance"" (#CITATION_TAG, 1986): that the system\""s processing of one signal (e.g. A) is the same when A is the only signal present and when a second signal is also present (e.g. AV). If no further assumptions are made, then response times faster than this bound cannot be explained by parallel processing. Times faster than this prediction have traditionally been interpreted as showing evidence for pooling of signals. In the present study, we used this approach to compare AV reaction times with Miller""s bound prediction."	o
CC1797	A second approach has been followed by Bayesian modelers who have sought to identify memory mechanisms that implement the Monte Carlo algorithms of rational process models. This has resulted in recent demonstrations that the importance sampling algorithm can be implemented by exemplar-based memory mechanisms (Abbott, Hamrick, & Griffiths, 2013;#CITATION_TAG & Griffiths, 2009;Shi et al., 2010). These mechanisms approximate sampling from the prior distribution by retrieving memories (hypotheses) according to their degree of similarity to the current context and then weighting them by the likelihood function to obtain an approximation to the posterior distribution.	0	A second approach has been followed by Bayesian modelers who have sought to identify memory mechanisms that implement the Monte Carlo algorithms of rational process models. This has resulted in recent demonstrations that the importance sampling algorithm can be implemented by exemplar-based memory mechanisms (Abbott, Hamrick, & Griffiths, 2013;#CITATION_TAG & Griffiths, 2009;Shi et al., 2010). These mechanisms approximate sampling from the prior distribution by retrieving memories (hypotheses) according to their degree of similarity to the current context and then weighting them by the likelihood function to obtain an approximation to the posterior distribution.	h
CC919	A great deal of variation in nitrogen fixation of cyanobacterial bryophytes has been attributed to abiotic factors; Zielke et al. (2002) reported that in the High Arctic there is a strong dependency of fixation rates on site hydrology, temperature and light intensity. In regions with low precipitation during the growing season, for instance, N fixation is limited either to the period of snowmelt or to sites which stay wet during summer, e.g. peat bogs, fens, or dense moss mats (Zielke et al. 2005). Experimental longterm increase in UV-B radiation, simulating atmospheric ozone depletion, reduces N fixation activity in Arctic mosses, but does not affect it in sub-Arctic ones (Solheim et al. , 2006. Fixation rates also depend on the successional age of boreal forests via differences in environmental moisture, nutrition and possible other factors (DeLuca et al. 2008;Lagerstr�_m et al. 2009;Zackrisson et al. 2004). In contrast to this emphasis on abiotic control over Nfixation, only a few studies have investigated the role of host specificity in the interaction between cyanobacteria and bryophytes (see During and Van Tooren 1990). For mosses the types of association probably range from simply fortuitous, through more or less specific epiphytism, to intracellular colonisation of dead moss cells or hollow hyaline cells (Solheim and Zielke 2002). Even though extensive information is available on cyanobacteria-plant symbioses (including therein liverworts and hornworts) (#CITATION_TAG et al. 2000), there is hardly any information on how mosscyanobacteria associations are formed, what genes are involved if any, and the extent of nitrogen exchange between host and symbiont. At an ecosystem level, a single study seldom combines more than a couple of host species and those are often closely related phylogenetically. This poses problems when estimations of regional biological atmospheric nitrogen fixation are attempted. Even the widely applied technique for measuring N 2 fixation rates by means of acetylene reduction capacity provides idiosyncratic data, unless calibrated for each analysis by means of N 2 labelled gas (DeLuca et al. 2002).	0	Fixation rates also depend on the successional age of boreal forests via differences in environmental moisture, nutrition and possible other factors (DeLuca et al. 2008;Lagerstr�_m et al. 2009;Zackrisson et al. 2004). In contrast to this emphasis on abiotic control over Nfixation, only a few studies have investigated the role of host specificity in the interaction between cyanobacteria and bryophytes (see During and Van Tooren 1990). For mosses the types of association probably range from simply fortuitous, through more or less specific epiphytism, to intracellular colonisation of dead moss cells or hollow hyaline cells (Solheim and Zielke 2002). Even though extensive information is available on cyanobacteria-plant symbioses (including therein liverworts and hornworts) (#CITATION_TAG et al. 2000), there is hardly any information on how mosscyanobacteria associations are formed, what genes are involved if any, and the extent of nitrogen exchange between host and symbiont. At an ecosystem level, a single study seldom combines more than a couple of host species and those are often closely related phylogenetically. This poses problems when estimations of regional biological atmospheric nitrogen fixation are attempted. Even the widely applied technique for measuring N 2 fixation rates by means of acetylene reduction capacity provides idiosyncratic data, unless calibrated for each analysis by means of N 2 labelled gas (DeLuca et al. 2002).	h
CC1903	Pyrroles are not present in newly-formed proteins but occur when lipid oxidation products such as 4,5-Epoxy-2-alkenals, e.g. 4,5(E)-epoxy-2(E)-heptenal (EH) which is produced from oxidation of n __� 3 polyunsaturated fatty acids, react with free amino groups such as lysine residues on proteins [1;2]. Sugars such as glucose can also react nonenzymaticly with free amino groups. This non-enzymic glycosylation is known as the Maillard reaction and is well known in food science. Both the Maillard reaction and lipid peroxidation follow similar reaction pathways, producing carbonyl derivatives which then form advanced glycation end products (AGE) or advanced lipid peroxidation end products (ALE) by means of carbonyl-amine reactions and aldol condensations forming protein crosslinks [3]. Pyrrole cross-links have been identified in long lived proteins such as lens crystallins and skin collagen and implicated in the stiffening of arteries and joints associated with aging [4;5]. Increased (carboxyalkyl) pyrrole immunoreactivity was detected in plasma from patients with renal failure and artherosclerosis compared with healthy volunteers #CITATION_TAG. AGE and ALE increasingly accumulate during aging and in chronic diseases [6], suggesting that detecting pyrroles in proteins should be a good way to develop biomarkers for early stage disease.	0	This non-enzymic glycosylation is known as the Maillard reaction and is well known in food science. Both the Maillard reaction and lipid peroxidation follow similar reaction pathways, producing carbonyl derivatives which then form advanced glycation end products (AGE) or advanced lipid peroxidation end products (ALE) by means of carbonyl-amine reactions and aldol condensations forming protein crosslinks [3]. Pyrrole cross-links have been identified in long lived proteins such as lens crystallins and skin collagen and implicated in the stiffening of arteries and joints associated with aging [4;5]. Increased (carboxyalkyl) pyrrole immunoreactivity was detected in plasma from patients with renal failure and artherosclerosis compared with healthy volunteers #CITATION_TAG. AGE and ALE increasingly accumulate during aging and in chronic diseases [6], suggesting that detecting pyrroles in proteins should be a good way to develop biomarkers for early stage disease.	a
CC1194	"It is instructive to count the number of physical degrees of freedom contained in the system ( 18)- (21). We have ten components of the Einstein metric g ���_ corresponding to the ten functions �_ ij , �_ i and �� in the ADM formulation. Four of these, the lapse and shift, are freely specifiable and do not contain physical information. The constraints impose four further conditions on the remaining functions �_ ij that must be satisfied on each hypersurface �� t and we are left with two gravitational degrees of freedom which correspond to the + and �� GW polarization modes; see e.g. [8]. The two gravitational degrees of freedom are recovered even more elegantly in the characteristic formulation of the Einstein equations developed by Bondi, Sachs and collaborators [9,10]. Here, one chooses at least one coordinate to be null and thus foliates spacetime in terms of light cones. The Einstein equations assume a natural hierarchy of 2 evolution equations, 4 hypersurface equations relating variables inside the hypersurfaces, 3 supplementary and 1 trivial equation; for details see [11] and references therein. Codes based on the characteristic formulation have been applied with great success in the presence of special spacetime symmetries and indeed been the first to model single BH spacetimes with long-term stability [12,#CITATION_TAG]. In spite of the formalism""s appealing properties, however, characteristic codes have as yet not been successfully generalized to BH binaries because the formation of caustics causes a breakdown of the coordinate system. It remains to be seen whether this obstacle can be overcome in future investigations; for a recent study see [14]."	0	"The two gravitational degrees of freedom are recovered even more elegantly in the characteristic formulation of the Einstein equations developed by Bondi, Sachs and collaborators [9,10]. Here, one chooses at least one coordinate to be null and thus foliates spacetime in terms of light cones. The Einstein equations assume a natural hierarchy of 2 evolution equations, 4 hypersurface equations relating variables inside the hypersurfaces, 3 supplementary and 1 trivial equation; for details see [11] and references therein. Codes based on the characteristic formulation have been applied with great success in the presence of special spacetime symmetries and indeed been the first to model single BH spacetimes with long-term stability [12,#CITATION_TAG]. In spite of the formalism""s appealing properties, however, characteristic codes have as yet not been successfully generalized to BH binaries because the formation of caustics causes a breakdown of the coordinate system. It remains to be seen whether this obstacle can be overcome in future investigations; for a recent study see [14]."	a
CC2962	"Speech itself can be regarded as a gestural system, comprising movements of the lips, the larynx, the velum, and the blade, body, and root of the tongue (Studdert-Kennedy, 2005). In the course of evolution, then, intentional communication may have evolved from manual gestures, to overt facial gestures, and finally to the largely hidden gestures that comprise speech-although all three forms of gesture remain present in conversation. Speech gestures, although largely contained within the mouth, retain a visible component, as illustrated by the McGurk effect: A syllable (such as da) is dubbed onto a mouth saying another syllable (such as ba), and people tend to ""hear"" what they see rather than what was actually voiced (McGurk and MacDonald, 1976). Other studies show the parts of the brain involved in producing speech are activated when people simply watch silent videos of people speaking (Calvert and Campbell, 2003;#CITATION_TAG et al., 2003). Rhesus monkeys also show dynamic interactions between perceptions of face movements and voicing, mediated by connections between the superior temporal sulcus and auditory cortex (Ghazanfar et al., 2008)."	2	"Speech itself can be regarded as a gestural system, comprising movements of the lips, the larynx, the velum, and the blade, body, and root of the tongue (Studdert-Kennedy, 2005). In the course of evolution, then, intentional communication may have evolved from manual gestures, to overt facial gestures, and finally to the largely hidden gestures that comprise speech-although all three forms of gesture remain present in conversation. Speech gestures, although largely contained within the mouth, retain a visible component, as illustrated by the McGurk effect: A syllable (such as da) is dubbed onto a mouth saying another syllable (such as ba), and people tend to ""hear"" what they see rather than what was actually voiced (McGurk and MacDonald, 1976). Other studies show the parts of the brain involved in producing speech are activated when people simply watch silent videos of people speaking (Calvert and Campbell, 2003;#CITATION_TAG et al., 2003). Rhesus monkeys also show dynamic interactions between perceptions of face movements and voicing, mediated by connections between the superior temporal sulcus and auditory cortex (Ghazanfar et al., 2008)."	e
CC2151	The simplest choice of a tensor model is to consider one which has a tensor with three indices as its only dynamical variable. Then, by identifying the rank-three tensor with the structure constant of an algebra charactering a fuzzy space, the tensor model can be interpreted as theory of a dynamical fuzzy space. Since one can in principle choose the values of the rank-three tensor to construct fuzzy spaces corresponding to any dimensional spaces, the rank-three tensor models can equally treat spaces in general dimensions. This idea has first been presented in Ref. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19]#CITATION_TAG[21][22][23][24][25][26] . The purpose of the present paper is to provide a full treatment of the original incomplete presentation of the idea, and to pursue the algebraic description of the tensor models. In the sequel, it is found that 3-ary algebras [27][28][29] describe the symmetries of the tensor models. 3-ary algebras have been introduced in physics by Nambu , and have recently been widely discussed in the context of M-theory [31][32][33] . This unexpected common appearance of 3-ary algebras suggests the general Tensor models and 3-ary algebras importance of this new way of describing symmetry in the physics of quantum spacetime. This paper is organized as follows. In the following section, the rank-three tensor model is presented. In Section III, the structure of the algebras corresponding to the rank-three tensor models is discussed. In Section IV, the commutative case of the algebras is discussed.	0	Then, by identifying the rank-three tensor with the structure constant of an algebra charactering a fuzzy space, the tensor model can be interpreted as theory of a dynamical fuzzy space. Since one can in principle choose the values of the rank-three tensor to construct fuzzy spaces corresponding to any dimensional spaces, the rank-three tensor models can equally treat spaces in general dimensions. This idea has first been presented in Ref. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19]#CITATION_TAG[21][22][23][24][25][26] . The purpose of the present paper is to provide a full treatment of the original incomplete presentation of the idea, and to pursue the algebraic description of the tensor models. In the sequel, it is found that 3-ary algebras [27][28][29] describe the symmetries of the tensor models. 3-ary algebras have been introduced in physics by Nambu , and have recently been widely discussed in the context of M-theory [31][32][33] This unexpected common appearance of 3-ary algebras suggests the general Tensor models and 3-ary algebras importance of this new way of describing symmetry in the physics of quantum spacetime.	a
CC162	Research into smoking cessation has achieved much. Researchers have identified numerous variables related to smoking cessation and relapse, including heaviness-of-smoking, quitting history, quit intentions, quit attempts, use of assistance, socio-economic status, gender, age, and exposure to mass-reach interventions such as mass media campaigns, price increases or retail regulation. [1] Behavioural scientists have developed a range of health behaviour models and constructs relevant to smoking cessation, such as the theory of planned behaviour, social cognitive theory, the transtheoretical model and the health belief model. [2][3][4]#CITATION_TAG These theories have provided constructs to smoking cessation research such as perceived behavioural control, subjective norms, [2] outcome expectations, self-regulation, [3] decisional balance, [4] perceived benefits, perceived barriers and self-efficacy. [5] The knowledge generated has informed the development of a range of pharmacological and behavioural smoking cessation interventions. Yet, although these interventions are efficacious, [6][7][8] the majority of smokers who quit successfully do so without using them, choosing instead to quit unassisted, that is without pharmacological or professional support. [9,10] Many smokers also appear to quit unplanned as a consequence of serendipitous events, [11] throwing into question the predictive validity of some of these cognitive models.	0	Research into smoking cessation has achieved much. Researchers have identified numerous variables related to smoking cessation and relapse, including heaviness-of-smoking, quitting history, quit intentions, quit attempts, use of assistance, socio-economic status, gender, age, and exposure to mass-reach interventions such as mass media campaigns, price increases or retail regulation. [1] Behavioural scientists have developed a range of health behaviour models and constructs relevant to smoking cessation, such as the theory of planned behaviour, social cognitive theory, the transtheoretical model and the health belief model. [2][3][4]#CITATION_TAG These theories have provided constructs to smoking cessation research such as perceived behavioural control, subjective norms, [2] outcome expectations, self-regulation, [3] decisional balance, [4] perceived benefits, perceived barriers and self-efficacy. [5] The knowledge generated has informed the development of a range of pharmacological and behavioural smoking cessation interventions. Yet, although these interventions are efficacious, [6][7][8] the majority of smokers who quit successfully do so without using them, choosing instead to quit unassisted, that is without pharmacological or professional support. [9,10] Many smokers also appear to quit unplanned as a consequence of serendipitous events, [11] throwing into question the predictive validity of some of these cognitive models.	[
CC2926	"In summary, translating RCA simultaneously as a means of re-establishing institutional legitimacy and as a system of governance has the effect of focussing attention towards the achievement of ""consensus closure and ""control"". These three aspects emerge clearly from our research. This risks transforming RCA from a process of learning and service improvement into a bureaucratised and routine management chore. What was conceived as a ""means to an end"" (producing organisational learning and change) becomes an ""end in itself"" regulated by a number of well-known bureaucratic principles. This stands in conflict with the espoused learning agenda. For example, #CITATION_TAG and Westley (1996) warn that closure and consensus are often enemies of the capacity of organisations to learn from incidents that require them ""to confront the possibility that the story being told is simultaneously a tale of disorder in which the reality of danger masquerades as safety and a tale of order in which the reality masquerades as danger"" (p. 456). Other authors emphasise that in the pursuit of organisational transformation, the process of inquiry is often more important than its results. Learning emerges through ""an interactive inquiry process that balances problem solving actions implemented in a collaborative context with data-driven collaborative analysis or research"" (Reason & Bradbury, 2007, p.12). Pursuing comprehensiveness and excessive rigour may thus hamper the reflexivity upon which the approach relies. At the same time, pursuing centralised control contradicts the bottom up, decentralised and democratic orientation that authors such as Iedema et al. (2008) consider most promising characteristics of RCA."	0	"This risks transforming RCA from a process of learning and service improvement into a bureaucratised and routine management chore. What was conceived as a ""means to an end"" (producing organisational learning and change) becomes an ""end in itself"" regulated by a number of well-known bureaucratic principles. This stands in conflict with the espoused learning agenda. For example, #CITATION_TAG and Westley (1996) warn that closure and consensus are often enemies of the capacity of organisations to learn from incidents that require them ""to confront the possibility that the story being told is simultaneously a tale of disorder in which the reality of danger masquerades as safety and a tale of order in which the reality masquerades as danger"" (p. 456). Other authors emphasise that in the pursuit of organisational transformation, the process of inquiry is often more important than its results. Learning emerges through ""an interactive inquiry process that balances problem solving actions implemented in a collaborative context with data-driven collaborative analysis or research"" (Reason & Bradbury, 2007, p.12)."	x
CC813	The present estimates of number of days needed to achieve a reliability of ICC = 0.80 for overall PA and MVPA, fall in between of previous estimates. While several studies have found that 2-6 days are required [5,#CITATION_TAG,8,6,30], other studies have shown that 12 [4] and 16-23 days are needed [3]. Jerome et al [3] used a wear time criteria of !6 h, which might have led to the need for many days of measurement, in line with the trend across the wear time criteria applied in the present study. Levin et al [4] applied a protocol with measurements during an entire year, which may have introduced greater variation (e.g., seasonal effects) than across subsequent days and weeks. Nevertheless, as ICC is based on variance partitioning and depends on the relative difference in variances (inter-vs. intra-individual sources of variance), the coefficient is context-specific. Thus, it change with the range of observations (similar to the correlation coefficient), despite residual variance and absolute measures of reliability being unaffected [26][27][28]. The current sample was a convenience sample exhibiting a higher activity level (but with approximately similar heterogeneity as evaluated by SDs for the different outcome variables) as compared to population estimates for the corresponding age group [34][35][36]. Thus, it could be argued that our estimates of ICC might be fairly generalizable to the general adult population. Yet, previous studies have concluded that it does not seem to be any relationship between activity level and reliability [6,4]. Based on the heteroscedastic patterns of variability found for overall PA level and MVPA (Fig 1), the current study indicates otherwise. Thus, the sample characteristics (exhibiting a high PA level) might be a likely explanation for the lower reliability found for PA compared to other studies. This means that researchers should determine reliability in their specific samples to make appropriate decisions on valid wear criteria in a given study. Yet, it could be hypothesized from the current study that SED are less influenced by sample characteristics than PA, as a homoscedastic pattern was evident for SED as opposed to the heteroscedastic pattern found for PA.	1	The present estimates of number of days needed to achieve a reliability of ICC = 0.80 for overall PA and MVPA, fall in between of previous estimates. While several studies have found that 2-6 days are required [5,#CITATION_TAG,8,6,30], other studies have shown that 12 [4] and 16-23 days are needed [3]. Jerome et al [3] used a wear time criteria of ! 6 h, which might have led to the need for many days of measurement, in line with the trend across the wear time criteria applied in the present study. Levin et al [4] applied a protocol with measurements during an entire year, which may have introduced greater variation (e.g., seasonal effects) than across subsequent days and weeks.	h
CC1238	It is increasingly evident that significant numbers of college students do not complete the courses in which they enrol, particularly courses with lower entry requirements (ACT, 2012;#CITATION_TAG et al., 2010). Enrolment numbers to tertiary education are increasing, as is the academic and social diversity in the student population (HEA, 2013;OECD, 2013). This adds to the challenge of both identifying students at risk of failing and provisioning the appropriate supports and learning environment to enable all students to perform optimally (Mooney et al., 2010). Tertiary education providers collect an ever-increasing volume of data on their students, particularly activity data from virtual learning environments and other online resources (Drachsler & Greller, 2012). As a result, the application of data analytics to educational settings is emerging as an evolving and growing research discipline (Sachin & Vijay, 2012;Siemens & Baker, 2012), with the primary aim of exploring the value of such data in providing learning professionals, and students, with actionable information that could be used to enhance the learning environment (Siemens, 2012;Chatti et al., 2012). A key challenge for learning analytics is the need to develop capability to explore and identify data that will contribute to improving learning models, including data not currently gathered systematically by tertiary education providers (Buckingham Shum & Deakin Crick, 2012;Tempelaar et al., 2013).	0	It is increasingly evident that significant numbers of college students do not complete the courses in which they enrol, particularly courses with lower entry requirements (ACT, 2012;#CITATION_TAG et al., 2010). Enrolment numbers to tertiary education are increasing, as is the academic and social diversity in the student population (HEA, 2013;OECD, 2013). This adds to the challenge of both identifying students at risk of failing and provisioning the appropriate supports and learning environment to enable all students to perform optimally (Mooney et al., 2010). Tertiary education providers collect an ever-increasing volume of data on their students, particularly activity data from virtual learning environments and other online resources (Drachsler & Greller, 2012).	I
CC2733	Recent research has stressed the importance of network structures in understanding business exchanges (Achrol, 1997;M�_ller & Rajala, 2007). These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;Porter, 1985) or delineated by a shared understanding of different companies (#CITATION_TAG, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980). Business networks have also been characterised in strategic marketing as value-creating systems (Parolini, 1999) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (M�_ller & Svahn, 2006;Normann & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998;Gulati, Nohria, & Zaheer, 2000).	1	Recent research has stressed the importance of network structures in understanding business exchanges (Achrol, 1997;M�_ller & Rajala, 2007). These complex networks have been researched using different approaches. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003;Porter, 1985) or delineated by a shared understanding of different companies (#CITATION_TAG, Stubbart, & Ramaprasad, 2001;Reger & Palmer, 1996). While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983;Guiltinan, 1974;Stern & Reve, 1980). Business networks have also been characterised in strategic marketing as value-creating systems (Parolini, 1999) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (M�_ller & Svahn, 2006;Normann & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998;Gulati, Nohria, & Zaheer, 2000).	r
CC1405	"Having identified a long list of uncertainties and possibilities for a hydrogen transition, we then structured these in terms of an established theoretical framework, the multi-level perspective on sociotechnical transitions. This perspective recognises events occurring at three levels of analysis: the overall background context or ""landscape"" level, in which systems change takes place; the socio-technical system or regime, made up of technologies along with the actors, institutions and networks that together represent the incumbent technological system; and niches, within which innovations are fostered and from which some emerge to replace the regime. In the interests of brevity, the framework is not elaborated here, and the interested reader is referred to Geels [3,#CITATION_TAG]."	5	"Having identified a long list of uncertainties and possibilities for a hydrogen transition, we then structured these in terms of an established theoretical framework, the multi-level perspective on sociotechnical transitions. This perspective recognises events occurring at three levels of analysis: the overall background context or ""landscape"" level, in which systems change takes place; the socio-technical system or regime, made up of technologies along with the actors, institutions and networks that together represent the incumbent technological system; and niches, within which innovations are fostered and from which some emerge to replace the regime. In the interests of brevity, the framework is not elaborated here, and the interested reader is referred to Geels [3,#CITATION_TAG]."	 
CC76	"However, simply writing papers on relevant topics and forming a tighter research network are not sufficient to produce policy-relevant research. The process of connecting science and decision makers must be undertaken. It is not clear from our analysis that such work is being done yet in the NACP (Dilling and Lemos, 2011;Lemos et al., 2002). Previous research has shown that just because research is policy relevant, does not mean that policy makers will use it (#CITATION_TAG et al., 2006). Science has to be done in a way that directly engages stakeholders and involves them in setting the research agenda, working with them iteratively over long periods of time to ensure that the research can be more usable in the end. The NACP abstract database doesn""t really provide evidence one way or another that that is happening."	0	"However, simply writing papers on relevant topics and forming a tighter research network are not sufficient to produce policy-relevant research. The process of connecting science and decision makers must be undertaken. It is not clear from our analysis that such work is being done yet in the NACP (Dilling and Lemos, 2011;Lemos et al., 2002). Previous research has shown that just because research is policy relevant, does not mean that policy makers will use it (#CITATION_TAG et al., 2006). Science has to be done in a way that directly engages stakeholders and involves them in setting the research agenda, working with them iteratively over long periods of time to ensure that the research can be more usable in the end. The NACP abstract database doesn""t really provide evidence one way or another that that is happening."	v
CC1163	"A similar result emerged from a conventional questionnaire study (#CITATION_TAG et al., 2012) that found life satisfaction to be negatively correlated with self-reported patterns of daydreaming about inaccessible people (for example, out of the person""s past or strangers) but positively correlated with daydreaming about people with whom the daydreamer was currently close. That is, the association of daydreaming patterns with affect varied (significantly but weakly) with daydream content. There was no consistent association with daydreaming frequency as such."	1	"A similar result emerged from a conventional questionnaire study (#CITATION_TAG et al., 2012) that found life satisfaction to be negatively correlated with self-reported patterns of daydreaming about inaccessible people (for example, out of the person""s past or strangers) but positively correlated with daydreaming about people with whom the daydreamer was currently close. That is, the association of daydreaming patterns with affect varied (significantly but weakly) with daydream content. There was no consistent association with daydreaming frequency as such."	A
CC2075	"These questions about social memory have received detailed consideration within archaeology over the last 20 years (e.g. Boric��, 2010;Bradley, 2002;Jones, 2007;Whittle et al., 2007b). This article reviews parts of that debate with reference to the particular problems of memory and natural places. One of the most profitable results of these works has been to shift discussions around remembering from a focus on discursive knowledge, where the only imaginable mechanism of transmission in prehistory is the oral poetic tradition, to embodied experience. An important influence here has been Paul Connerton""s 1989 book, How Societies Remember. #CITATION_TAG (1989: 21-25) begins by analysing memory claims in general by dividing them into three broad classes. Firstly, there are personal memory claims, memories which ""take as their object one""s life history "" (1989: 22). Secondly, there are cognitive memory claims which relate to things one has met, learned of or experienced in the past. A typical example would be abstract knowledge, such as the meaning of words. One of the identifying characteristics of this kind of memory claim for Connerton is that you do not need to remember when you learnt something to make use of it. Finally there is habit-memory, the ability to reproduce a physical performance; Connerton (1989: 22) uses the example of remembering how to ride a bicycle. Jones (2007: 7-12) has reviewed different metaphors for how personal memory claims, Connerton""s first class, might work. He rejects metaphors of memory as storage in which the brain is variously conceived as a storehouse, library or encyclopaedia of finite capacity. Jones (2007: 9) regards this vision of memory as problematic for two reasons. Firstly, it relies on a model of memories as objective ""lumps"" of data which are not interpreted in any way by the mind storing them. Secondly, and arising from that characterisation, the authenticity of knowledge is solely derived from the accuracy or otherwise of the mind""s recall of these objective memories. Jones follows Clark (1997) in viewing memory as a process of pattern re-creation which involves the mind, the body and the world. Clark views cognition as something which is created within the interaction of brain, body and world. For Jones, memory is just this kind of knowledge: one that is contextually specific, experiential and embodied. This analysis of personal memory claims is extremely persuasive. The rest of this article is primarily concerned with the slightly different case of the social transmission of memory, but it takes as its starting point Jones"" analysis of memory as a contextually specific, embodied and experiential phenomenon."	4	"This article reviews parts of that debate with reference to the particular problems of memory and natural places. One of the most profitable results of these works has been to shift discussions around remembering from a focus on discursive knowledge, where the only imaginable mechanism of transmission in prehistory is the oral poetic tradition, to embodied experience. An important influence here has been Paul Connerton""s 1989 book, How Societies Remember. #CITATION_TAG (1989: 21-25) begins by analysing memory claims in general by dividing them into three broad classes. Firstly, there are personal memory claims, memories which ""take as their object one""s life history "" (1989: 22). Secondly, there are cognitive memory claims which relate to things one has met, learned of or experienced in the past. A typical example would be abstract knowledge, such as the meaning of words."	A
CC2794	"Instead of having a regime of classical antagonistic capitalism riddled with class struggles the social democracy calls for a cooperative capitalism which is expected to mitigate the potential destabilising force of class struggles and social unrest (communist insurgencies). In ""structuralist""/neo-Kaleckian macro models (see for e.g. Dutt 1984), it is even shown that better income distribution in favour of the working class solves the problem of ""realisation crisis"" of capitalism (pinpointed by Karl Marx) as it increases effective demand and profitability of production thereby promoting investment and growth. These models, however, over-emphasised the forces of ""realisation ""and overlooked the ""profit squeeze"" force (also pinpointed by Karl Marx) -better income distribution and higher real wages reduce profitability and dampen investment and growth (see Bhaduri and Margin, 1990and #CITATION_TAG 1992, 1993."	5	"Instead of having a regime of classical antagonistic capitalism riddled with class struggles the social democracy calls for a cooperative capitalism which is expected to mitigate the potential destabilising force of class struggles and social unrest (communist insurgencies). In ""structuralist""/neo-Kaleckian macro models (see for e.g. Dutt 1984), it is even shown that better income distribution in favour of the working class solves the problem of ""realisation crisis"" of capitalism (pinpointed by Karl Marx) as it increases effective demand and profitability of production thereby promoting investment and growth. These models, however, over-emphasised the forces of ""realisation ""and overlooked the ""profit squeeze"" force (also pinpointed by Karl Marx) -better income distribution and higher real wages reduce profitability and dampen investment and growth (see Bhaduri and Margin, 1990and #CITATION_TAG 1992, 1993."	e
CC1488	The principle of local support assumes that local people who are dissatis�_ ed with conservation because of the costs and constraints it imposes on them will resist and this will cause conservation efforts to fail. Local people may become dissatis�_ ed with protected areas because they displace them from their homes, restrict their livelihoods by limiting access to natural resources, fail to deliver promised bene�_ ts, and other reasons (West and Brockington 2006). They may chose to resist these costs through formal political opposition such as legal challenges, lobbying, and protest marches (e.g., Sullivan 2003), but more frequently through more subtle, indirect protests such as non-cooperation and sabotage (#CITATION_TAG 2007). In short, the principle argues that dissatis�_ ed local people have the power to make protected areas fail. Failure is rarely de�_ ned but is implied as an inability to protect biodiversity (particularly emblematic species) within a protected area, or the weakening or collapse of a protected area as an institution.	0	The principle of local support assumes that local people who are dissatis�_ ed with conservation because of the costs and constraints it imposes on them will resist and this will cause conservation efforts to fail. Local people may become dissatis�_ ed with protected areas because they displace them from their homes, restrict their livelihoods by limiting access to natural resources, fail to deliver promised bene�_ ts, and other reasons (West and Brockington 2006). They may chose to resist these costs through formal political opposition such as legal challenges, lobbying, and protest marches (e.g., Sullivan 2003), but more frequently through more subtle, indirect protests such as non-cooperation and sabotage (#CITATION_TAG 2007). In short, the principle argues that dissatis�_ ed local people have the power to make protected areas fail. Failure is rarely de�_ ned but is implied as an inability to protect biodiversity (particularly emblematic species) within a protected area, or the weakening or collapse of a protected area as an institution.	e
CC2703	The greedy randomized adaptive search procedure (GRASP for short) is a multi-start heuristic technique consisting of constructive and a local search phases to tackle hard combinatorial optimization problems (see #CITATION_TAG and Ribeiro (2010)). In the first phase of GRASP, known as the construction phase, a feasible initial solution is built one at a time. The construction of these feasible solutions is based on the creation of a restricted candidate list (RCL) made up of good attributes including those of the best solution. The choice of this list is a crucial issue in GRASP and can be based on two aspects. It may be size-based, where the best |RCL| elements are selected. Alternatively, it may be attributebased: those candidates whose solution quality is better than a certain threshold ( ) = K L B n L B max 100, 5 , 3	0	The greedy randomized adaptive search procedure (GRASP for short) is a multi-start heuristic technique consisting of constructive and a local search phases to tackle hard combinatorial optimization problems (see #CITATION_TAG and Ribeiro (2010)). In the first phase of GRASP, known as the construction phase, a feasible initial solution is built one at a time. The construction of these feasible solutions is based on the creation of a restricted candidate list (RCL) made up of good attributes including those of the best solution. The choice of this list is a crucial issue in GRASP and can be based on two aspects.	T
CC1398	The literature indicates that uptake can be rapid where the economics of alternative fuels are attractive, typically as a result of fuel or vehicle subsidies, leading to good payback times #CITATION_TAG. However, the cost of driving on natural gas is substantially below that of gasoline in many European countries, but this has not driven a transition in vehicle technology [22), perhaps because of the perceived inferiority of CNG vehicles or because of the lack of infrastructure.	0	The literature indicates that uptake can be rapid where the economics of alternative fuels are attractive, typically as a result of fuel or vehicle subsidies, leading to good payback times #CITATION_TAG. However, the cost of driving on natural gas is substantially below that of gasoline in many European countries, but this has not driven a transition in vehicle technology [22), perhaps because of the perceived inferiority of CNG vehicles or because of the lack of infrastructure.	T
CC2025	"Cross-matches of the metacatalogue entries have been made with the Reid & Parker planetary-nebula survey (Reid & Parker 2006), and the Sabogal et al. (2005) catalogue of Be-star candidates (proposed on the grounds of characteristic photometric variability); this helped to resolve identification ambiguities in a few cases (particularly in crowded fields). For completeness, matches have also been made with the Henry Draper Extension (HDE; Cannon 1936), the Sanduleak catalogue (Sk; Sanduleak 1970), the Radcliffe luminous-star catalogue (RMC; Feast et al. 1960), and the VLT-Flames Tarantula Survey (VFTS; Evans et al. 2011). Results are included in Table A2. #CITATION_TAG (2010) has compiled a related metacatalogue of LMC stars, of all types. His effort differs from that reported here both in its more ambitious scale (4011 entries), and in that he appears to rely on the co-ordinates published in primary sources as the main criterion for identifying catalogue matches (leaving the question of the actual stellar identifications unresolved in many cases, although Skiff has since provided modern identifications, and precise astrometry, for most of the targets). The current metacatalogue has been compared with Duflot""s; results are included in Table A2, and discrepancies discussed in Table A3. In general, the agreement in matchings of primary catalogues is remark-In addition, Massey et al. (2000) newly identify Sk __�69 _�� 194 (L63 289) as a WR star. ably good; in only three cases does Duflot appear to assign physically different stars to a single identifier (see Table A3 entries for LMCe 951, 1282, and 1285), while overlooking 10 matches identified here, and omitting two stars (AL-29 and BE74-527). Reassuringly (and surprisingly!), no revisions to the composition of the emission-line metacatalogue were required as a result of the comparison with Duflot (2010), encouraging a view that probably no more than a handful of straighforwardly identifiable errors remain."	1	"Cross-matches of the metacatalogue entries have been made with the Reid & Parker planetary-nebula survey (Reid & Parker 2006), and the Sabogal et al. (2005) catalogue of Be-star candidates (proposed on the grounds of characteristic photometric variability); this helped to resolve identification ambiguities in a few cases (particularly in crowded fields). For completeness, matches have also been made with the Henry Draper Extension (HDE; Cannon 1936), the Sanduleak catalogue (Sk; Sanduleak 1970), the Radcliffe luminous-star catalogue (RMC; Feast et al. 1960), and the VLT-Flames Tarantula Survey (VFTS; Evans et al. 2011). Results are included in Table A2. #CITATION_TAG (2010) has compiled a related metacatalogue of LMC stars, of all types. His effort differs from that reported here both in its more ambitious scale (4011 entries), and in that he appears to rely on the co-ordinates published in primary sources as the main criterion for identifying catalogue matches (leaving the question of the actual stellar identifications unresolved in many cases, although Skiff has since provided modern identifications, and precise astrometry, for most of the targets). The current metacatalogue has been compared with Duflot""s; results are included in Table A2, and discrepancies discussed in Table A3. In general, the agreement in matchings of primary catalogues is remark-In addition, Massey et al. (2000) newly identify Sk __�69 _�� 194 (L63 289) as a WR star."	T
CC1777	In this paper, we have started the study of the critical behavior of generic solutions of the focusing nonlinear Schr�_dinger equation. We have formulated the conjectural analytic description of this behavior in terms of the tritronquͩe solution to the Painlevͩ-I equation restricted to certain lines in the complex plane. We provided analytical as well as numerical evidence supporting our conjecture. In subsequent publications, we plan to further study the main conjecture of the present paper by applying techniques based, first of all, on the Riemann-Hilbert problem method (Kamvissis et al. 2003;Tovbis et al. 2004Tovbis et al. , 2006 and the theory of Whitham equations (see  for the numerical implementation of the Whitham procedure in the analysis of oscillatory behavior of solutions to the KdV equations). The latter will also be applied to the asymptotic description of solutions inside the oscillatory zone. Furthermore, we plan to study the possibility of extending the main conjecture to the critical behavior of solutions to the Hamiltonian perturbations of more general first order quasilinear systems of elliptic type. Certainly the main challenge would be to also include an asymptotic description of critical behavior in the general nonintegrable perturbations (as it was done in Dubrovin (2006) for the case of scalar Hamiltonian equations). There is, however, an important difference between the scalar Hamiltonian equations and the more general case of systems of Hamiltonian evolutionary PDEs of order greater or equal to 2. In the scalar case, any Hamiltonian perturbation remains integrable within the order approximation. Breaking of integrability in higher orders does not change the structure of the leading term of the asymptotics. It turns out that for systems, a generic Hamiltonian perturbation destroys integrability already at the order . This was shown in #CITATION_TAG (2008) for the particular class of perturbations of the second order nonlinear wave equation. The perturbations preserving integrability at the order 2 have been classified in this paper; the critical behavior for these perturbations is expected to be described by the same tritronquͩe solution to the Painlevͩ-I. The case of more general perturbations violating integrability at this order is currently under investigation.	0	In the scalar case, any Hamiltonian perturbation remains integrable within the order approximation. Breaking of integrability in higher orders does not change the structure of the leading term of the asymptotics. It turns out that for systems, a generic Hamiltonian perturbation destroys integrability already at the order This was shown in #CITATION_TAG (2008) for the particular class of perturbations of the second order nonlinear wave equation. The perturbations preserving integrability at the order 2 have been classified in this paper; the critical behavior for these perturbations is expected to be described by the same tritronquͩe solution to the Painlevͩ-I. The case of more general perturbations violating integrability at this order is currently under investigation.	o
CC2288	Recent studies have suggested that macrophage expression of Programmed Death Ligand (PDL)-1 is important in regulating T cell responses to influenza infection #CITATION_TAG. PDL1 is the ligand for the Programmed Cell Death (PD) receptor 1, which is a member of the CD28 family of T cell receptors with CD80 and CD86 being ligands for CD28. In the standard model of T cell receptor (TCR) activation, activation of CD28 provides a necessary co-stimulation to prevent T cell anergy [8]. In contrast, binding of PDL1 to PD1 causes inhibition of TCR-mediated phosphatidylinositol-3-kinase (PI3Kinase) activation leading to inhibition of T cell proliferation and cytokine release [9]. The increased expression and activation of the PD1/PDL1 axis in chronic viral infections such as HIV and Hepatitis C (HCV) can lead to progressive loss of T cell function [10,11]. However, it was only recently that a role for this PD1/PDL1 pathway has been elucidated in the control of immune function in acute infections and increased PDL1 expression in response to pathogens was demonstrated to be crucial for impairment of CD8 cytotoxicity [7] and the development of regulatory T cells [12]. We have recently demonstrated that CD4 cytotoxic T cells play an important role in protecting against severe influenza infection. This work demonstrated an additional role for MHC class II expressing cells and T helper cell responses [13] highlighting the potential importance of macrophage-T cell interactions in the control of influenza infection.	4	Recent studies have suggested that macrophage expression of Programmed Death Ligand (PDL)-1 is important in regulating T cell responses to influenza infection #CITATION_TAG. PDL1 is the ligand for the Programmed Cell Death (PD) receptor 1, which is a member of the CD28 family of T cell receptors with CD80 and CD86 being ligands for CD28. In the standard model of T cell receptor (TCR) activation, activation of CD28 provides a necessary co-stimulation to prevent T cell anergy [8]. In contrast, binding of PDL1 to PD1 causes inhibition of TCR-mediated phosphatidylinositol-3-kinase (PI3Kinase) activation leading to inhibition of T cell proliferation and cytokine release [9].	R
CC2983	"One possibility is that any change in behavioral patterns in our species was the outcome, not of a rewiring of the brain, nor of the ""unimaginable transition"" declared by Tattersall (2012), but was the outcome of a change in the manner of communication. I suggested above that language may have originated in manual gestures, and of course it persists in this form in the signed languages of the deaf. If this scenario is correct, then, it must have switched to the vocal form that we call speech at some point. Some have argued against the gestural theory on the grounds that it must have required an unlikely transition from a visuo-manual format to an auditory-vocal one (e.g., #CITATION_TAG, 2005;MacNeilage, 2012). However, in my view the transition is better viewed not as a switch of modalities, but rather as a switch in gestural format. In a recent analysis of the neural mechanisms of speech articulation, Bouchard et al. (2011) conclude that their findings ""support gestural theories of speech control over alternative acoustic . . . or vocal-tract geometry theories"" (p. 331)."	1	"One possibility is that any change in behavioral patterns in our species was the outcome, not of a rewiring of the brain, nor of the ""unimaginable transition"" declared by Tattersall (2012), but was the outcome of a change in the manner of communication. I suggested above that language may have originated in manual gestures, and of course it persists in this form in the signed languages of the deaf. If this scenario is correct, then, it must have switched to the vocal form that we call speech at some point. Some have argued against the gestural theory on the grounds that it must have required an unlikely transition from a visuo-manual format to an auditory-vocal one (e.g., #CITATION_TAG, 2005;MacNeilage, 2012). However, in my view the transition is better viewed not as a switch of modalities, but rather as a switch in gestural format. In a recent analysis of the neural mechanisms of speech articulation, Bouchard et al. (2011) conclude that their findings ""support gestural theories of speech control over alternative acoustic . . . or vocal-tract geometry theories"" (p. 331)."	e
CC1876	In addition to age, there are other variables that may affect the frequency of reporting AVH. In clinical samples, rates of auditory hallucinations have been suggested to be higher in female patients (Rector & Seeman, 1992;#CITATION_TAG, Dowd & Janicak, 1999), although there are many confounding variables which may account for apparent gender differences (e.g., sampling biases, comorbid depression, etc.). In general non-clinical populations, women have been found to have higher incidence of positive psychotic symptoms (Maric, Krabbendam, Vollebergh, de Graaf & Van Os, 2003), and specifically auditory hallucinations (Shevlin, Murphy, Dorahy & Adamson, 2007;Tien, 1991). However, this difference may be mediated by depression (Tien, 1991). In young adults (age 18-24), a similar tendency for women reporting higher levels of hallucinations has been found (Scott, Welham, Martin et al., 2008). It is not clear exactly when this tendency emerges due to few studies focusing on auditory hallucinations in adolescence. Kelleher et al. (2012b) report a trend for (nonspecified) psychotic symptoms being more prevalent among mid-adolescent boys than girls. By contrast, a slight (albeit non-significant) difference with girls reporting more hallucinations than boys has been reported (Dhossche, Ferdinand, Van der Ende, Hofstra & Verhulst, 2002). Although not specifically concentrating on hallucinations, Spauwen, Krabbendam, Lieb, Wittchen and Van Os (2003) report interaction for psychotic-like experiences (PLE) between sex and age for adolescents and young adults (age range 17-28 years), with higher PLE risk for males in younger age, but no gender difference in older age. In summary, it appears that the studies that specifically concentrate on hallucinations tend to show higher prevalence in women, whereas studies that examine non-specified PLE find higher prevalence in men; however, age should be taken into consideration when examining the gender differences. There are many relevant biological factors that may influence the differential expression of AVH between genders. Hormones, in particular estrogens, have been suggested as a factor which modulates the onset and symptomatology of psychotic disorders (Seeman, 1997). Also, the developmental trajectories of language networks in the brain, implicated in the neural basis of AVH (Catani, Craig, Forkel et al., 2011), differ between adolescent boys and girls (Hirnstein, Westerhausen, Korsnes & Hugdahl, 2013). Childhood trauma has also been shown to increase the risk for having AVH in adult age (Daalman, Diederen, Derks, van Lutterveld, Kahn & Sommer, 2012;Janssen, Krabbendam, Bak et al., 2004) and may also explain the higher rate of AVH in females as this is more often reported by females (Daalman et al., 2012;Janssen et al., 2004;Molnar, Berkman & Buka, 2001).	0	In addition to age, there are other variables that may affect the frequency of reporting AVH. In clinical samples, rates of auditory hallucinations have been suggested to be higher in female patients (Rector & Seeman, 1992;#CITATION_TAG, Dowd & Janicak, 1999), although there are many confounding variables which may account for apparent gender differences (e.g., sampling biases, comorbid depression, etc.). In general non-clinical populations, women have been found to have higher incidence of positive psychotic symptoms (Maric, Krabbendam, Vollebergh, de Graaf & Van Os, 2003), and specifically auditory hallucinations (Shevlin, Murphy, Dorahy & Adamson, 2007;Tien, 1991). However, this difference may be mediated by depression (Tien, 1991). In young adults (age 18-24), a similar tendency for women reporting higher levels of hallucinations has been found (Scott, Welham, Martin et al., 2008).	n
CC1834	Consistency of choice and valuation (choice spread independent of whether the choice was made before or after revaluation) was higher when stimuli were of positive valence. This choice consistency is not merely revaluation caused by the choice but also includes conforming the choice to fluctuations in valuation. The fact that this effect was stronger for positive stimuli shows a choice bias that is sensitive to stimulus valence. Additionally, the magnitude of the choice-induced preference change was not modulated by action or valence per se, but by their interaction. This effect was driven by changes across all conditions, including the control condition. Schonberg at al. [14] showed that subsequent choice is biased after as little as eight repetitions of Go responses that coincided with item presentation. In this study we show that when action is integrated into the choice both choice and valuation are affected. Note that action and inaction choice trials happen once per item and merely differ by a single keypress obviating any learning effect per se. A possible explanation for the effects that valence and action have on choice and valuation involves a Pavlovian coupling of response tendencies (e.g. action/inaction) with the valence of the outcome (e.g. choosing among overall positive or overall negative outcomes) [15,#CITATION_TAG]. This is usually seen to arise out of a consequence that in natural environments action and reward tend to be closely aligned, i.e. approach for reward. Furthermore, a recent study shows that devaluation of stimuli can be induced by stopping actions [23]. The present study shows how this intrinsic coupling of action and valence affects forced choices among similarly valued rewards and the dynamics of revaluation. In this regard the findings add to an emerging picture that action expression in itself is relevant for valuation and behavior [16,17,[24][25][26]. A similar account could also explain the increased consistency of choice and valuation for items of positive valence. The fact that dynamic of revaluation is sensitive to stimulus valence favours a Pavlovian congruence account over more non-specific biases, e.g. attentional focus due to action. However, the factors of action and valence also modulate the spread of value due to choice in the Session 2 Choice condition (see Fig. 2a). Effects in the Session 1 Choice condition could be attributed to a change in revaluation caused by active or inactive choice. Effects in the Session 2 Choice condition could be attributed to a drift in ratings between session that interacts with active and inaction choice tendencies (for example the relatively more positive value change in Session 2 Choice Go Unchosen may be due to the relatively high values of both options biasing the choice towards action). Therefore, the precise effects of action and valence on the dynamics of revaluation, i.e. the interaction of choice biasing valuation and vice versa, remain to be established. A disposition towards viewing the outcomes of active choices more favourably post choice may provide a basis for cognitive biases, e.g. the aforementioned asymmetry in regret over committed or omitted action [27]. A role for action rather than passive inaction may also underpin the omission bias in moral judgements. Here omitting an action that causes harm is seen as more favourable than committing an action with the same outcome. One possibility is this is related to an increased sense of causality and personal responsibility associated with overt actions [28]. Therefore, the effect of action on dynamics of choice-induced preference change is also consistent with accounts of preference change focusing on self-perception (see [13,29] for discussions).	0	Schonberg at al. [14] showed that subsequent choice is biased after as little as eight repetitions of Go responses that coincided with item presentation. In this study we show that when action is integrated into the choice both choice and valuation are affected. Note that action and inaction choice trials happen once per item and merely differ by a single keypress obviating any learning effect per se. A possible explanation for the effects that valence and action have on choice and valuation involves a Pavlovian coupling of response tendencies (e.g. action/inaction) with the valence of the outcome (e.g. choosing among overall positive or overall negative outcomes) [15,#CITATION_TAG]. This is usually seen to arise out of a consequence that in natural environments action and reward tend to be closely aligned, i.e. approach for reward. Furthermore, a recent study shows that devaluation of stimuli can be induced by stopping actions [23]. The present study shows how this intrinsic coupling of action and valence affects forced choices among similarly valued rewards and the dynamics of revaluation.	l
CC1909	"Albumin from human serum (essentially fatty acid free) (HSA), human ApoA1 and serum from human male AB plasma were obtained from Sigma-Aldrich (UK) and were treated with different oxidising agents to produce pyrroles. Solutions of 1 mg/ml HSA, 1 mg/ml Apo A1 and a 1:1 dilution of human serum in 0.9 M NaCl, 50 mM sodium phosphate, pH 7.2 (PBS) were incubated in the presence of different concentrations of 4,5(E)-epoxy-2(E)-heptenal, (EH) as described previously for bovine serum albumin #CITATION_TAG. EH (0.25 ��l), synthesised as described by Zamora & Hidalgo [12;13], was added to 0.5 ml of PBS and incubated with shaking for 10 min at 45 ��C, then spun and portions of the supernatant were added to protein and serum samples. Unless otherwise stated, EH treatment comprised adding 10 ��l of the supernatant per ml of sample and incubating at 37 ��C for 16 h. HSA and human serum were oxidised in a FeSO 4 (200mg/L) and H 2 O 2 (100mg/L) mixture for 30 min at room temperature. To produce non-enzymic glycation, protein and serum samples were incubated in the presence of 10 mM glucose or 10 mM ribose for 7 d at 37 ��C. Control samples of HSA and serum were incubated under identical conditions where the oxidising reagents were replaced with PBS. The protein and serum samples were passed through Hi Trap desalting columns (GE Biosciences, UK) and concentrated using Amicon Centricon concentrators (Millipore, UK). Protein concentrations were determined using the Pierce 660 protein assay (Thermo Fischer, UK) following the manufacturer""s recommended protocol."	0	Albumin from human serum (essentially fatty acid free) (HSA), human ApoA1 and serum from human male AB plasma were obtained from Sigma-Aldrich (UK) and were treated with different oxidising agents to produce pyrroles. Solutions of 1 mg/ml HSA, 1 mg/ml Apo A1 and a 1:1 dilution of human serum in 0.9 M NaCl, 50 mM sodium phosphate, pH 7.2 (PBS) were incubated in the presence of different concentrations of 4,5(E)-epoxy-2(E)-heptenal, (EH) as described previously for bovine serum albumin #CITATION_TAG. EH (0.25 ��l), synthesised as described by Zamora & Hidalgo [12;13], was added to 0.5 ml of PBS and incubated with shaking for 10 min at 45 ��C, then spun and portions of the supernatant were added to protein and serum samples. Unless otherwise stated, EH treatment comprised adding 10 ��l of the supernatant per ml of sample and incubating at 37 ��C for 16 h. HSA and human serum were oxidised in a FeSO 4 (200mg/L) and H 2 O 2 (100mg/L) mixture for 30 min at room temperature. To produce non-enzymic glycation, protein and serum samples were incubated in the presence of 10 mM glucose or 10 mM ribose for 7 d at 37 ��C.	o
CC2705	A Constructive Method and a Guided Hybrid GRASP for the Capacitated Multi-source Weber Problem in the Presence of Fixed Cost Secondly, our heuristics are designed to operate in a multi-start fashion; it is unlikely that all runs would be trapped by false minima. 3. The procedure would be computationally expensive, requiring the CMSWP to be solved several times. Hence, once we find the initial solution for M = LB, we construct subsequent solutions by adding one facility at a time to existing solutions using an efficient implementation of the ADD heuristic originally proposed by #CITATION_TAG and Hamburger (1963). This is much faster than restarting some constructive algorithm from scratch for every value of M.	0	A Constructive Method and a Guided Hybrid GRASP for the Capacitated Multi-source Weber Problem in the Presence of Fixed Cost Secondly, our heuristics are designed to operate in a multi-start fashion; it is unlikely that all runs would be trapped by false minima. 3. The procedure would be computationally expensive, requiring the CMSWP to be solved several times. Hence, once we find the initial solution for M = LB, we construct subsequent solutions by adding one facility at a time to existing solutions using an efficient implementation of the ADD heuristic originally proposed by #CITATION_TAG and Hamburger (1963). This is much faster than restarting some constructive algorithm from scratch for every value of M.	c
CC404	"Given the potential competition between carpooling and public transport, the promotion of carpooling in public transport-rich areas can be a counterproductive mobility management strategy. Therefore, we follow the line of reasoning developed by Wang (2011) who states that governments should remove unnecessary barriers to carpooling, but that excessive subsidies to carpooling are detrimental to collective welfare since bicycles and public transport produce less emissions and use space more efficiently. Unwanted barriers to carpooling are taxation and insurance issues like the uncertainty of being insured while making a detour to pick up a passenger or when using a company car. Innocent"" carpool incentives are on-line ride-matching services, preferential parking and guaranteed ride home services (preferably also applicable to public transport). In contrast, allowances or free parking for carpoolers in areas with high parking costs is oversubsidising since public transport is most of the time a viable alternative in these areas. For the same reason, we do not advise the introduction of carpool lanes in Belgian cities. Analogously, carpool parkings and park and ride facilities should be carefully planned since they often generate additional traffic and encourage car-oriented land use development outside urban areas (Meek et al., 2008;#CITATION_TAG, 2000). If carpooling is heavily promoted at a workplace due to a lack of public transport, one should always check whether this is caused by bad land use planning or poorly organised public transport. Workplaces can be located outside agglomerations for safety and environmental reasons and carpooling might be the most efficient way to reduce levels of SOV driving in such cases. However, the clustering of businesses in e.g. chemical industrial parks (Reniers et al., 2010) creates the opportunity to invest in bus services as can be observed in the Antwerp port area where large chemical companies have established an extensive network of bus routes. When scale effects are absent, carpooling might be preferred over collective transport. Also for companies with particular characteristics, there are few alternatives to carpooling to reduce levels of SOV commuting. A noticeable example is the construction sector where the changing location of work makes of ridesharing the most rational way of travelling. Note that in this case it is hard to distinguish between carpooling/ridesharing and transport organised by the employer (Meersman et al., 1998). The high levels of carpooling in construction, manufacturing and transport (Table 3, Fig. 4) indicate that in these sectors, carpooling has the highest potential to reduce the amount of SOV commuters. As discussed earlier, the promotion of carpooling must, however, not result in increased urban sprawl or lower levels of public transport or bicycle use."	0	"Innocent"" carpool incentives are on-line ride-matching services, preferential parking and guaranteed ride home services (preferably also applicable to public transport). In contrast, allowances or free parking for carpoolers in areas with high parking costs is oversubsidising since public transport is most of the time a viable alternative in these areas. For the same reason, we do not advise the introduction of carpool lanes in Belgian cities. Analogously, carpool parkings and park and ride facilities should be carefully planned since they often generate additional traffic and encourage car-oriented land use development outside urban areas (Meek et al., 2008;#CITATION_TAG, 2000). If carpooling is heavily promoted at a workplace due to a lack of public transport, one should always check whether this is caused by bad land use planning or poorly organised public transport. Workplaces can be located outside agglomerations for safety and environmental reasons and carpooling might be the most efficient way to reduce levels of SOV driving in such cases. However, the clustering of businesses in e.g. chemical industrial parks (Reniers et al., 2010) creates the opportunity to invest in bus services as can be observed in the Antwerp port area where large chemical companies have established an extensive network of bus routes."	o
CC232	We observed a small difference in the distribution of cases across the socioeconomic quintiles, with a greater proportion of both incident and prevalent cases in the upper quintiles and fewer in the lower. Similar observations have been made in the past [30][31][32][33]#CITATION_TAG, although others have reported either no relationship or a negative association with SES #CITATION_TAG.	1	We observed a small difference in the distribution of cases across the socioeconomic quintiles, with a greater proportion of both incident and prevalent cases in the upper quintiles and fewer in the lower. Similar observations have been made in the past [30][31][32][33]#CITATION_TAG, although others have reported either no relationship or a negative association with SES #CITATION_TAG.	i
CC1334	"Statistical models have dominated data analysis in the social sciences, including educational psychology (#CITATION_TAG et al., 2009;Freedman, 1987;Herzog, 2006). For example, the studies cited in section 2 primarily used correlation (78% of the studies) and regression (54% of the studies), with some papers citing path analysis results (14%) and structural equation models (11%). Statistical modelling has a sound theoretical basis, allowing verifiable conclusions to be drawn from model coefficients; therefore, statistical models have made, and will continue to make, a valuable contribution to the understanding of learners and the learning process. However, such models are based on assumptions, including assumptions of normality, independency, linear additively, and constant variance (Nisbet et al., 2009). It is evident from current knowledge of the factors influencing academic performance, that such factors are interdependent (Prinsloo et al., 2012). While each factor measures unique attributes, overlaps occur in the constructs being measured. In addition, there is evidence to suggest variance is not constant for all attributes. For example, De Feyter et al. (2012) found that low levels of self-efficacy had a positive, direct effect on academic performance for neurotic students only, and for stable students, average or higher levels of self-efficacy had a direct effect on academic performance. In addition, Vancouver and Kendall (2006) found evidence that high levels of self-efficacy can lead to overconfidence regarding exam preparedness, which in turn can have a negative impact on academic performance. Similarly, Poropat (2009) cites evidence of non-linear relationships between factors of personality and academic performance, including conscientiousness and openness. Duff et al. (2004) observed that because academic performance is itself a complex measure, calculated as an aggregate of a variety of assessment types, this weakens the result of correlation analysis with other learning dimensions. While recognizing the continuing importance of statistical models, Freedman (1987) and Breiman (2001) argued that alternative-modelling approaches should be considered when dimensionality is high, and relationships are complex such as in the social sciences. Cox, in a response to Breiman""s paper, notes the importance of the probabilistic base of standard statistical modelling, but agrees with Breiman that in some circumstances, an empirical approach is better (Breiman, 2001, p. 18). It is therefore pertinent to ask if data mining""s empirical modelling approach can add value to psychometric data analysis, in particular their relevance to models of academic achievement."	0	Statistical models have dominated data analysis in the social sciences, including educational psychology (#CITATION_TAG et al., 2009;Freedman, 1987;Herzog, 2006). For example, the studies cited in section 2 primarily used correlation (78% of the studies) and regression (54% of the studies), with some papers citing path analysis results (14%) and structural equation models (11%). Statistical modelling has a sound theoretical basis, allowing verifiable conclusions to be drawn from model coefficients; therefore, statistical models have made, and will continue to make, a valuable contribution to the understanding of learners and the learning process. However, such models are based on assumptions, including assumptions of normality, independency, linear additively, and constant variance (Nisbet et al., 2009).	S
CC2332	The ability of human listeners to adapt to distortions of speech is of interest both in relation to the effectiveness of auditory prostheses and in more basic investigations of speech perception. This ability also raises the question of how such adaptation can be facilitated through training. Much work has centered on noise or tone-vocoding, which has become popular as a technique for manipulating the level of spectral detail, and because of its similarity to the processing in cochlear implants. Listeners can adapt rapidly to low spectral resolution vocoded speech (e.g., Davis et al., 2005). However, a combination of vocoding with spectral shifting can be more challenging. When vocoded speech is spectrally shifted upward to simulate relatively shallow CI electrode insertions, shifts in excess of 3 mm of basilar membrane distance have large acute effects on speech perception (Dorman et al., 1997;Shannon et al., 1998). These effects can be markedly reduced with training, but this requires several hours, substantially longer than for vocoding alone (Faulkner et al., 2006;#CITATION_TAG and Galvin, 2003;Nogaki et al., 2007;Rosen et al., 1999). Cochlear implant listeners show comparable adaptation to changes of frequency mapping (Dorman and Ketten, 2003;Fu et al., 2002), and several studies indicate that explicit speech-based training can facilitate this Fu and Galvin, 2008).	0	Listeners can adapt rapidly to low spectral resolution vocoded speech (e.g., Davis et al., 2005). However, a combination of vocoding with spectral shifting can be more challenging. When vocoded speech is spectrally shifted upward to simulate relatively shallow CI electrode insertions, shifts in excess of 3 mm of basilar membrane distance have large acute effects on speech perception (Dorman et al., 1997;Shannon et al., 1998). These effects can be markedly reduced with training, but this requires several hours, substantially longer than for vocoding alone (Faulkner et al., 2006;#CITATION_TAG and Galvin, 2003;Nogaki et al., 2007;Rosen et al., 1999). Cochlear implant listeners show comparable adaptation to changes of frequency mapping (Dorman and Ketten, 2003;Fu et al., 2002), and several studies indicate that explicit speech-based training can facilitate this Fu and Galvin, 2008).	e
CC1042	The difficulty is that this way of conceiving cognition is increasingly out of step with developments in cognitive science. The field has changed significantly in the last two decades. Commitments from earlier years that have recently been revised (and in some cases overturned) include some of those that particularly inform proposals from cognitive archeology. Where cognitive science once emphasized factors of modular decomposition (e.g. Newell and Simon, 1972;Johnson-Laird, 1983;Fodor, 1983;Haugeland, 1985), it now more strongly stresses efficient coding and information use (e.g. Eliasmith, 2007;Griffiths, 2009;Friston, 2010). Where it once emphasized the importance of representational multiplicity and centralized integration (e.g. Anderson, 1983;#CITATION_TAG, 1984) it now gives as much weight to exploitation of scaffolding and embodiment (e.g. Wheeler, 1994;Beer, 2000). And where it once committed to symbolic reasoning being the medium of high-level integration (e.g. Marr, 1977;Boden, 1977;Winston, 1984) it increasingly recognizes the greater potential (and neural plausibility) of probabilistic forms (e.g. Doya et al., 2007;Chater and Oaksford, 2009;Clark, 2008).	0	The field has changed significantly in the last two decades. Commitments from earlier years that have recently been revised (and in some cases overturned) include some of those that particularly inform proposals from cognitive archeology. Where cognitive science once emphasized factors of modular decomposition (e.g. Newell and Simon, 1972;Johnson-Laird, 1983;Fodor, 1983;Haugeland, 1985), it now more strongly stresses efficient coding and information use (e.g. Eliasmith, 2007;Griffiths, 2009;Friston, 2010). Where it once emphasized the importance of representational multiplicity and centralized integration (e.g. Anderson, 1983;#CITATION_TAG, 1984) it now gives as much weight to exploitation of scaffolding and embodiment (e.g. Wheeler, 1994;Beer, 2000). And where it once committed to symbolic reasoning being the medium of high-level integration (e.g. Marr, 1977;Boden, 1977;Winston, 1984) it increasingly recognizes the greater potential (and neural plausibility) of probabilistic forms (e.g. Doya et al., 2007;Chater and Oaksford, 2009;Clark, 2008).	e
CC1448	Since the time of these results about statistical reasoning and introspective access to reasoning, other forms of reason have also been studied, and with similar results. In an effort to highlight reasoning that might be isomorphic to philosophical reasoning, I will turn to logical reasoninge.g., deductive and conditional reasoning-the study of which has revealed further systematic errors in reasoning (Barwise 1993, Bell and Johnson-Laird-1998, Bonatti 1994, Cummins 1991, Evans Clibbens and Rood 1996, Evans, Dugan and Revlin 1990, Hardman and Payne 1995, Johnson-Laird 1999, #CITATION_TAG and Byrne 2002, Shynkaruk and Thompson 2006, Thompson 2010, Turner and Thompson 2009, Wetherick and Golhooly 1990.	0	Since the time of these results about statistical reasoning and introspective access to reasoning, other forms of reason have also been studied, and with similar results. In an effort to highlight reasoning that might be isomorphic to philosophical reasoning, I will turn to logical reasoninge.g., deductive and conditional reasoning-the study of which has revealed further systematic errors in reasoning (Barwise 1993, Bell and Johnson-Laird-1998, Bonatti 1994, Cummins 1991, Evans Clibbens and Rood 1996, Evans, Dugan and Revlin 1990, Hardman and Payne 1995, Johnson-Laird 1999, #CITATION_TAG and Byrne 2002, Shynkaruk and Thompson 2006, Thompson 2010, Turner and Thompson 2009, Wetherick and Golhooly 1990.	n
CC2590	"The social aspects of energy consumption lend themselves to incorporating social proof comparisons as part of feedbackeither in reference to a social norm (what is`normal"" consumption) or comparisons to other building users"" energy use, perhaps within a community. A kind of comparative feedback could in fact come from discussion between building users themselves for example, #CITATION_TAG & Bernstein (2006) noted, in a study of two housing developments in California, one a zero-emission home (ZEH) development and one more conventional, that awareness of the value of energy eciency in non-ZEH home owners appeared to have grown over the past year of home ownership, having been associated with paying energy bills and communications with ZEH home owner neighbours whose bills are substantially lower."	0	"The social aspects of energy consumption lend themselves to incorporating social proof comparisons as part of feedbackeither in reference to a social norm (what is`normal"" consumption) or comparisons to other building users"" energy use, perhaps within a community. A kind of comparative feedback could in fact come from discussion between building users themselves for example, #CITATION_TAG & Bernstein (2006) noted, in a study of two housing developments in California, one a zero-emission home (ZEH) development and one more conventional, that awareness of the value of energy eciency in non-ZEH home owners appeared to have grown over the past year of home ownership, having been associated with paying energy bills and communications with ZEH home owner neighbours whose bills are substantially lower."	 
CC1914	Only one study investigated regular and irregular forms in Arabic after aphasia (Mimouni et al., 1998). However, the authors used a theoretical linguistic approach (prosodic nonconcatenative morphology developed by #CITATION_TAG (1975)) rather than models of processing to account for their data. In their study on Algerian Arabic, Mimouni et al. (1998) investigated the process of word form recognition of singular and plural nouns in 24 healthy speakers and two participants with aphasia. The aim was to examine the lexical representation of plurals in Algerian Arabic.	4	Only one study investigated regular and irregular forms in Arabic after aphasia (Mimouni et al., 1998). However, the authors used a theoretical linguistic approach (prosodic nonconcatenative morphology developed by #CITATION_TAG (1975)) rather than models of processing to account for their data. In their study on Algerian Arabic, Mimouni et al. (1998) investigated the process of word form recognition of singular and plural nouns in 24 healthy speakers and two participants with aphasia. The aim was to examine the lexical representation of plurals in Algerian Arabic.	o
CC606	"One fundamental theory in physics that allows us to address these questions is thermodynamics. Central to thermodynamics are the first and second laws. While the first law of thermodynamics quantifies how much work can be extracted from gradients in heating and cooling under the constraint of energy conservation, the second law tells us about the irreversibility of processes and thereby provides us with an ""arrow of time"" [11]. Most common applications of thermodynamics are usually found in engineering, for instance the typical applications to the Carnot cycle of heat engines and refrigerators. Thermodynamics has also been applied to physical processes of the Earth system, e.g. the generation and dissipation of atmospheric motion [12] and associated optimality [13][14][15], the intensity of hurricanes [16], hydrological processes at the land surface [17,18], the atmospheric branch of hydrologic cycle [#CITATION_TAG,20], ocean dynamics [21,22], and, obviously, geochemical transformations. These processes generally operate away from a state of thermodynamic equilibrium and there is some evidence that these operate in steady states at which they maximize their dissipative activity, or, almost equivalently, maximize power generation or entropy production (the proposed Maximum Entropy Production (MEP) principle, [23][24][25][26][27])."	5	"Central to thermodynamics are the first and second laws. While the first law of thermodynamics quantifies how much work can be extracted from gradients in heating and cooling under the constraint of energy conservation, the second law tells us about the irreversibility of processes and thereby provides us with an ""arrow of time"" [11]. Most common applications of thermodynamics are usually found in engineering, for instance the typical applications to the Carnot cycle of heat engines and refrigerators. Thermodynamics has also been applied to physical processes of the Earth system, e.g. the generation and dissipation of atmospheric motion [12] and associated optimality [13][14][15], the intensity of hurricanes [16], hydrological processes at the land surface [17,18], the atmospheric branch of hydrologic cycle [#CITATION_TAG,20], ocean dynamics [21,22], and, obviously, geochemical transformations. These processes generally operate away from a state of thermodynamic equilibrium and there is some evidence that these operate in steady states at which they maximize their dissipative activity, or, almost equivalently, maximize power generation or entropy production (the proposed Maximum Entropy Production (MEP) principle, [23][24][25][26][27])."	m
CC1249	"A Decision Tree (DT) algorithm identifies patterns in a dataset as conditions, represented visually as a decision tree (#CITATION_TAG, 1986). For example, the following two conditions depict a branch of depth two that capture characteristics of instances in a class ""grade=good"": ""if Conscientiousness > 5.6 and Self-Efficacy > 6.3 then Grade = Good. The size of the tree (rule depth) is configurable, influencing the specificity of the resulting model (Quinlan, 1986). Simpler implementations (e.g., C5.0) limit each branch to value ranges from a single attribute, making this a linear classifier with a further restriction that each condition is an axis-parallel hyperplane (Tan et al., 2006). Less restrictive implementations can incorporate a greater range of patterns (e.g., CART, Breiman et al., 1984). Model interpretability makes decision trees a popular choice (Han & Kamber, 2006)."	5	"A Decision Tree (DT) algorithm identifies patterns in a dataset as conditions, represented visually as a decision tree (#CITATION_TAG, 1986). For example, the following two conditions depict a branch of depth two that capture characteristics of instances in a class ""grade=good"": ""if Conscientiousness > 5.6 and Self-Efficacy > 6.3 then Grade = Good. The size of the tree (rule depth) is configurable, influencing the specificity of the resulting model (Quinlan, 1986). Simpler implementations (e.g., C5.0) limit each branch to value ranges from a single attribute, making this a linear classifier with a further restriction that each condition is an axis-parallel hyperplane (Tan et al., 2006)."	A
CC2821	"A systematic search of the literature identified no studies reporting utility scores for adults with autism. In order to estimate QALYs for adults with autism being in either the ""employed"" or the ""unemployed"" health state, we utilised relevant data reported in Squires et al. (2012), who conducted an economic analysis to support NICE public health guidance on managing long-term sickness absence and incapacity for work (NICE, 2009). Squires et al. (2012) estimated utility scores for the health states of ""being at work"" and ""being on long-term sick leave"" using the findings of Peasgood et al. (2006), who transformed 36-item Short-Form Health Survey (SF-36) data derived from members of the general population participating in the British Household Panel Survey (Office for National Statistics, 2001) into Short-Form Health Survey six-dimensional health state classification (SF-6D) utility scores, using the algorithm developed by  #CITATION_TAG et al. (2002)."	5	"A systematic search of the literature identified no studies reporting utility scores for adults with autism. In order to estimate QALYs for adults with autism being in either the ""employed"" or the ""unemployed"" health state, we utilised relevant data reported in Squires et al. (2012), who conducted an economic analysis to support NICE public health guidance on managing long-term sickness absence and incapacity for work (NICE, 2009). Squires et al. (2012) estimated utility scores for the health states of ""being at work"" and ""being on long-term sick leave"" using the findings of Peasgood et al. (2006), who transformed 36-item Short-Form Health Survey (SF-36) data derived from members of the general population participating in the British Household Panel Survey (Office for National Statistics, 2001) into Short-Form Health Survey six-dimensional health state classification (SF-6D) utility scores, using the algorithm developed by  #CITATION_TAG et al. (2002)."	u
CC2445	"Spin foam models are an attempt to produce a theory of quantum gravity starting from a discrete, path integral-like approach. They were first defined a decade ago [6,10]. More recently, we have seen significant progress toward extraction of their semiclassical behavior and its favorable comparison to the expected weak field limit of gravity, starting with Rovelli and collaborators"" calculation of the graviton propagator [30,34]. Unfortunately, further calculations have revealed that the standard spin foam model due to Barrett and Crane produced incorrect results for some of the propagator matrix elements [#CITATION_TAG,4]. This result has motivated several proposals to replace the Barrett-Crane (BC) spin foam vertex amplitude [10] for quantum gravity. The first proposal, by Engle, Pereira and Rovelli (EPR) [20,21], aimed also to identify the spin foam boundary state space with that of loop quantum gravity spin networks; this model is also referred to as the ""flipped"" vertex model. Another proposal, by Livine and Speziale [27,28], used SU (2)-coherent states to define the spin foam amplitudes and reproduced the EPR proposal up to an edge normalization factor. Finally, a paper by Freidel and Krasnov [18,22], suggested that the EPR model corresponds to a topological theory related to gravity and proposed a generalization thereof corresponding to gravity itself (the FK model). The present paper, along with most previous work, concerns only the Riemannian signature models of gravity."	0	"Spin foam models are an attempt to produce a theory of quantum gravity starting from a discrete, path integral-like approach. They were first defined a decade ago [6,10]. More recently, we have seen significant progress toward extraction of their semiclassical behavior and its favorable comparison to the expected weak field limit of gravity, starting with Rovelli and collaborators"" calculation of the graviton propagator [30,34]. Unfortunately, further calculations have revealed that the standard spin foam model due to Barrett and Crane produced incorrect results for some of the propagator matrix elements [#CITATION_TAG,4]. This result has motivated several proposals to replace the Barrett-Crane (BC) spin foam vertex amplitude [10] for quantum gravity. The first proposal, by Engle, Pereira and Rovelli (EPR) [20,21], aimed also to identify the spin foam boundary state space with that of loop quantum gravity spin networks; this model is also referred to as the ""flipped"" vertex model. Another proposal, by Livine and Speziale [27,28], used SU (2)-coherent states to define the spin foam amplitudes and reproduced the EPR proposal up to an edge normalization factor."	o
CC1143	"What follows is shaped by two evolutionary considerations. First, any kind of activity that absorbs up to a half of conscious brain activity must have been selected for its contribution to the human species"" survival. Indeed, it appears that the brain\""s default-mode network provides the substrate for mind-wandering (e.g., Mason et al., 2007;Christoff et al., 2009;Andrews-Hanna et al., 2010a;Stawarczyk et al., 2011b), a network of several ""hubs"" and ""subsystems"" (#CITATION_TAG, 2012) that probably constitutes a majority of the brain\""s energy consumption (Raichle, 2009). It must serve important functions. These include a variety of mental processes, including retrieval of past experiences and imagining future scenarios (Buckner et al., 2008), which are essential for planning and are also stock components of mind-wandering sequences. Raichle (2009) argues that the default-mode network is not coextensive with conscious mind-wandering, citing the continuation of the network""s activity into lighter states of anesthesia and Stages 1 and 2 of sleep (Horovitz et al., 2008), when dreaming is most frequent, and the demonstration (Christoff et al., 2009) of executive-network elements during resting states. However, if one accepts that there is continuity between mind-wandering states and dreaming, and if one recognizes that non-conscious processes (meaning-complexes; Klinger, 1971Klinger, , 2011 underlie both the thought and dream segments, there is no reason to doubt the close relationship of mind-wandering and its variants with the default-mode network."	0	"What follows is shaped by two evolutionary considerations. First, any kind of activity that absorbs up to a half of conscious brain activity must have been selected for its contribution to the human species"" survival. Indeed, it appears that the brain\""s default-mode network provides the substrate for mind-wandering (e.g., Mason et al., 2007;Christoff et al., 2009;Andrews-Hanna et al., 2010a;Stawarczyk et al., 2011b), a network of several ""hubs"" and ""subsystems"" (#CITATION_TAG, 2012) that probably constitutes a majority of the brain\""s energy consumption (Raichle, 2009). It must serve important functions. These include a variety of mental processes, including retrieval of past experiences and imagining future scenarios (Buckner et al., 2008), which are essential for planning and are also stock components of mind-wandering sequences. Raichle (2009) argues that the default-mode network is not coextensive with conscious mind-wandering, citing the continuation of the network""s activity into lighter states of anesthesia and Stages 1 and 2 of sleep (Horovitz et al., 2008), when dreaming is most frequent, and the demonstration (Christoff et al., 2009) of executive-network elements during resting states."	d
CC2441	The Belgian law requires competency at the time that euthanasia is provided. Only in cases of irreversible total unconsciousness (e.g., by brain injury) can witnessed advance directives in writing for euthanasia be honoured. Therefore, such advance directives cannot legally be honoured for still conscious patients who because of, for example, a brain tumour or dementia can be suspected not to be fully competent to confirm their previous wish. Thus the suffering caused by the intolerable prospect of future loss of higher cognitive capacity can bring people whose life could still have been enjoyable for quite some time to opt for a premature death (#CITATION_TAG 2008;Davis 2014). 9 Whether advance directives of non-treatment or termination of life can apply also for dementia is hotly debated (de Boer et al. 2010). The Belgian Senate heard testimonies also on this controversial issue in 2013. Lawmakers and health professionals are discussing not so much whether the law needs to be extended, but what effective safeguards can be devised.	4	The Belgian law requires competency at the time that euthanasia is provided. Only in cases of irreversible total unconsciousness (e.g., by brain injury) can witnessed advance directives in writing for euthanasia be honoured. Therefore, such advance directives cannot legally be honoured for still conscious patients who because of, for example, a brain tumour or dementia can be suspected not to be fully competent to confirm their previous wish. Thus the suffering caused by the intolerable prospect of future loss of higher cognitive capacity can bring people whose life could still have been enjoyable for quite some time to opt for a premature death (#CITATION_TAG 2008;Davis 2014). 9 Whether advance directives of non-treatment or termination of life can apply also for dementia is hotly debated (de Boer et al. 2010). The Belgian Senate heard testimonies also on this controversial issue in 2013. Lawmakers and health professionals are discussing not so much whether the law needs to be extended, but what effective safeguards can be devised.	s
CC2922	RCA is the umbrella term describing methodologies and tools for the retrospective and structured investigation of adverse incidents, near misses and sentinel events (#CITATION_TAG and Shojania, 2001). Originally developed to analyse major industrial incidents (Andersen & Fagerhaug, 2000;Carroll, 1998), since the mid-1990s it has been taken up in healthcare systems, such as the US (Bagian et al., 2002;Wu et al., 2008), Australia, and the UK (��vretveit, 2005;NPSA, 2004). Although each country (indeed sector and company) has developed its own variant, it is characterised by a common set of assumptions and operational activities.	0	RCA is the umbrella term describing methodologies and tools for the retrospective and structured investigation of adverse incidents, near misses and sentinel events (#CITATION_TAG and Shojania, 2001). Originally developed to analyse major industrial incidents (Andersen & Fagerhaug, 2000;Carroll, 1998), since the mid-1990s it has been taken up in healthcare systems, such as the US (Bagian et al., 2002;Wu et al., 2008), Australia, and the UK (��vretveit, 2005;NPSA, 2004). Although each country (indeed sector and company) has developed its own variant, it is characterised by a common set of assumptions and operational activities.	R
CC204	It has been long established that treating all citations with equal weight is counterintuitive. Garfield, the original proponent of the JIF [2], proposed a range of 15 di__�erent reasons a paper may be cited #CITATION_TAG. These can include such reasons as: paying homage to pioneers, substantiating or refuting the earlier work of others, identifying methodologies used or simply giving background information regarding previous work. It can be seen from Garfields original list that simply counting citations cannot paint an entire picture of a papers impact.	0	It has been long established that treating all citations with equal weight is counterintuitive. Garfield, the original proponent of the JIF [2], proposed a range of 15 di__�erent reasons a paper may be cited #CITATION_TAG. These can include such reasons as: paying homage to pioneers, substantiating or refuting the earlier work of others, identifying methodologies used or simply giving background information regarding previous work. It can be seen from Garfields original list that simply counting citations cannot paint an entire picture of a papers impact.	a
CC785	"Historically, sedentary behavior (SED) was conceptualized as the lower end of the PA spectrum, as opposed to moderate-to vigorous PA (MVPA), but is now increasingly being viewed as a behavior distinct from PA, defined as waking behavior characterized by an energy expenditure 1.5 metabolic equivalents (METs) [21]. Although somewhat arbitrary, studies using Actigraph instruments mainly operationalize SED as the time spent <100 cpm. This cutoff differs substantially from that used to define ""inactivity"" (<500 cpm) applied by Matthews et al [6]. Thus, besides the study of reliability of SED in older adults [8], to the best of our knowledge, reliability of SED obtained by accelerometry have not been investigated in adults, and should be prioritized [22]. Furthermore, no studies have determined the intra-individual week-by-week agreement of accelerometer outcomes using absolute measures of reliability (i.e., standard error of the measurement and limits of agreement). Most evidence suggest that a reliability of !0.70-0.80 are achieved for PA with 3-7 days of monitoring by estimation of the number of days needed based on the Spearman Brown prophecy formula, when measurements are conducted over a single 7-day period [5-7, 2, 8]. However, such study designs have received critique for possibly leaving to optimistic results and should be interpreted with caution [23][24][25]. First, the results are in principle only generalizable to the included days, as inclusion of additional days, weeks or seasons will add variability. Secondly, the assumption of compound symmetry (i.e., similar variances and co-variances) across days of measurement might not be fulfilled. Additionally, ICC is the variance partitioning of subjects to the total variance, thus ICC is a relative and context-specific estimate that depends on the heterogeneity of the sample #CITATION_TAG[27][28]. Thus, research targeting agreement of SED and PA measurements by means of absolute measures of reliability, which allow for a direct quantification of how much outcomes vary over time independent of the variability of observations should be given priority."	5	However, such study designs have received critique for possibly leaving to optimistic results and should be interpreted with caution [23][24][25]. First, the results are in principle only generalizable to the included days, as inclusion of additional days, weeks or seasons will add variability. Secondly, the assumption of compound symmetry (i.e., similar variances and co-variances) across days of measurement might not be fulfilled. Additionally, ICC is the variance partitioning of subjects to the total variance, thus ICC is a relative and context-specific estimate that depends on the heterogeneity of the sample #CITATION_TAG[27][28]. Thus, research targeting agreement of SED and PA measurements by means of absolute measures of reliability, which allow for a direct quantification of how much outcomes vary over time independent of the variability of observations should be given priority.	l
CC2373	"This said, the ""translation"" of scientific evidence into health policy is a complex process that is subject to inertia and cultural impediments (Liverani, Hawkins, and Parkhurst 2013). The eventual acceptance of medical ethical developments after prolonged professional and political opposition is not without historical precedents, for example, contraception, assisted procreation, and abortion. Just as the European Association for Palliative Care (EAPC) so far rejects legal euthanasia (Materstvedt et al. 2012), the International Federation of Gynecology and Obstetrics (FIGO) has opposed abortion for many decades. However, in 1998 it took the position that all women must have access to professionally performed abortion. FIGO now states ""neither society, nor members of the health care team responsible for counseling women, have the right to impose their religious or cultural convictions regarding abortion on those whose attitudes are different"" (FIGO 2013, 104) and concludes that ""the Committee recommend[s] that after appropriate counselling, a woman [has] the right to have access to medical or surgical induced abortion, and that the health care service [has] an obligation to provide such services as safely as possible"" (FIGO 2013, 105;Erdman et al. 2013). But also here, in addition to philosophical motives, pragmatic motives have been operating: Another important objective of FIGO was to lift abortion out of clandestine illegal activity, to let it be performed in medically correct conditions and so ensure that abortion is safe and accessible. Similarly, illegal clandestine end-of-life practices, performed without peer control, as documented in Belgium before the euthanasia law and elsewhere (Kuhse et al. 1997;Deliens et al. 2000;#CITATION_TAG and Owens 2003) can be considered more worrying than even imperfectly regulated legal euthanasia. Evidently, bioethical evolutions come about more readily when ethical and pragmatic motives coincide and operate synergistically."	0	"However, in 1998 it took the position that all women must have access to professionally performed abortion. FIGO now states ""neither society, nor members of the health care team responsible for counseling women, have the right to impose their religious or cultural convictions regarding abortion on those whose attitudes are different"" (FIGO 2013, 104) and concludes that ""the Committee recommend[s] that after appropriate counselling, a woman [has] the right to have access to medical or surgical induced abortion, and that the health care service [has] an obligation to provide such services as safely as possible"" (FIGO 2013, 105;Erdman et al. 2013). But also here, in addition to philosophical motives, pragmatic motives have been operating: Another important objective of FIGO was to lift abortion out of clandestine illegal activity, to let it be performed in medically correct conditions and so ensure that abortion is safe and accessible. Similarly, illegal clandestine end-of-life practices, performed without peer control, as documented in Belgium before the euthanasia law and elsewhere (Kuhse et al. 1997;Deliens et al. 2000;#CITATION_TAG and Owens 2003) can be considered more worrying than even imperfectly regulated legal euthanasia. Evidently, bioethical evolutions come about more readily when ethical and pragmatic motives coincide and operate synergistically."	r
CC14	Recommender systems are software tools and methods which provide suggestions for items to users, according to their preferences and needs #CITATION_TAG. They are typically classified as collaborative filtering approaches, content-based filtering approaches and hybrid approaches [14].	0	Recommender systems are software tools and methods which provide suggestions for items to users, according to their preferences and needs #CITATION_TAG. They are typically classified as collaborative filtering approaches, content-based filtering approaches and hybrid approaches [14].	R
CC2208	Possible deviations from pure 1D evolution might be driven by intense oscillations or kinks, as described in #CITATION_TAG (2009). The effect of the three-dimensional loop structure should then be taken into account to describe the interaction with excited MHD waves (McLaughlin and Ofman, 2008;Pascoe et al., 2009;Selwa and Ofman, 2009). However, the real power of 1D loop models, that makes them still on the edge, is that they fully exploit the property of the confined plasma to evolve as a fluid and practically independent of the magnetic field, and that they can include the coronal part, the transition region, and the photospheric footpoint in a single model with thermal conduction. In this framework, we may even simulate a multi-thread structure only by collecting many single loop models together, still with no need to include the description and interaction with the magnetic field (Guarrasi et al., 2010). We should, however, be aware that the magnetic confinement of the loop material is not as strong and the thermal conduction is not as anisotropic below the coronal part of the loop as it is in the corona.	5	Possible deviations from pure 1D evolution might be driven by intense oscillations or kinks, as described in #CITATION_TAG (2009). The effect of the three-dimensional loop structure should then be taken into account to describe the interaction with excited MHD waves (McLaughlin and Ofman, 2008;Pascoe et al., 2009;Selwa and Ofman, 2009). However, the real power of 1D loop models, that makes them still on the edge, is that they fully exploit the property of the confined plasma to evolve as a fluid and practically independent of the magnetic field, and that they can include the coronal part, the transition region, and the photospheric footpoint in a single model with thermal conduction. In this framework, we may even simulate a multi-thread structure only by collecting many single loop models together, still with no need to include the description and interaction with the magnetic field (Guarrasi et al., 2010).	P
CC2905	One strategy pertains to reducing opportunism. Previous studies have demonstrated that opportunism can be reduced mainly through two mechanisms: Increased formalization and/or trust. Consequently, by implementing more formalized procedures such as rules and routines, clearer role responsibilities, and a better identification of complementary tasks and responsibilities between the MNC and the subsidiary, opportunities for opportunism are reduced (Dahlstrom and Nygaard, 1999;#CITATION_TAG et al., 1987). Likewise, continuous evaluation of the results, as well as implementation of plans, and budgeting systems in the subsidiary, is sometimes required to limit opportunities for shirking (Baliga and Jaeger, 1984). Furthermore, MNC headquarters may try to increase trust between headquarters and subsidiaries (Harvey, et al., 2011). Trust is normally a result of investment in bonding activities. As our study is cross-sectional, we cannot observe previous investments in bonding for each MNC in our study, but intuitively it seems reasonable that high opportunism may be a result of low bonding investments over time. Trust needs to be built over time and grows when one party starts to behave in a way the other party finds trustworthy (Serva et al., 2005).	0	One strategy pertains to reducing opportunism. Previous studies have demonstrated that opportunism can be reduced mainly through two mechanisms: Increased formalization and/or trust. Consequently, by implementing more formalized procedures such as rules and routines, clearer role responsibilities, and a better identification of complementary tasks and responsibilities between the MNC and the subsidiary, opportunities for opportunism are reduced (Dahlstrom and Nygaard, 1999;#CITATION_TAG et al., 1987). Likewise, continuous evaluation of the results, as well as implementation of plans, and budgeting systems in the subsidiary, is sometimes required to limit opportunities for shirking (Baliga and Jaeger, 1984). Furthermore, MNC headquarters may try to increase trust between headquarters and subsidiaries (Harvey, et al., 2011). Trust is normally a result of investment in bonding activities.	n
CC676	The model for fibrillation presented here does not take into account the potential fragmentation of growing fibrils [#CITATION_TAG]. The rate of fragmentation would be expected to increase with the average size of growing fibrils and hence an increase in the number of growth sites with time, leading to a potentially cooperative accumulation of fibrillated protein with time [26]. An increasingly crowding-accelerated rate of addition of monomer to an increasing number of growth sites would enhance the cooperative appearance of fibril exhibited by the model presented here, thus strengthening the qualitative conclusions derived from this model. While the models proposed above are highly speculative, we believe that they may have physiological relevance for the following reasons. The appearance of neurodegenerative diseases is relatively abrupt and occurs at an advanced age, except in the case of unstable mutant proteins. There have been many hypotheses proposed relating the aggregation of a protein or peptide associated with a particular disease to the ensuing neuronal damage and death [27,28], including loss of essential native protein function and toxicity of oligomers or large aggregates. The models proposed here suggest that if neurons are particularly susceptible to water loss and the concomitant increase in intracellular crowding with aging, then many neuronal proteins, independent of their intrinsic stability, would at some point abruptly appear to lose solubility as their respective rates of condensation or fiber formation begin to rapidly accelerate. Which protein first becomes substantially insoluble, and thus identified as the cause of a particular infirmity, depends upon the time-dependent macromolecular composition of a particular cell or tissue type, and is therefore subject to variation between individuals.	0	The model for fibrillation presented here does not take into account the potential fragmentation of growing fibrils [#CITATION_TAG]. The rate of fragmentation would be expected to increase with the average size of growing fibrils and hence an increase in the number of growth sites with time, leading to a potentially cooperative accumulation of fibrillated protein with time [26]. An increasingly crowding-accelerated rate of addition of monomer to an increasing number of growth sites would enhance the cooperative appearance of fibril exhibited by the model presented here, thus strengthening the qualitative conclusions derived from this model. While the models proposed above are highly speculative, we believe that they may have physiological relevance for the following reasons.	T
CC2885	"Another key assumption in TCE is that of bounded rationality (Simon, 1957) -i.e. decision-makers are limited in their ability to cope with an uncertain environment and/or have difficulties with validating human performance. Bounded rationality implies problems of getting information about other parties"" performance (Williamson, 1985). The conventional way of dealing with uncertainty and safeguarding problems within TCE logic, is to bring both sides of a transaction into common governance, typically a firm; which hence explains the vertical and horizontal boundaries of firms (#CITATION_TAG, 1984;Williamson, 1985). We argue that there may be variations in certainty about the behaviour of another party even between internalized transactions, and that these variations effect levels of ex post governance costs."	0	"Another key assumption in TCE is that of bounded rationality (Simon, 1957) -i.e. decision-makers are limited in their ability to cope with an uncertain environment and/or have difficulties with validating human performance. Bounded rationality implies problems of getting information about other parties"" performance (Williamson, 1985). The conventional way of dealing with uncertainty and safeguarding problems within TCE logic, is to bring both sides of a transaction into common governance, typically a firm; which hence explains the vertical and horizontal boundaries of firms (#CITATION_TAG, 1984;Williamson, 1985). We argue that there may be variations in certainty about the behaviour of another party even between internalized transactions, and that these variations effect levels of ex post governance costs."	e
CC2603	"In terms of users learning new behaviours from each other, or deciding to adopt more ecient technology, #CITATION_TAG and Dowlatabadi (2007) suggest that this will be easier when such changes are very visible: Establishing social norms works most eectively for technologies or behaviours that are observable by potential adopters, favouring solar photovoltaics over insulation for example. Interventions at the community level are particularly relevant where social norms at the household level might actually be barriers to adoption as in some cases with photovoltaics. Work has been done on giving householders normative feedback, explicitly comparing their energy use against that of`comparable households"" or neighbours. Darby (2006) notes that while householders are interested in comparisons, they do not necessarily make savings when shown them. The choice of comparison groups is problematic (people may be unhappy with the validity of the group they are assigned to) and the response to comparisons may not be positive."	0	"In terms of users learning new behaviours from each other, or deciding to adopt more ecient technology, #CITATION_TAG and Dowlatabadi (2007) suggest that this will be easier when such changes are very visible: Establishing social norms works most eectively for technologies or behaviours that are observable by potential adopters, favouring solar photovoltaics over insulation for example. Interventions at the community level are particularly relevant where social norms at the household level might actually be barriers to adoption as in some cases with photovoltaics. Work has been done on giving householders normative feedback, explicitly comparing their energy use against that of`comparable households"" or neighbours. Darby (2006) notes that while householders are interested in comparisons, they do not necessarily make savings when shown them."	I
CC465	"In AGL tasks it is possible that participants respond on the basis of familiarity of single stimuli or stimulus combinations, with a bias towards combinations that appeared more frequently. Such statistical sensitivity is likely part of configurational processing. To account for such behavior we calculated three familiarity variables for each test sequence (Knowlton & Squire, 1994;Redington & Chater, 1996): associative chunk strength (the mean frequency in which bigrams and trigrams appeared during training), anchor strength (the mean frequency in which initial and final bigrams and trigrams appeared at the respective positions during training) and chunk novelty (the number of bigrams and trigrams which did not appear during training). We then ran a logistic binary regression with acceptance/rejection of each sequence as a binary outcome variable. The predictors were the three familiarity variables as well as the categorical variable Sequence Type (six categories: violation types 1-5, grammatical). Logistic regressions cannot converge if categories are completely full or empty and, as for some cases in , we added or subtracted one rejection from a category if it was full or empty. As a result, the regression assumed that one violation of type 1 and type 2 was accepted, and that one violation of type 3 was rejected. Note that this necessary operation results in an underestimation of the effect of sequence type. A test of the full model was statistically significant, indicating that the predictors distinguished between acceptance and rejection of sequences, __�_=75.649, p<.001, df=8. Nagelkerke""s R 2 of .707 indicated a moderately strong relationship between prediction and response grouping. Only Sequence Type made a significant contribution to prediction, Wald=32.194, p<.001. Familiarity variables made no significant contribution. applied a template analysis to explore the use of different acceptance/rejection criteria by participants. The template analysis describes a number of possible grammaticality criteria (summarized here as configurational only, non-configurational only, or both configurational and non-configurational). For instance, to test whether participants only rejected configurational violations, the analysis classifies not only A n B n sequences, but also sequences of violation types 1 and 2 as grammatical, and all other sequences as ungrammatical. For each template, a D Score (see also #CITATION_TAG & Pacteau, 1990) was calculated by subtracting the percentage of grammatical sequences rejected from the percentage of ungrammatical sequences rejected. D Scores range from -100 to 100, with 100 representing fully consistent application of grammaticality criteria and zero representing chance. We compared WR""s scores with previously collected data from four severely aphasic participants with syntactic disorder, five aphasic participants without syntactic disorder, ten older and 20 younger controls. WR, along with two severely aphasic participants (SA and SO), were the only to consistently reject non-configurational violations but accepted configurational violations. One other severely aphasic participant, JB, showed some sensitivity to configuration."	0	"applied a template analysis to explore the use of different acceptance/rejection criteria by participants. The template analysis describes a number of possible grammaticality criteria (summarized here as configurational only, non-configurational only, or both configurational and non-configurational). For instance, to test whether participants only rejected configurational violations, the analysis classifies not only A n B n sequences, but also sequences of violation types 1 and 2 as grammatical, and all other sequences as ungrammatical. For each template, a D Score (see also #CITATION_TAG & Pacteau, 1990) was calculated by subtracting the percentage of grammatical sequences rejected from the percentage of ungrammatical sequences rejected. D Scores range from -100 to 100, with 100 representing fully consistent application of grammaticality criteria and zero representing chance. We compared WR""s scores with previously collected data from four severely aphasic participants with syntactic disorder, five aphasic participants without syntactic disorder, ten older and 20 younger controls. WR, along with two severely aphasic participants (SA and SO), were the only to consistently reject non-configurational violations but accepted configurational violations."	,
CC775	This occurs for example in the Matutes and Vives (1996) model of depositors and is also See #CITATION_TAG, Binmore, and Samuelson (1995). See section 3.4 in Farrell and Klemperer (2007) for an extensive discussion of the coordination problem in network industries.	0	This occurs for example in the Matutes and Vives (1996) model of depositors and is also See #CITATION_TAG, Binmore, and Samuelson (1995). See section 3.4 in Farrell and Klemperer (2007) for an extensive discussion of the coordination problem in network industries.	T
CC1935	Lanthanum aluminate (LaAlO 3 -LAO) is a prototype compound for soft-mode driven antiferrodistorsive phase transitions. At ambient conditions, LAO crystallizes in the R3c space group, as a result of the condensation of a soft mode at the R point of the Brillouin zone boundary. At ambient conditions, the rhombohedral cell has the lattice parameters a R = 5.357�� and �� R = 60.12 _�� , or in the hexagonal setting a H = 5.366�� and c H = 13.109�� [7]. In this structure, the oxygen octahedra are rotated along the [111] C direction of the parent cubic cell. The rhombohedral distortion can be described by this single tilt angle. Following early structural studies [8,9], the temperature-induced phase transition at 813 K has been studied in great details (e.g. [10] and references within). On the theoretical side, the parameters of a Landau potential were fitted to available experimental data [11]. The pressure-induced rhombohedral to cubic transition of LAO was revealed by a powder Raman spectroscopy and synchrotron diffraction study #CITATION_TAG. It was at that time the first exception to the so-far general rule stating that tilt angles in antiferrodistorsive perovskites should increase under pressure [13]. This example has motivated theoretical work to explain this behaviour and formulate new rules and models to predict qualitatively and quantitavely the evolution of distortions (tilt angles) under pressure [14,15,16,17]. The evolution of the tilt angles in LAO itself was investigated by single crystal diffraction under hydrostatic stress up to 8 GPa [7] and non-hydrostatic stress [18].	2	The rhombohedral distortion can be described by this single tilt angle. Following early structural studies [8,9], the temperature-induced phase transition at 813 K has been studied in great details (e.g. [10] and references within). On the theoretical side, the parameters of a Landau potential were fitted to available experimental data [11]. The pressure-induced rhombohedral to cubic transition of LAO was revealed by a powder Raman spectroscopy and synchrotron diffraction study #CITATION_TAG. It was at that time the first exception to the so-far general rule stating that tilt angles in antiferrodistorsive perovskites should increase under pressure [13]. This example has motivated theoretical work to explain this behaviour and formulate new rules and models to predict qualitatively and quantitavely the evolution of distortions (tilt angles) under pressure [14,15,16,17]. The evolution of the tilt angles in LAO itself was investigated by single crystal diffraction under hydrostatic stress up to 8 GPa [7] and non-hydrostatic stress [18].	s
CC496	"The greatest challenge for explaining sensory and multisensory development -in the present case, two-fold improvements in unisensory speed, together with improving efficiency in poolingis to understand the mechanisms of change. Such mechanisms need to be understood at several distinct levels of analysis -for example, following #CITATION_TAG (1982) influential formulation, both the algorithms used for sensory processing and their ""biophysical"" underpinnings need to be understood, which can be considered to provide separate (albeit inter-related) levels of Integration of audio-visual information p. 34"	3	"The greatest challenge for explaining sensory and multisensory development -in the present case, two-fold improvements in unisensory speed, together with improving efficiency in poolingis to understand the mechanisms of change. Such mechanisms need to be understood at several distinct levels of analysis -for example, following #CITATION_TAG (1982) influential formulation, both the algorithms used for sensory processing and their ""biophysical"" underpinnings need to be understood, which can be considered to provide separate (albeit inter-related) levels of Integration of audio-visual information p. 34"	u
CC803	"In adults, !3-5 days of monitoring are normally considered appropriate, which is in accordance with recommendations given [2]. However, estimates of how many days of monitoring that should be included to obtain a reliable result vary considerably between studies [3-7, 2, 8], and might also vary between outcome variables of interest [6,8]. According to Matthews et al [6], inclusion of more days may be needed to arrive at reliable estimates (intraclass correlation coefficient (ICC) !0.80) for ""physical inactivity"" (<500 cpm from the Actigraph 7164) (!7 days), compared to PA (! 3-4 days). A comparable finding has been shown in older adults, where 2-3 days was needed for PA, whereas 5 days of monitoring was needed for SED (<50 cpm from the Actigraph 7164). The possible impaired reliability for SED compared to other variables may be of critical importance, given the increased interest in SED in the primary and secondary prevention of a range of chronic diseases as well as premature death [9][10][11]. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included !3-5 days of measurement [12][13][14][15][16][17], with some exceptions (!1 day [18]; !6-7 days [19,#CITATION_TAG]). Moreover, as SED are likely to be related to wear time, correction for wear time might improve reliability. Consistent with this hypothesis, percent SED has previously been shown to be superior to minutes of SED as a predictor of metabolic risk [18]."	0	Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included ! 3-5 days of measurement [12][13][14][15][16][17], with some exceptions (! 1 day [18]; ! 6-7 days [19,#CITATION_TAG]). Moreover, as SED are likely to be related to wear time, correction for wear time might improve reliability. Consistent with this hypothesis, percent SED has previously been shown to be superior to minutes of SED as a predictor of metabolic risk [18].	,
CC170	"From the studies included in this review, it appears that-at least in smokers"" self-understanding-commitment might be more important than motivation as an explanation of successful unassisted cessation. The enthusiastic and explicit talk about being determined, committed, or serious suggests that this concept resonates more with smokers than the concept of motivation. The overlapping and at times contradictory natures of commitment and motivation have been highlighted recently by Balmford and Borland who concluded that it may be possible to quit successfully while ambivalent, as long as the smoker remains committed in the face of ebbs and flows in motivation. [56] Further complicating the relationship, some regard commitment as a component of motivation, [57] operationalizing motivation as, for example, ""determination to quit"" #CITATION_TAG or ""commitment to quit"". [59] The greater research interest in reasons for quitting or pros and cons of quitting (i.e., motivation) as opposed to commitment may be because motivation is simpler to measure, for example by asking people to rate or rank reasons, costs or benefits. From a policy and practice perspective, it may also be easier to draw attention to these reasons, costs and benefits, rather than engage with commitment. For example, mass media campaigns can remind smokers of why they should quit by pointing out the benefits to short-term and long-term health. However this review draws attention to the importance of commitment for sustained quitting, at least from the point of view of smokers and quitters. The UK""s annual Stoptober campaign in which smokers committed to being smoke-free for 28 days indicates that creative approaches to addressing commitment can be successful. [60] The final concept identified, willpower, was described in terms of multiple constructs (a personal quality or trait, a method of quitting, a strategy to counteract cravings or urges), suggesting smokers and researchers may use it as a convenient or shorthand heuristic when talking about or reporting on quit success. Despite this lack of clarity, the word has persisted in the qualitative and quantitative smoking cessation literature. It could be fruitful for future research to further examine the meaning of willpower, and particularly its relationship to other more tightly defined concepts such as self-efficacy, [61] self-regulation[62] and self-determination, [63] from the perspective of both researchers and smokers."	2	"From the studies included in this review, it appears that-at least in smokers"" self-understanding-commitment might be more important than motivation as an explanation of successful unassisted cessation. The enthusiastic and explicit talk about being determined, committed, or serious suggests that this concept resonates more with smokers than the concept of motivation. The overlapping and at times contradictory natures of commitment and motivation have been highlighted recently by Balmford and Borland who concluded that it may be possible to quit successfully while ambivalent, as long as the smoker remains committed in the face of ebbs and flows in motivation. [56] Further complicating the relationship, some regard commitment as a component of motivation, [57] operationalizing motivation as, for example, ""determination to quit"" #CITATION_TAG or ""commitment to quit"". [59] The greater research interest in reasons for quitting or pros and cons of quitting (i.e., motivation) as opposed to commitment may be because motivation is simpler to measure, for example by asking people to rate or rank reasons, costs or benefits. From a policy and practice perspective, it may also be easier to draw attention to these reasons, costs and benefits, rather than engage with commitment. For example, mass media campaigns can remind smokers of why they should quit by pointing out the benefits to short-term and long-term health."	]
CC2449	The BC model amplitudes are given in section 2.2.1. The evaluation of this vertex amplitude is discussed in several papers [8,9,16,#CITATION_TAG], where variations on the face and edge amplitudes have also been considered.	2	The BC model amplitudes are given in section 2.2. 1. The evaluation of this vertex amplitude is discussed in several papers [8,9,16,#CITATION_TAG], where variations on the face and edge amplitudes have also been considered.	e
CC2169	Most dopamine neurons are activated by rewards and rewardpredicting stimuli. The reward response codes a prediction error; a reward that is better than predicted elicits an activation (positive prediction error, R > V), a fully predicted reward (R = V) draws no response, and a reward that is worse than predicted induces a depression (negative error, R < V) (Figure 2A) (Schultz et al., 1997). The response implements the teaching term of efficient reinforcement learning models (Rescorla and Wagner, 1972;Sutton and Barto, 1981) and occurs during learning (Schultz et al., 1993;Hollerman and Schultz, 1998). Stimuli not associated with prediction errors are blocked from behavioral and neuronal learning (#CITATION_TAG et al., 2001).	0	Most dopamine neurons are activated by rewards and rewardpredicting stimuli. The reward response codes a prediction error; a reward that is better than predicted elicits an activation (positive prediction error, R > V), a fully predicted reward (R = V) draws no response, and a reward that is worse than predicted induces a depression (negative error, R < V) (Figure 2A) (Schultz et al., 1997). The response implements the teaching term of efficient reinforcement learning models (Rescorla and Wagner, 1972;Sutton and Barto, 1981) and occurs during learning (Schultz et al., 1993;Hollerman and Schultz, 1998). Stimuli not associated with prediction errors are blocked from behavioral and neuronal learning (#CITATION_TAG et al., 2001).	m
CC959	Other methods have been described in the literature, but I have not been able to use them here. The method in [33] is too slow (analysis of data sets of 200 cases exceeding 4 hours; see further details in Additional file 2) if we need to do more than 100,000 analysis, as in this paper. The methods in [7,25] have no software available. Therefore, this paper includes all currently existing approaches for which software is available. It is worth emphasizing the crucial role of the availability of free and open source software both in the growth and development of bioinformatics and computational biology [56][57][58] and for implementing reproducible research #CITATION_TAG. Moreover, the lack of public implementations precludes comparison of otherwise promising approaches, which ultimately hurts practitioners [60].	1	The method in [33] is too slow (analysis of data sets of 200 cases exceeding 4 hours; see further details in Additional file 2) if we need to do more than 100,000 analysis, as in this paper. The methods in [7,25] have no software available. Therefore, this paper includes all currently existing approaches for which software is available. It is worth emphasizing the crucial role of the availability of free and open source software both in the growth and development of bioinformatics and computational biology [56][57][58] and for implementing reproducible research #CITATION_TAG. Moreover, the lack of public implementations precludes comparison of otherwise promising approaches, which ultimately hurts practitioners [60].	s
CC2965	"Perhaps more critical than mental time travel per se, though, were the adaptive advantages to be gained by sharing memories and plans with others. Mental time travel, including memory and prospective cognition, has probably long served to the benefit of the individual, but the ability to share has vast potential to enhance experience and increase survival, at both individual and societal levels. The Pleistocene, the epoch that began with the Oldowan, is widely recognized as the period in which hominins came to occupy what has been termed the ""cognitive niche"" (#CITATION_TAG and DeVore, 1987), depending on social bonding and enhanced communication for survival in the more exposed and dangerous environment of the African savanna. Social sharing seems to be ingrained in humans in a manner not evident in our closest non-human relatives. Tomasello (2008) notes, for example, that infants point to interesting objects in their environments, not to request them, but to share the experience with those around them. This may be a precursor to language, in phylogeny as well as ontogeny. Chimpanzees, in contrast, rarely point, and when they do the aim is usually to request something out of their reach."	0	"Perhaps more critical than mental time travel per se, though, were the adaptive advantages to be gained by sharing memories and plans with others. Mental time travel, including memory and prospective cognition, has probably long served to the benefit of the individual, but the ability to share has vast potential to enhance experience and increase survival, at both individual and societal levels. The Pleistocene, the epoch that began with the Oldowan, is widely recognized as the period in which hominins came to occupy what has been termed the ""cognitive niche"" (#CITATION_TAG and DeVore, 1987), depending on social bonding and enhanced communication for survival in the more exposed and dangerous environment of the African savanna. Social sharing seems to be ingrained in humans in a manner not evident in our closest non-human relatives. Tomasello (2008) notes, for example, that infants point to interesting objects in their environments, not to request them, but to share the experience with those around them. This may be a precursor to language, in phylogeny as well as ontogeny."	e
CC446	The results show large variations in supply and demand shifts among different regions and over time. While some regions experience negative growth in supply or demand, others experience a yearly average growth at double-digit levels. The only significant average negative impact on price from supply growth is from Norway, with an average annual supply shift and impact on price of 10 and -4.5 %. Large standard errors of supply shifts suggest that regional salmon supply, perhaps with the exception of Norway, is anything but smooth. The variance of the supply shifts are significantly larger in all regions compared to that of Norway (results can be obtained from the author upon request). While productivity growth in Norwegian salmon farming has been extensively studied (#CITATION_TAG et al. 2007Asheim et al. 2011;Vassdal and Holst 2011;Asche and Roll 2013), a lack of data has hindered studies of other regions (Asche and Bj�_rndal 2011). The differences in variance of supply shifts between Norway and other regions suggest results from studies of productivity and supply growth based on Norwegian data may not be representative of other salmon-producing countries.	0	The only significant average negative impact on price from supply growth is from Norway, with an average annual supply shift and impact on price of 10 and -4.5 %. Large standard errors of supply shifts suggest that regional salmon supply, perhaps with the exception of Norway, is anything but smooth. The variance of the supply shifts are significantly larger in all regions compared to that of Norway (results can be obtained from the author upon request). While productivity growth in Norwegian salmon farming has been extensively studied (#CITATION_TAG et al. 2007Asheim et al. 2011;Vassdal and Holst 2011;Asche and Roll 2013), a lack of data has hindered studies of other regions (Asche and Bj�_rndal 2011). The differences in variance of supply shifts between Norway and other regions suggest results from studies of productivity and supply growth based on Norwegian data may not be representative of other salmon-producing countries.	 
CC1457	"In order to begin the discussion of the systematic and predictable errors in reasoning, I will introduce the study of statistical reasoning-e.g., probabilistic inference (Bar-Hillel 1973, Chapman and Chapman 1967, 1969, Edwards et al 1968, Epley et al 2004, Epley and Golivoch 2001, Evans 1977, Galbraith and Underwood 1973, Hogarth 1975, Mans 1970, #CITATION_TAG 1975, Savage 1971, Sta�_l von Holstein 1971. Statistical reasoning tasks often required participants to describe something statistically or make an inference under conditions that impose uncertainty. What these tasks reveal is that it is very common for both expert and novice reasoners to make errors in statistical reasoning tasks (for a helpful summary of this kind of research, see Tversky and Kahneman 1974). Interestingly, patterns and similarities manifest in participants"" errors, implying that these errors might have been systematic and perhaps symptomatic of a certain cognitive strategy. Tversky and Kahneman (1974) and others have attempted provide an account of these systematic errors by introducing and characterizing these cognitive strategies: ""heuristics and biases"" (Evans 1984(Evans , 1989Gilovich Griffin and Kahneman 2002, Kahneman et al 2002, Levinson 1995, Tversky 1974. Heuristics and biases reduce the complexity and higher-level computation required by certain tasks to simpler judgments (Tverksy and Kahneman 1974Kahneman , 1124. In other words, Kahneman and Tversky claimed that participants recruited heuristics or biases when answering certain statistical questions-as opposed to performing complete calculations-and that the recruitment of these heuristics or biases accounted for the patterns and similarities in participants"" errors-e.g., why participants"" answers to various statistical questions were apparently insensitive to sample size (ibid., Kahneman andTversky 1973, Shortliffe andBuchanon 1975)."	0	"In order to begin the discussion of the systematic and predictable errors in reasoning, I will introduce the study of statistical reasoning-e.g., probabilistic inference (Bar-Hillel 1973, Chapman and Chapman 1967, 1969, Edwards et al 1968, Epley et al 2004, Epley and Golivoch 2001, Evans 1977, Galbraith and Underwood 1973, Hogarth 1975, Mans 1970, #CITATION_TAG 1975, Savage 1971, Sta�_l von Holstein 1971. Statistical reasoning tasks often required participants to describe something statistically or make an inference under conditions that impose uncertainty. What these tasks reveal is that it is very common for both expert and novice reasoners to make errors in statistical reasoning tasks (for a helpful summary of this kind of research, see Tversky and Kahneman 1974). Interestingly, patterns and similarities manifest in participants"" errors, implying that these errors might have been systematic and perhaps symptomatic of a certain cognitive strategy."	I
CC2002	Very little is known about either the complexity or basis of the successful strategies of lichens in BSCs despite their world-wide importance (Grube et al. 2010). Because of their slow growth, lichens cannot compete effectively against vascular plants but, in areas with Fig. 1 Typical lichen dominated soil crust in high alpine areas, with Psora decipiens, Fulgensia sp. and mosses extreme abiotic conditions such as long periods of drought or cold, higher plants are excluded and lichens fill this important niche (Lalley et al. 2006). The symbiotic life form of lichens is composed of a fungal (mycobiont) and an photosynthetic partner (photobionts), and the latter can be an eukaryotic green alga (chlorobiont) and/or a cyanobacterium (cyanobiont). The ability of mycobionts to switch photobionts (#CITATION_TAG and Gargas 2009;Otalora et al. 2010;Henskens et al. 2012) and associate with more than one photobiont species or genotype along a climatic gradient appears to be a mechanism used by lichens to adapt to particular habitats. This has been reported for crustose lichens (Blaha et al. 2006;Muggia et al. 2008;Ruprecht et al. 2012) and for fruticose lichens (Kroken and Taylor 2000). The influence of photobiont selection on the ecological amplitude of lichens is still largely underexplored (Peksa and Skaloud 2011) and shedding more light on this phenomenon would help towards understanding structure, composition and development of BCSs (Bowker 2007;Lazaro et al. 2008).	1	Because of their slow growth, lichens cannot compete effectively against vascular plants but, in areas with Fig. 1 Typical lichen dominated soil crust in high alpine areas, with Psora decipiens, Fulgensia sp. and mosses extreme abiotic conditions such as long periods of drought or cold, higher plants are excluded and lichens fill this important niche (Lalley et al. 2006). The symbiotic life form of lichens is composed of a fungal (mycobiont) and an photosynthetic partner (photobionts), and the latter can be an eukaryotic green alga (chlorobiont) and/or a cyanobacterium (cyanobiont). The ability of mycobionts to switch photobionts (#CITATION_TAG and Gargas 2009;Otalora et al. 2010;Henskens et al. 2012) and associate with more than one photobiont species or genotype along a climatic gradient appears to be a mechanism used by lichens to adapt to particular habitats. This has been reported for crustose lichens (Blaha et al. 2006;Muggia et al. 2008;Ruprecht et al. 2012) and for fruticose lichens (Kroken and Taylor 2000). The influence of photobiont selection on the ecological amplitude of lichens is still largely underexplored (Peksa and Skaloud 2011) and shedding more light on this phenomenon would help towards understanding structure, composition and development of BCSs (Bowker 2007;Lazaro et al. 2008).	a
CC1423	"Theories and models that endorse or assume the kind of dichotomy just described are often referred to as dual-process or dual-system theories (Chaiken and Trope 1999, Evans 2003, 2014a, 2014b, Evans and Stanovich 2013, Frankish 2010, #CITATION_TAG 1996, Samuels 2009, Sloman 1996, Smith and DeCoster 2000, Thompson 2009, 2010, Wilson Lindsey and Schooler 2000. The duality referred to by ""dualprocess"" has many names, each with it""s own story: associative vs. rulebased (Sloman 1996), heuristic vs. analytic (Evans 1984(Evans , 1989, tacit thought vs. explicit thought (Evans and Over 1996), implicit cognition vs. explicit learning (Reber 1989), interactional vs. analytic (Levinson 1995), experiential vs. rational (Epstein 1994), quick and inflexive modules vs. intellection (Pollock 1991), intuitive cognition vs. analytical cognition (Hammond 1996), recognition primed choice vs. rational choice strategy (Klein 1998), implicit inference vs. explicit inference (Johnson-Laird 1983), automatic vs. controlled processing (Shiffrin and Schneider 1977), automatic activation vs. conscious processing system (Posner andSnyder 1975, 2004), rationality vs. rationality (Evans & Over 1996, intuitive vs. reflective , model-based vs. model-free (Daw et al 2005) and system 1 vs. system 2 (see Stanovich and West 2000 for the first mention of these terms as well as a useful, albeit dated, list of dualprocess terminology; see also Frankish 2010 for a list of features commonly associated with System 1 and System 2). While there are nuanced differences between certain dual-process theories, dual-process theories are those which claim that one can distinguish between at least two cognitive strategies in reasoning, learning, deciding, etc.-I emphasize that there might be more than two processes to avoid problems that result from positing ""binary oppositions"" (Newell 1973). One strategy is characterized by quick, effortless, and possibly associative seemings-referred to in this paper as intuitive-and the other of which is characterized by longer, more effortful, deliberative, perhaps calculative or even rule-based judgmentsreferred to in this paper as reflective. Although it is not entirely clear how these two strategies operate (e.g., serially vs. in parallel), how these strategies interact (e.g., competitively vs. non-competitively, mutually inhibitorily vs. complementarily, etc.), how these strategies are realized neurobiologically, or what these strategies actually are (e.g., systems, processes, styles, habits, personality traits, etc.), there are a variety of reasoning tasks ! 12 that demonstrate the dissociability of the two strategies (see Sloman 1996 for a helpful summary)."	1	"Theories and models that endorse or assume the kind of dichotomy just described are often referred to as dual-process or dual-system theories (Chaiken and Trope 1999, Evans 2003, 2014a, 2014b, Evans and Stanovich 2013, Frankish 2010, #CITATION_TAG 1996, Samuels 2009, Sloman 1996, Smith and DeCoster 2000, Thompson 2009, 2010, Wilson Lindsey and Schooler 2000. The duality referred to by ""dualprocess"" has many names, each with it""s own story: associative vs. rulebased (Sloman 1996), heuristic vs. analytic (Evans 1984(Evans , 1989, tacit thought vs. explicit thought (Evans and Over 1996), implicit cognition vs. explicit learning (Reber 1989), interactional vs. analytic (Levinson 1995), experiential vs. rational (Epstein 1994), quick and inflexive modules vs. intellection (Pollock 1991), intuitive cognition vs. analytical cognition (Hammond 1996), recognition primed choice vs. rational choice strategy (Klein 1998), implicit inference vs. explicit inference (Johnson-Laird 1983), automatic vs. controlled processing (Shiffrin and Schneider 1977), automatic activation vs. conscious processing system (Posner andSnyder 1975, 2004), rationality vs. rationality (Evans & Over 1996, intuitive vs. reflective , model-based vs. model-free (Daw et al 2005) and system 1 vs. system 2 (see Stanovich and West 2000 for the first mention of these terms as well as a useful, albeit dated, list of dualprocess terminology; see also Frankish 2010 for a list of features commonly associated with System 1 and System 2). While there are nuanced differences between certain dual-process theories, dual-process theories are those which claim that one can distinguish between at least two cognitive strategies in reasoning, learning, deciding, etc.-I emphasize that there might be more than two processes to avoid problems that result from positing ""binary oppositions"" (Newell 1973). One strategy is characterized by quick, effortless, and possibly associative seemings-referred to in this paper as intuitive-and the other of which is characterized by longer, more effortful, deliberative, perhaps calculative or even rule-based judgmentsreferred to in this paper as reflective."	T
CC896	"The considerations of agency and autonomy that are so central to ethical appraisals of biomedical technologies are likewise key issues in relation to psychopharmaceuticals [#CITATION_TAG,64]. Yet, wider changes in pharmaceutical consumption also direct our attention to less frequently regarded ethical issues around the innovation, testing and circulation of drugs. Social scientists have increasingly focused on such matters, and their scholarship could have import for bioethics. For instance, Petryna""s [50] work on the outsourcing of clinical trials to middle and low income countries has revealed a range of problematic developments, including biased trial designs that ensure drugs look safer and more efficacious, and proceduralism in ethical review and administration that """"can hide contextual uncertainties"""" [50: 187]. However, anthropological and sociological studies of biomedicine highlight that such problems are not solely salient in contexts beyond ""the West"". Rather, as Abadie [1] starkly illustrates, participation in trials in the US can likewise involve what Singh [65] might call ""cryptic coercion""-as well as more overt forms. Practices of coercion and the strategies of resistance that these impel may impact in important ways on the knowledge trials seek to produce, with a number of ethically significant consequences."	0	"The considerations of agency and autonomy that are so central to ethical appraisals of biomedical technologies are likewise key issues in relation to psychopharmaceuticals [#CITATION_TAG,64]. Yet, wider changes in pharmaceutical consumption also direct our attention to less frequently regarded ethical issues around the innovation, testing and circulation of drugs. Social scientists have increasingly focused on such matters, and their scholarship could have import for bioethics. For instance, Petryna""s [50] work on the outsourcing of clinical trials to middle and low income countries has revealed a range of problematic developments, including biased trial designs that ensure drugs look safer and more efficacious, and proceduralism in ethical review and administration that """"can hide contextual uncertainties"""" [50: 187]."	T
CC429	Multilevel modelling has the advantage of getting a better understanding and more clear interpretation of the effects of higher levels (by estimating and reporting random effects). Furthermore, standard regression analysis ignores the grouping of data and this can cause underestimated standard errors of regression coefficients (Goldstein, 1995;Hox, 2002;#CITATION_TAG and Hox, 2004;Rasbash et al., 2005;Schwanen et al., 2004). The main disadvan- tage is that models become more complex. As a consequence, diagnostics can be more complicated as well. The comparison of different models is often used to evaluate a model. In what follows, we will refer to the so-called empty model (model A), i.e. a model with a multilevel structure (an extra error term) but without any independent variables. The empty model is then compared with the full model (model B), i.e. the multilevel model with the full set of independent variables.	0	Multilevel modelling has the advantage of getting a better understanding and more clear interpretation of the effects of higher levels (by estimating and reporting random effects). Furthermore, standard regression analysis ignores the grouping of data and this can cause underestimated standard errors of regression coefficients (Goldstein, 1995;Hox, 2002;#CITATION_TAG and Hox, 2004;Rasbash et al., 2005;Schwanen et al., 2004). The main disadvan- tage is that models become more complex. As a consequence, diagnostics can be more complicated as well. The comparison of different models is often used to evaluate a model.	u
CC2658	Propensity score matching allows to rule out the impact of unobservable factors and to relax linearity assumptions (#CITATION_TAG and Rubin, 1983). To avoid confounding the effect of political regime transition with that of factors determining this shift, and since one does not observe what would have happened if a democratic country had remained in autocracy (or vice versa), an estimate of the counterfactual is constructed. Conditional on the number of observable characteristics the probability of regime change is calculated for each country, i.e. the propensity score. Based on this estimate, the next step involves evaluating the difference in the evolution of agricultural protection between the countries with and without a regime change. Since matching relies on comparing countries with similar values of propensity score the inferences are not distorted by counterfactuals very different from the treated observations.	0	Propensity score matching allows to rule out the impact of unobservable factors and to relax linearity assumptions (#CITATION_TAG and Rubin, 1983). To avoid confounding the effect of political regime transition with that of factors determining this shift, and since one does not observe what would have happened if a democratic country had remained in autocracy (or vice versa), an estimate of the counterfactual is constructed. Conditional on the number of observable characteristics the probability of regime change is calculated for each country, i.e. the propensity score. Based on this estimate, the next step involves evaluating the difference in the evolution of agricultural protection between the countries with and without a regime change.	P
CC1735	"It is clear that the majority of complex diseases do not harbor this level of proof today; neither do most monogenic disorders. As the case for Marfan syndrome demonstrates, the identification of fibrillin 1 mutations was insufficient to identify therapies without the concomitant understanding of the pathophysiology (Brooke et al., 2008). Animal models are attractive because of the ability to do experimental manipulations that test predictions of gene function, but these experiments test the function of a gene in a context that is decidedly different from that with a human patient. However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-�_ -vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families shows how environmental factors beyond diet can be examined even for congenital disorders (#CITATION_TAG et al., 2012). Despite these successes, pursuit of Koch""s postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately. Nevertheless, such an analysis might reveal an underlying neural phenotype or a molecular or cellular correlate that is in common and subject to testing of the postulates."	4	"However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-�_ -vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families shows how environmental factors beyond diet can be examined even for congenital disorders (#CITATION_TAG et al., 2012). Despite these successes, pursuit of Koch""s postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately."	n
CC196	Research into smoking cessation has achieved much. Researchers have identified numerous variables related to smoking cessation and relapse, including heaviness-of-smoking, quitting history, quit intentions, quit attempts, use of assistance, socio-economic status, gender, age, and exposure to mass-reach interventions such as mass media campaigns, price increases or retail regulation. [1] Behavioural scientists have developed a range of health behaviour models and constructs relevant to smoking cessation, such as the theory of planned behaviour, social cognitive theory, the transtheoretical model and the health belief model. [2][3]#CITATION_TAG[5] These theories have provided constructs to smoking cessation research such as perceived behavioural control, subjective norms, [2] outcome expectations, self-regulation, [3] decisional balance, #CITATION_TAG perceived benefits, perceived barriers and self-efficacy. [5] The knowledge generated has informed the development of a range of pharmacological and behavioural smoking cessation interventions. Yet, although these interventions are efficacious, [6][7][8] the majority of smokers who quit successfully do so without using them, choosing instead to quit unassisted, that is without pharmacological or professional support. [9,10] Many smokers also appear to quit unplanned as a consequence of serendipitous events, [11] throwing into question the predictive validity of some of these cognitive models.	0	Research into smoking cessation has achieved much. Researchers have identified numerous variables related to smoking cessation and relapse, including heaviness-of-smoking, quitting history, quit intentions, quit attempts, use of assistance, socio-economic status, gender, age, and exposure to mass-reach interventions such as mass media campaigns, price increases or retail regulation. [1] Behavioural scientists have developed a range of health behaviour models and constructs relevant to smoking cessation, such as the theory of planned behaviour, social cognitive theory, the transtheoretical model and the health belief model. [2][3]#CITATION_TAG[5] These theories have provided constructs to smoking cessation research such as perceived behavioural control, subjective norms, [2] outcome expectations, self-regulation, [3] decisional balance, #CITATION_TAG perceived benefits, perceived barriers and self-efficacy. [5] The knowledge generated has informed the development of a range of pharmacological and behavioural smoking cessation interventions. Yet, although these interventions are efficacious, [6][7][8] the majority of smokers who quit successfully do so without using them, choosing instead to quit unassisted, that is without pharmacological or professional support. [9,10] Many smokers also appear to quit unplanned as a consequence of serendipitous events, [11] throwing into question the predictive validity of some of these cognitive models.	[
CC2556	In general, simulations of complex systems, including biological systems, are difficult to describe: it is hard to explain and justify complex interactions, either in real biological systems or in simulations. Toxicology and human risk assessment studies, for example, use adverse outcome pathway (AOP) tools to demonstrate existing understanding of how molecular, cellular, organ and organism interactions link a molecular initiating event with a particular adverse outcome, such as skin inflammation [17]. This information, derived from the literature or experimental studies, is analysed and presented as a flow diagram. The strength of the evidence supporting each event, which may be established as well as hypothetical or predictive, is evaluated and accompanies the diagram. Yet, AOPs have been criticized for providing a representation of the toxicological process that is simplistic [17], splitting the representation of the process and the evidence. For the description of computational models, both unified modelling language, adopted in the development of computer software, and systems biology mark-up language (SBML) may be applied [18][19]#CITATION_TAG; the latter possessing the benefit of allowing model execution by a number of SBML-supported software tools. Both however are limited by restrictions in the extent of the system they can capture: UML lacks the formalism to capture some biological features (such as cyclicfeedback) [21], and SBML cannot currently describe complex agent-based models. In addition, both are purely descriptive: neither provides complete, evidence-supported detail stating how that model has been composed. In ecology, the ODD (overview, design concepts, details) protocol is starting to address this, through application of a standard protocol completed while implementing a computational model, with the aim of ensuring reproducibility of results [22]. The ODD protocol addresses the purpose behind the creation of the model, details the inclusion of each biological component of interest (e.g. cell type) and defines submodels that describe how observed biological behaviour and attributes are implemented. Any specific assumptions underlying the implemented behaviour are also recorded. The ODD authors note that completion of the protocol provides researchers with all the information they require to run the simulation and reproduce the published results: the level of information required for a typical methods section of a publication [22]. Yet, ODD also does not provide a motivation and justification for the detailed model and implementation. Scientific repeatability is addressed, not fitness for purpose. The ODD authors also state that, within the protocol, there should be no recording of information concerning experimental scenarios, simulation experiments and results from statistical techniques such as sensitivity analyses: these should be recorded and published separately [22]. However, we contend that having such information within the simulation design is a key part of an argument of fitness for purpose that convinces researchers the simulation is appropriate for the studies in which it will be applied.	1	This information, derived from the literature or experimental studies, is analysed and presented as a flow diagram. The strength of the evidence supporting each event, which may be established as well as hypothetical or predictive, is evaluated and accompanies the diagram. Yet, AOPs have been criticized for providing a representation of the toxicological process that is simplistic [17], splitting the representation of the process and the evidence. For the description of computational models, both unified modelling language, adopted in the development of computer software, and systems biology mark-up language (SBML) may be applied [18][19]#CITATION_TAG; the latter possessing the benefit of allowing model execution by a number of SBML-supported software tools. Both however are limited by restrictions in the extent of the system they can capture: UML lacks the formalism to capture some biological features (such as cyclicfeedback) [21], and SBML cannot currently describe complex agent-based models. In addition, both are purely descriptive: neither provides complete, evidence-supported detail stating how that model has been composed. In ecology, the ODD (overview, design concepts, details) protocol is starting to address this, through application of a standard protocol completed while implementing a computational model, with the aim of ensuring reproducibility of results [22].	h
